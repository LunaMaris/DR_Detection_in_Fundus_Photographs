{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GPU-UNet_Softmax_Training.ipynb","provenance":[{"file_id":"1MshBmCalzqGz5xnx1H0ErFeGsBx_02XT","timestamp":1596527489779},{"file_id":"1pvSj2cOo177dqWp1y6lbiwohGTGeGVm1","timestamp":1596124850987},{"file_id":"1ko8HCMMTNlaR1MCC-1f6QL1uAaqzHXwO","timestamp":1595928299309},{"file_id":"1cjKN36peUbhC36sgbu1cp5rwTPmzAtcg","timestamp":1595852695030}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ga1Mx266ltfH"},"source":["This script is used to perform lesion segmentation in fundus photographs. The lesions that can be segmented are hard exudates, soft exudates, microaneurysms and hemorrhages. The segmentation is based on a UNet, a CNN that takes an image as an input and that outputs a probability map indicating for every pixel the probability of belonging to a certain type of lesion or not."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9eyAksxJOFsi","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597744109940,"user_tz":-120,"elapsed":2265,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"981782e8-be2d-4698-dd49-4c35268db284"},"source":["# import necessary libraries\n","\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import precision_recall_curve\n","\n","import time\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D-lg7YM4d9uB","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597744111231,"user_tz":-120,"elapsed":3541,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"6854efaf-fb4f-4f38-bea5-5e5af1346fad"},"source":["# define how many gpus are available and set a memmory limit\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","print(\"Number of GPUs Available: \", len(gpus))\n","for i in range(1):\n","    tf.config.experimental.set_virtual_device_configuration(gpus[i], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7900)]) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of GPUs Available:  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BeXvl4eft7zY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597744115316,"user_tz":-120,"elapsed":7615,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"182b514a-0d2d-4cf0-f301-46368465719f"},"source":["strategy = tf.distribute.MirroredStrategy()\n","# the number of replicas that is created by the strategy should be equal to the number of GPU's available\n","print ('Number of synchronized replicas created: {}'.format(strategy.num_replicas_in_sync))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","Number of synchronized replicas created: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"M85RveB_uPv3","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1597744135380,"user_tz":-120,"elapsed":27670,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"69bc33da-c576-4c15-d389-f0d8df5b2a81"},"source":["# read in train and test data in case Google DRIVE is used\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x4SbTTqBltfN","colab":{}},"source":["# read in the train and test data for a certain lesion type\n","\n","# Basepath depends on the lesion\n","# LesionType = 'SoftExudates'\n","LesionType = 'HardExudates'\n","# LesionType = 'Microaneurysms'\n","# LesionType = 'Hemorrhages'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Na905WmrltfS","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1597744140809,"user_tz":-120,"elapsed":33083,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"155685ae-a663-417b-ef52-ca611ac517b6"},"source":["# Basepath for Google DRIVE:\n","Basepath = '/content/drive/My Drive/Stage_ENT_Studios/Data/IDRiD/' + LesionType + '/Arrays/'\n","\n","# Basepath for Jupyter notebooks:\n","# Basepath = 'C:/Users/lunam/Documents/1steMaster/Stage/Data_FinalArrays/IDRiD/'+ LesionType+'/Arrays/'\n","\n","# Basepath for KILI\n","# Basepath = '/home/kili/Desktop/Data_FinalArrays/IDRiD/'+ LesionType+'/Arrays/'\n","\n","# train data\n","train_images = np.load(Basepath + 'train_images_Final.npy')\n","print('Shape train images: {}'.format(train_images.shape))\n","\n","train_annotations =  np.load(Basepath + 'train_annotations_Final.npy')\n","# train_annotations = np.expand_dims(train_annotations, axis = 3)\n","print('Shape train annotations: {}'.format(train_annotations.shape))\n","\n","# test data\n","test_images = np.load(Basepath + 'test_images_Final.npy')\n","print('Shape test images: {}'.format(test_images.shape))\n","\n","test_annotations = np.load(Basepath + 'test_annotations_Final.npy')\n","# test_annotations = np.expand_dims(test_annotations, axis = 3)\n","print('Shape test annotations: {}'.format(test_annotations.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape train images: (324, 256, 256, 3)\n","Shape train annotations: (324, 256, 256)\n","Shape test images: (156, 256, 256, 3)\n","Shape test annotations: (156, 256, 256)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WWjV59oPltfZ","colab":{}},"source":["# path to save the model and the tensorboard logs\n","\n","# Basepath for Google DRIVE:\n","base_path = '/content/drive/My Drive/Stage_ENT_Studios/Unet/Logs/'\n","\n","# Basepath for jupyter notebooks:\n","# 'C:/Users/lunam/Documents/1steMaster/Stage/Code_Final/DR_classification/FeatureBasedClassification/Lesion_Segmentation/UNet_Softmax_GPU/Logs/' + LesionType + '/'\n","\n","# Base path for Kili\n","# base_path = '/home/kili/Desktop/FeatureBasedClassification/Lesion_Segmentation/UNet_Softmax_GPU/Logs/' + LesionType + '/'\n","\n","# direction where the tensorboard files will be stored\n","log_dir_tens = base_path + 'Tensorboard_Logs/'\n","# direction where the trained models will be stored\n","log_dir_model = base_path + 'Trained_Model/'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FICWJHVxltfh","colab":{}},"source":["# The UNet network\n","def UNet(drop_prob = 0.1, init_filters = 64):\n","    '''This function defines the original UNet network'''\n","  \n","    # initialization of the weights\n","    W_init = tf.initializers.GlorotUniform()\n","    \n","    # Unet network\n","            \n","    # LEFT part\n","    # input layer\n","    # input_image = tf.keras.layers.InputLayer(input_shape = (512,512,3))\n","    input_image = tf.keras.layers.Input(shape = (256,256,3))\n","        \n","    # Convolutional block 1\n","    conv2d_1 = tf.keras.layers.Conv2D(init_filters, (3, 3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(input_image)\n","    conv2d_2 = tf.keras.layers.Conv2D(init_filters, (3, 3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_1)\n","    pool_1 = tf.keras.layers.MaxPool2D((2, 2), (2, 2))(conv2d_2)\n","    dropout_1 = tf.keras.layers.Dropout(rate = drop_prob)(pool_1)\n","            \n","    # Convolutional block 2\n","    conv2d_3 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_1)\n","    conv2d_4 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_3)\n","    pool_2 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_4)\n","    dropout_2 = tf.keras.layers.Dropout(rate = drop_prob)(pool_2 )\n","            \n","    # Convolutional block 3\n","    conv2d_5 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_2)\n","    conv2d_6 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_5)\n","    pool_3 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_6)\n","    dropout_3 = tf.keras.layers.Dropout(rate = drop_prob)(pool_3)\n","            \n","    # Convolutional block 4\n","    conv2d_7 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_3)\n","    conv2d_8 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_7)\n","    pool_4 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_8)\n","    dropout_4 = tf.keras.layers.Dropout(rate = drop_prob)(pool_4)\n","            \n","    # MIDDLE part\n","    conv2d_9 = tf.keras.layers.Conv2D(16*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_4)\n","    conv2d_10 = tf.keras.layers.Conv2D(16*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_9)\n","            \n","            \n","    # RIGHT part\n","    # Convolutional block 1\n","    upsampling_1 = tf.keras.layers.UpSampling2D((2,2))(conv2d_10)\n","    concat_1 = tf.keras.layers.Concatenate(3)([upsampling_1, conv2d_8])\n","    dropout_5 = tf.keras.layers.Dropout(rate = drop_prob)(concat_1)\n","    conv2d_11 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_5)\n","    conv2d_12 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_11)\n","            \n","    # Convolutional block 2\n","    upsampling_2 = tf.keras.layers.UpSampling2D((2,2))(conv2d_12)\n","    concat_2 = tf.keras.layers.Concatenate(3)([upsampling_2, conv2d_6])\n","    dropout_6 = tf.keras.layers.Dropout(rate = drop_prob)(concat_2)\n","    conv2d_13 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_6)\n","    conv2d_14 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_13)\n","        \n","            \n","    # Convolutional block 3\n","    upsampling_3 = tf.keras.layers.UpSampling2D((2,2))(conv2d_14)\n","    concat_3 = tf.keras.layers.Concatenate(3)([upsampling_3,conv2d_4])\n","    dropout_7 = tf.keras.layers.Dropout(rate = drop_prob)(concat_3)\n","    conv2d_15 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_7)\n","    conv2d_16 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_15)\n","            \n","    # Convolutional block 4\n","    upsampling_4 = tf.keras.layers.UpSampling2D((2,2))(conv2d_16)\n","    concat_4 = tf.keras.layers.Concatenate(3)([upsampling_4,conv2d_2])\n","    dropout_8 = tf.keras.layers.Dropout(rate = drop_prob)(concat_4)\n","    conv2d_17 = tf.keras.layers.Conv2D(init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_8)\n","    conv2d_18 = tf.keras.layers.Conv2D(init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_17)\n","            \n","    # ouput layer\n","    output_image = tf.keras.layers.Conv2D(2, (1,1), kernel_initializer= W_init, padding = 'same')(conv2d_18)\n","\n","    # define the model\n","    model = tf.keras.Model(inputs=input_image, outputs=output_image)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uXe-N1A0JgM5","colab":{}},"source":["# define some different losses\n","\n","# loss defined as in the original UNet paper\n","# The energy function is computed by a pixel-wise soft-max over the final feature map combined with the cross-entropy loss function\n","def loss_UNet(predicted_logits, real_annotations, GlobalBatchSize):\n","    # removes the last dimension of real_annotaitons (axis channel 1 has to be removed)\n","    real_annotations = tf.cast(real_annotations, tf.int32)\n","    loss = tf.nn.sparse_softmax_cross_entropy_with_logits (logits= predicted_logits, labels= real_annotations)\n","    loss = tf.reduce_mean(loss, axis = (1,2))   \n","    return tf.reduce_sum(loss) * (1. / GlobalBatchSize)\n","\n","\n","# here it is the softmax focal cross entropy\n","def loss_sfce(predicted_logits, real_annotations, GlobalBatchSize , alpha = 0.25, gamma = 2.0):\n","    '''\n","    This type of loss function tries to avoid data imbalance in image segmentation\n","    There are two parameters alpha and gamma, the default values are indicated\n","    gamma should always be greater than or equal to 0\n","    '''\n","\n","    pred_prob = tf.nn.softmax(predicted_logits)[:,:,:,1]\n","    real_annotations = tf.cast(real_annotations, tf.float32)\n","  \n","    # classic binary cross_entropy is calculated\n","    ce = K.binary_crossentropy(real_annotations, pred_prob, from_logits= False)\n","\n","    # binary cross-entropy is multiplied with two factors: alpha and modulating factor\n","    # convert the logits predictions into probabilities\n","    alpha_factor = 1.0\n","    modulating_factor = 1.0\n","  \n","    if alpha:\n","        alpha = tf.convert_to_tensor(alpha, dtype=K.floatx())\n","        alpha_factor = real_annotations * alpha + (1 - real_annotations) * (1 - alpha)\n","\n","\n","    p_t = (real_annotations * pred_prob) + ((1 - real_annotations) * (1 - pred_prob))\n","    if gamma:\n","        gamma = tf.convert_to_tensor(gamma, dtype=K.floatx())\n","        modulating_factor = tf.pow((1.0 - p_t), gamma)\n","\n","    # compute the final loss and return\n","    loss = alpha_factor * modulating_factor * ce\n","    loss = tf.reduce_mean(loss, axis =(1,2))\n","    return tf.reduce_sum(loss) * (1. / GlobalBatchSize)\n","\n","\n","# Asymmetric similarity loss function, to balance recall and precision\n","# the larger beta, the more important the recall becomes relative to the precision\n","def loss_asl(predicted_logits, real_annotations,GlobalBatchSize, beta = 2):\n","    real_annotations = tf.cast(real_annotations, tf.float32)\n","    pred_prob = tf.nn.softmax(predicted_logits)[:,:,:,1]\n","  \n","    prod_pos = pred_prob * real_annotations\n","    sum_prod_pos = tf.reduce_sum(tf.reduce_sum(prod_pos, axis = 2), axis = 1)\n","    prod_neg_pred = (1-pred_prob) * real_annotations\n","    sum_prod_neg_pred = tf.reduce_sum(tf.reduce_sum(prod_neg_pred, axis = 2), axis = 1)\n","    prod_neg_real = (pred_prob) * (1-real_annotations)\n","    sum_prod_neg_real = tf.reduce_sum(tf.reduce_sum(prod_neg_real, axis = 2), axis = 1)\n","\n","    beta = tf.convert_to_tensor(beta, dtype=K.floatx())\n","\n","    num = (1+beta**2) * sum_prod_pos\n","    denom = (1+beta**2) *sum_prod_pos + beta**2 * sum_prod_neg_pred + sum_prod_neg_real\n","\n","    loss = num/denom\n","    return tf.reduce_sum(loss) * (1. / GlobalBatchSize)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VQg7qWs9Wo9r","colab":{}},"source":["def train_network(TrainImages, TrainAnnotations, TestImages, TestAnnotations, \n","                  Drop_Prob = 0.1, Init_Filters = 64, batch_size = 3, loss_function = 'UNet_loss', optim = 'Adam', \n","                  learning_rate = tf.Variable(1e-5, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True, print_freq = 1):\n","    '''\n","    This function trains the UNet on the indicated train data with corresponding annotations\n","    At the end the trained model is being saved\n","    '''\n","    # setting up saver for the tensorboard logs\n","    if SaveResults:\n","        # creating summary which stores the results that can be visualised with tensorboard\n","        print(\"Setting up summary writer for tensorboard...\")\n","        summary_writer = tf.summary.create_file_writer(log_dir_tens)\n","\n","    # define the train and test batches that can be fed into the network\n","    # global batch size defines the batch size over all availabel GPU's\n","    print('Creating distributed data')\n","    Global_batch_size = batch_size * strategy.num_replicas_in_sync\n","    train_batch_data  = tf.data.Dataset.from_tensor_slices((TrainImages, TrainAnnotations)).shuffle(TrainImages.shape[0]).batch(Global_batch_size) \n","    test_batch_data = tf.data.Dataset.from_tensor_slices((TestImages, TestAnnotations)).batch(Global_batch_size) \n","\n","    # distribute the data over the different GPU's\n","    train_dist_data =  strategy.experimental_distribute_dataset(train_batch_data)\n","    test_dist_data =  strategy.experimental_distribute_dataset(test_batch_data)\n","\n","    # define the model that will be used for training and for testing\n","    # the model, optimisation and loss have to be distributed among GPU's\n","    tf.compat.v1.reset_default_graph()\n","    with strategy.scope():\n","        \n","        # model\n","        print('Defining the model')\n","        model = UNet(drop_prob = Drop_Prob, init_filters = Init_Filters)\n","    \n","        # loss\n","        print('Defining loss')\n","        def compute_loss(logits, annotations):\n","            if loss_function == 'UNet_loss':\n","                loss = loss_UNet(logits, annotations, Global_batch_size)\n","            elif loss_function == 'Sfce_loss':\n","                loss = loss_sfce(logits, annotations, Global_batch_size)\n","            elif loss_function == 'Asl_loss':\n","                loss = loss_asl(logits, annotations, Global_batch_size)\n","            return loss\n","\n","        # optimization\n","        # a decaying learning rate is used\n","        steps_per_epoch = int(TrainImages.shape[0]/Global_batch_size)\n","        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate= learning_rate, decay_steps= MAX_EPOCH*steps_per_epoch*0.25, decay_rate=0.2, staircase = True)\n","        print('Defining optimization')\n","        if optim == 'Adam':\n","            train_op = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n","        elif optim == 'sgd':\n","            train_op = tf.keras.optimizers.SGD(learning_rate = lr_schedule)\n","\n","        # defining the metrics\n","        print('Defining the metrics')\n","        train_roc_auc = tf.keras.metrics.AUC(curve = 'ROC')\n","        train_pr_auc = tf.keras.metrics.AUC(curve = 'PR')\n","        test_roc_auc = tf.keras.metrics.AUC(curve = 'ROC')\n","        test_pr_auc = tf.keras.metrics.AUC(curve = 'PR')\n","\n","    # one train step is a step in which one batch of data is fed to every GPU\n","    # one train step is a step in which one batch of data is fed to every GPU\n","    def Train_Step(input):\n","\n","        with tf.GradientTape() as tape:\n","            train_images_batch, train_annotations_batch = input\n","            # make prediction with model\n","            pred_logits = model(train_images_batch, training = True)\n","            # compute loss\n","            train_err = compute_loss(pred_logits, train_annotations_batch)\n","\n","        # update model\n","        train_weights = model.trainable_variables\n","        gradients = tape.gradient(train_err, train_weights)\n","        train_op.apply_gradients(zip(gradients, train_weights))\n","        \n","        # turn into probability map\n","        train_pos_prob = tf.nn.softmax(pred_logits)[:,:,:,1]\n","            \n","        # compute auc and aupr score\n","        temp_train_annotations = tf.reshape(train_annotations_batch,[-1])\n","        temp_train_positive_prob = tf.reshape(train_pos_prob,[-1])\n","        train_roc_auc.update_state(temp_train_annotations, temp_train_positive_prob)\n","        train_pr_auc.update_state(temp_train_annotations, temp_train_positive_prob)\n","\n","        # the error per replica is returned\n","        return train_err, train_roc_auc.result(), train_pr_auc.result()\n","\n","    # for the last epoch some testing has to be done, in a test step one batch of test data is fed to every GPU\n","    def Test_Step(input):\n","        test_images_batch, test_annotations_batch = input\n","\n","        # make prediction with model\n","        pred_logits = model(test_images_batch, training = False)\n","        # compute loss\n","        test_err = compute_loss(pred_logits, test_annotations_batch)\n","\n","        # turn into probability map\n","        test_pos_prob = tf.nn.softmax(pred_logits)[:,:,:,1]\n","        \n","        # compute auc and aupr score\n","        temp_test_annotations = tf.reshape(test_annotations_batch,[-1])\n","        temp_test_positive_prob = tf.reshape(test_pos_prob,[-1])\n","        test_roc_auc.update_state(temp_test_annotations, temp_test_positive_prob)\n","        test_pr_auc.update_state(temp_test_annotations, temp_test_positive_prob)\n","    \n","        # the error per replica is returned\n","        return test_err, test_roc_auc.result(), test_pr_auc.result()\n","\n","    @tf.function\n","    def distributed_train_step(dataset_inputs):\n","        per_replica_losses, per_replica_roc_auc, per_replica_pr_auc = strategy.run(Train_Step, args=(dataset_inputs,))\n","        return per_replica_losses, per_replica_roc_auc, per_replica_pr_auc\n","\n","    @tf.function\n","    def distributed_test_step(dataset_inputs):\n","        per_replica_losses, per_replica_roc_auc, per_replica_pr_auc = strategy.run(Test_Step, args=(dataset_inputs,))\n","        return per_replica_losses, per_replica_roc_auc, per_replica_pr_auc\n","    \n","  \n","    # the train and test steps now have to be performed with the distributed strategy\n","    print('Training')\n","    for epo in range(1,MAX_EPOCH+1):\n","        start_time = time.time()\n","    \n","        n_train_steps = 0\n","        total_train_loss = 0\n","        total_train_auc = 0\n","        total_train_aupr = 0\n","        # go over all global batches\n","        for train_input_data in train_dist_data:\n","            n_train_steps+=1\n","            per_replica_train_losses, per_replica_train_roc_auc, per_replica_train_pr_auc = distributed_train_step(train_input_data)\n","            total_train_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_train_losses, axis=None)\n","            total_train_auc += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_train_roc_auc, axis=None)\n","            total_train_aupr += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_train_pr_auc, axis=None)\n","\n","        # every print frequency the train and test resutls are printed out\n","        if epo % print_freq == 0 or epo == 1 or epo == (MAX_EPOCH):\n","            \n","            # calculate the final training results for this epoch\n","            total_train_loss = total_train_loss/n_train_steps\n","            total_train_auc =  total_train_auc/n_train_steps\n","            total_train_aupr =  total_train_aupr/n_train_steps \n","\n","            # print out the train results\n","            print('epoch {} took {}s'.format(epo, time.time() - start_time))\n","            print('   train loss: {}'.format(total_train_loss))\n","            print('   train auc: {}'.format(total_train_auc))\n","            print('   train aupr: {}'.format(total_train_aupr))\n","\n","            if SaveResults:\n","                # save these values to visualize them later with tensorboard\n","                with summary_writer.as_default():\n","                    tf.summary.scalar('train_loss', total_train_loss, step = epo)\n","                    tf.summary.scalar('train_roc_auc', total_train_auc, step = epo)\n","                    tf.summary.scalar('train_pr_auc', total_train_aupr, step = epo)\n","\n","\n","            # some testing has to be done at these print frequencies\n","            print('Testing')\n","            n_test_steps = 0\n","            total_test_loss = 0\n","            total_test_auc = 0\n","            total_test_aupr = 0\n","      \n","            for test_input_data in test_dist_data:\n","                n_test_steps+=1\n","                per_replica_test_losses, per_replica_test_roc_auc, per_replica_test_pr_auc = distributed_test_step(test_input_data)\n","                total_test_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_test_losses, axis=None)\n","                total_test_auc += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_test_roc_auc, axis=None)\n","                total_test_aupr += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_test_pr_auc, axis=None)\n","\n","            total_test_loss = total_test_loss/n_test_steps\n","            total_test_auc =  total_test_auc/n_test_steps\n","            total_test_aupr =  total_test_aupr/n_test_steps\n","\n","            # print out the test results      \n","            print('   test loss: {}'.format(total_test_loss))\n","            print('   test auc: {}'.format(total_test_auc))\n","            print('   test aupr: {}'.format(total_test_aupr))\n","\n","            if SaveResults:\n","                with summary_writer.as_default():\n","                    tf.summary.scalar('test_loss', total_test_loss, step = epo)\n","                    tf.summary.scalar('test_roc_auc', total_test_auc, step = epo)\n","                    tf.summary.scalar('test_pr_auc', total_test_aupr, step = epo)  \n","                    summary_writer.flush()     \n","        \n","\n","        if SaveResults:\n","            # storing the model weights at two time-points\n","            if epo == int(MAX_EPOCH/2):\n","                print('Saving the intermediate model weights...')\n","                model.save_weights(log_dir_model + 'UNet_Softmax_' + str(epo) +'_epochs')\n","                print('Done')\n","\n","        if epo == MAX_EPOCH:\n","            print('Saving the model weights...')\n","            model.save_weights(log_dir_model + 'UNet_Softmax_' + str(epo) +'_epochs')\n","            print('Done')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AY41_1b53yRn","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3e8e93db-730c-4405-8207-6eb97fcd73a7"},"source":["train_network(train_images, train_annotations, test_images, test_annotations, Drop_Prob = 0.3, Init_Filters = 32, batch_size = 6, loss_function = 'UNet_loss', optim = 'Adam', \n","                  learning_rate = tf.Variable(1e-3, dtype=tf.float32), MAX_EPOCH = 1000, SaveResults = True)\n","\n","# (TrainImages, TrainAnnotations, TestImages, TestAnnotations, \n","#                   Drop_Prob = 0.1, Init_Filters = 64, batch_size = 3, loss_function = 'UNet_loss', optim = 'Adam', \n","#                   learning_rate = tf.Variable(1e-3, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaminguitvoer ingekort tot de laatste 5000 regels.\u001b[0m\n","   train aupr: 0.6604405045509338\n","Testing\n","   test loss: 0.027646753937005997\n","   test auc: 0.925621747970581\n","   test aupr: 0.6377239227294922\n","epoch 70 took 9.688624858856201s\n","   train loss: 0.018380019813776016\n","   train auc: 0.9581286311149597\n","   train aupr: 0.6619768738746643\n","Testing\n","   test loss: 0.03697033226490021\n","   test auc: 0.9260436296463013\n","   test aupr: 0.638687252998352\n","epoch 71 took 9.694610834121704s\n","   train loss: 0.019544091075658798\n","   train auc: 0.9584984183311462\n","   train aupr: 0.6638537049293518\n","Testing\n","   test loss: 0.035928331315517426\n","   test auc: 0.9262715578079224\n","   test aupr: 0.6390348672866821\n","epoch 72 took 9.673747062683105s\n","   train loss: 0.019527878612279892\n","   train auc: 0.9587839841842651\n","   train aupr: 0.6650792360305786\n","Testing\n","   test loss: 0.03528199717402458\n","   test auc: 0.9265276789665222\n","   test aupr: 0.6394938826560974\n","epoch 73 took 9.675388097763062s\n","   train loss: 0.018391070887446404\n","   train auc: 0.9591025710105896\n","   train aupr: 0.6666297316551208\n","Testing\n","   test loss: 0.04313119873404503\n","   test auc: 0.9264926314353943\n","   test aupr: 0.63941890001297\n","epoch 74 took 9.673262596130371s\n","   train loss: 0.018418805673718452\n","   train auc: 0.9594248533248901\n","   train aupr: 0.6682772040367126\n","Testing\n","   test loss: 0.0418086014688015\n","   test auc: 0.9262968897819519\n","   test aupr: 0.6389235258102417\n","epoch 75 took 9.693062782287598s\n","   train loss: 0.01720893383026123\n","   train auc: 0.959761917591095\n","   train aupr: 0.6700001358985901\n","Testing\n","   test loss: 0.04473809897899628\n","   test auc: 0.9261003136634827\n","   test aupr: 0.6383543610572815\n","epoch 76 took 9.693013668060303s\n","   train loss: 0.017076127231121063\n","   train auc: 0.9601299166679382\n","   train aupr: 0.672046959400177\n","Testing\n","   test loss: 0.03987834230065346\n","   test auc: 0.9259027242660522\n","   test aupr: 0.6380668878555298\n","epoch 77 took 9.679998874664307s\n","   train loss: 0.016679665073752403\n","   train auc: 0.9604719877243042\n","   train aupr: 0.673931360244751\n","Testing\n","   test loss: 0.03636118397116661\n","   test auc: 0.92591792345047\n","   test aupr: 0.6384559869766235\n","epoch 78 took 9.700163841247559s\n","   train loss: 0.01694670133292675\n","   train auc: 0.9608025550842285\n","   train aupr: 0.6757636070251465\n","Testing\n","   test loss: 0.04755725711584091\n","   test auc: 0.9257475733757019\n","   test aupr: 0.6381517648696899\n","epoch 79 took 9.701256513595581s\n","   train loss: 0.01814742013812065\n","   train auc: 0.961120069026947\n","   train aupr: 0.6774269342422485\n","Testing\n","   test loss: 0.040306977927684784\n","   test auc: 0.9254894256591797\n","   test aupr: 0.6376175284385681\n","epoch 80 took 9.705673694610596s\n","   train loss: 0.016851110383868217\n","   train auc: 0.9614201188087463\n","   train aupr: 0.678996205329895\n","Testing\n","   test loss: 0.04134298861026764\n","   test auc: 0.9253562092781067\n","   test aupr: 0.6376243233680725\n","epoch 81 took 9.696016788482666s\n","   train loss: 0.016496313735842705\n","   train auc: 0.9617118239402771\n","   train aupr: 0.6805508136749268\n","Testing\n","   test loss: 0.04083077609539032\n","   test auc: 0.92518150806427\n","   test aupr: 0.6377658247947693\n","epoch 82 took 9.718822240829468s\n","   train loss: 0.016307201236486435\n","   train auc: 0.9620466828346252\n","   train aupr: 0.6825717091560364\n","Testing\n","   test loss: 0.03389584645628929\n","   test auc: 0.9251850843429565\n","   test aupr: 0.6382633447647095\n","epoch 83 took 9.698037147521973s\n","   train loss: 0.02084680087864399\n","   train auc: 0.9622647762298584\n","   train aupr: 0.68376225233078\n","Testing\n","   test loss: 0.040438637137413025\n","   test auc: 0.9252458810806274\n","   test aupr: 0.6386550068855286\n","epoch 84 took 9.69713544845581s\n","   train loss: 0.016978183761239052\n","   train auc: 0.9624831080436707\n","   train aupr: 0.684866726398468\n","Testing\n","   test loss: 0.04116980358958244\n","   test auc: 0.9251782298088074\n","   test aupr: 0.6387813091278076\n","epoch 85 took 9.698268175125122s\n","   train loss: 0.01611539162695408\n","   train auc: 0.9627938866615295\n","   train aupr: 0.6867077350616455\n","Testing\n","   test loss: 0.028700552880764008\n","   test auc: 0.9253438711166382\n","   test aupr: 0.6396052837371826\n","epoch 86 took 9.705389976501465s\n","   train loss: 0.016117040067911148\n","   train auc: 0.9630757570266724\n","   train aupr: 0.6882917881011963\n","Testing\n","   test loss: 0.0294752586632967\n","   test auc: 0.9257335066795349\n","   test aupr: 0.6410143375396729\n","epoch 87 took 9.687469959259033s\n","   train loss: 0.016087010502815247\n","   train auc: 0.9633663296699524\n","   train aupr: 0.6899901032447815\n","Testing\n","   test loss: 0.0382610559463501\n","   test auc: 0.9259181618690491\n","   test aupr: 0.6419133543968201\n","epoch 88 took 9.705223798751831s\n","   train loss: 0.015751458704471588\n","   train auc: 0.963645875453949\n","   train aupr: 0.6916466951370239\n","Testing\n","   test loss: 0.027800722047686577\n","   test auc: 0.926123321056366\n","   test aupr: 0.6428504586219788\n","epoch 89 took 9.712199687957764s\n","   train loss: 0.015682058408856392\n","   train auc: 0.9639373421669006\n","   train aupr: 0.6934166550636292\n","Testing\n","   test loss: 0.042442385107278824\n","   test auc: 0.9262007474899292\n","   test aupr: 0.64359450340271\n","epoch 90 took 9.708572626113892s\n","   train loss: 0.015548535622656345\n","   train auc: 0.9642273783683777\n","   train aupr: 0.695209264755249\n","Testing\n","   test loss: 0.03517788648605347\n","   test auc: 0.9261256456375122\n","   test aupr: 0.6439952254295349\n","epoch 91 took 9.700983047485352s\n","   train loss: 0.01531449519097805\n","   train auc: 0.9644996523857117\n","   train aupr: 0.6968570351600647\n","Testing\n","   test loss: 0.04090176150202751\n","   test auc: 0.9260900020599365\n","   test aupr: 0.6444777250289917\n","epoch 92 took 9.710700511932373s\n","   train loss: 0.01520967110991478\n","   train auc: 0.9647529721260071\n","   train aupr: 0.6983222961425781\n","Testing\n","   test loss: 0.04352245479822159\n","   test auc: 0.9259092807769775\n","   test aupr: 0.6445457935333252\n","epoch 93 took 9.710766315460205s\n","   train loss: 0.01504694763571024\n","   train auc: 0.965045154094696\n","   train aupr: 0.7002025842666626\n","Testing\n","   test loss: 0.03767899423837662\n","   test auc: 0.9257878065109253\n","   test aupr: 0.6447690725326538\n","epoch 94 took 9.741943120956421s\n","   train loss: 0.016146399080753326\n","   train auc: 0.9652949571609497\n","   train aupr: 0.7016812562942505\n","Testing\n","   test loss: 0.05741222947835922\n","   test auc: 0.9253365397453308\n","   test aupr: 0.6442466974258423\n","epoch 95 took 9.706756830215454s\n","   train loss: 0.02499021589756012\n","   train auc: 0.9654581546783447\n","   train aupr: 0.7025993466377258\n","Testing\n","   test loss: 0.037236280739307404\n","   test auc: 0.9250710010528564\n","   test aupr: 0.643585205078125\n","epoch 96 took 9.684978246688843s\n","   train loss: 0.020898906514048576\n","   train auc: 0.9653853178024292\n","   train aupr: 0.702276349067688\n","Testing\n","   test loss: 0.03907845541834831\n","   test auc: 0.9251940250396729\n","   test aupr: 0.6436505913734436\n","epoch 97 took 9.712329864501953s\n","   train loss: 0.01703067123889923\n","   train auc: 0.9655693173408508\n","   train aupr: 0.7031469941139221\n","Testing\n","   test loss: 0.03261777386069298\n","   test auc: 0.9253504872322083\n","   test aupr: 0.6441279649734497\n","epoch 98 took 9.662117719650269s\n","   train loss: 0.016193479299545288\n","   train auc: 0.9657876491546631\n","   train aupr: 0.7043861746788025\n","Testing\n","   test loss: 0.033595357090234756\n","   test auc: 0.925591230392456\n","   test aupr: 0.644888162612915\n","epoch 99 took 9.688241481781006s\n","   train loss: 0.01569877378642559\n","   train auc: 0.9660044312477112\n","   train aupr: 0.7056294083595276\n","Testing\n","   test loss: 0.033835940062999725\n","   test auc: 0.925767183303833\n","   test aupr: 0.645622193813324\n","epoch 100 took 9.684929370880127s\n","   train loss: 0.015432083047926426\n","   train auc: 0.9662273526191711\n","   train aupr: 0.706957221031189\n","Testing\n","   test loss: 0.030869808048009872\n","   test auc: 0.9259795546531677\n","   test aupr: 0.646524965763092\n","epoch 101 took 9.694051265716553s\n","   train loss: 0.015301467850804329\n","   train auc: 0.9664648771286011\n","   train aupr: 0.708444356918335\n","Testing\n","   test loss: 0.029332147911190987\n","   test auc: 0.926259458065033\n","   test aupr: 0.6475962400436401\n","epoch 102 took 9.682515621185303s\n","   train loss: 0.01746531017124653\n","   train auc: 0.9666731357574463\n","   train aupr: 0.7096184492111206\n","Testing\n","   test loss: 0.02983575314283371\n","   test auc: 0.9265811443328857\n","   test aupr: 0.6484388113021851\n","epoch 103 took 9.661883354187012s\n","   train loss: 0.029961861670017242\n","   train auc: 0.9666629433631897\n","   train aupr: 0.7095208168029785\n","Testing\n","   test loss: 0.0703258216381073\n","   test auc: 0.9261115789413452\n","   test aupr: 0.6476086378097534\n","epoch 104 took 9.655839681625366s\n","   train loss: 0.020811792463064194\n","   train auc: 0.9665989279747009\n","   train aupr: 0.7090041637420654\n","Testing\n","   test loss: 0.034761521965265274\n","   test auc: 0.9256124496459961\n","   test aupr: 0.6468257308006287\n","epoch 105 took 9.644469261169434s\n","   train loss: 0.017597300931811333\n","   train auc: 0.9667519927024841\n","   train aupr: 0.7096081376075745\n","Testing\n","   test loss: 0.05230467766523361\n","   test auc: 0.9254468679428101\n","   test aupr: 0.6464378833770752\n","epoch 106 took 9.668625593185425s\n","   train loss: 0.01661917194724083\n","   train auc: 0.9669396281242371\n","   train aupr: 0.7106024622917175\n","Testing\n","   test loss: 0.052069440484046936\n","   test auc: 0.9249890446662903\n","   test aupr: 0.6456302404403687\n","epoch 107 took 9.668795108795166s\n","   train loss: 0.01629512570798397\n","   train auc: 0.9671372771263123\n","   train aupr: 0.7117061614990234\n","Testing\n","   test loss: 0.03807183355093002\n","   test auc: 0.9247801899909973\n","   test aupr: 0.6455641984939575\n","epoch 108 took 9.659382343292236s\n","   train loss: 0.01599774695932865\n","   train auc: 0.9673295617103577\n","   train aupr: 0.7127803564071655\n","Testing\n","   test loss: 0.037277717143297195\n","   test auc: 0.9247646331787109\n","   test aupr: 0.6459182500839233\n","epoch 109 took 9.68857192993164s\n","   train loss: 0.0175356213003397\n","   train auc: 0.9674929976463318\n","   train aupr: 0.7136684060096741\n","Testing\n","   test loss: 0.03615648299455643\n","   test auc: 0.9248390793800354\n","   test aupr: 0.6463422775268555\n","epoch 110 took 9.674910068511963s\n","   train loss: 0.01607833243906498\n","   train auc: 0.967663586139679\n","   train aupr: 0.7146070003509521\n","Testing\n","   test loss: 0.03483545035123825\n","   test auc: 0.9249544739723206\n","   test aupr: 0.6468044519424438\n","epoch 111 took 9.672617435455322s\n","   train loss: 0.019296608865261078\n","   train auc: 0.9678254127502441\n","   train aupr: 0.7154744267463684\n","Testing\n","   test loss: 0.03924546018242836\n","   test auc: 0.9250116944313049\n","   test aupr: 0.6472299098968506\n","epoch 112 took 9.658615350723267s\n","   train loss: 0.017593666911125183\n","   train auc: 0.9679396152496338\n","   train aupr: 0.716032087802887\n","Testing\n","   test loss: 0.03509270399808884\n","   test auc: 0.9250831604003906\n","   test aupr: 0.6476043462753296\n","epoch 113 took 9.687469959259033s\n","   train loss: 0.016055531799793243\n","   train auc: 0.9681060910224915\n","   train aupr: 0.716978132724762\n","Testing\n","   test loss: 0.038301821798086166\n","   test auc: 0.9251354932785034\n","   test aupr: 0.6479501128196716\n","epoch 114 took 9.676515817642212s\n","   train loss: 0.015565963461995125\n","   train auc: 0.9682872295379639\n","   train aupr: 0.7180626392364502\n","Testing\n","   test loss: 0.030421825125813484\n","   test auc: 0.925227165222168\n","   test aupr: 0.6485792994499207\n","epoch 115 took 9.676819086074829s\n","   train loss: 0.015298440121114254\n","   train auc: 0.9684681296348572\n","   train aupr: 0.7191477417945862\n","Testing\n","   test loss: 0.03452194109559059\n","   test auc: 0.9253629446029663\n","   test aupr: 0.6493746638298035\n","epoch 116 took 9.675379753112793s\n","   train loss: 0.01500660926103592\n","   train auc: 0.9686428308486938\n","   train aupr: 0.7202017307281494\n","Testing\n","   test loss: 0.029185263440012932\n","   test auc: 0.9255247712135315\n","   test aupr: 0.6501871347427368\n","epoch 117 took 9.697674751281738s\n","   train loss: 0.015030529350042343\n","   train auc: 0.9688251614570618\n","   train aupr: 0.7213352918624878\n","Testing\n","   test loss: 0.04335250332951546\n","   test auc: 0.9255290627479553\n","   test aupr: 0.6507031917572021\n","epoch 118 took 9.677048206329346s\n","   train loss: 0.01496813166886568\n","   train auc: 0.9689956903457642\n","   train aupr: 0.7223384380340576\n","Testing\n","   test loss: 0.036650948226451874\n","   test auc: 0.9254189729690552\n","   test aupr: 0.651046633720398\n","epoch 119 took 9.677319288253784s\n","   train loss: 0.014882378280162811\n","   train auc: 0.9691804051399231\n","   train aupr: 0.723523736000061\n","Testing\n","   test loss: 0.04217740148305893\n","   test auc: 0.9253174662590027\n","   test aupr: 0.6514290571212769\n","epoch 120 took 9.661767959594727s\n","   train loss: 0.014842555858194828\n","   train auc: 0.969359278678894\n","   train aupr: 0.7246429920196533\n","Testing\n","   test loss: 0.04664880037307739\n","   test auc: 0.9250791072845459\n","   test aupr: 0.6515035033226013\n","epoch 121 took 9.698768138885498s\n","   train loss: 0.014643601141870022\n","   train auc: 0.969539225101471\n","   train aupr: 0.7258086800575256\n","Testing\n","   test loss: 0.034070879220962524\n","   test auc: 0.924991250038147\n","   test aupr: 0.6517956256866455\n","epoch 122 took 9.676735162734985s\n","   train loss: 0.014429955743253231\n","   train auc: 0.9697082042694092\n","   train aupr: 0.7268545627593994\n","Testing\n","   test loss: 0.03557289019227028\n","   test auc: 0.925047755241394\n","   test aupr: 0.6524104475975037\n","epoch 123 took 9.678784608840942s\n","   train loss: 0.01450023427605629\n","   train auc: 0.9698848128318787\n","   train aupr: 0.7279841899871826\n","Testing\n","   test loss: 0.03922106325626373\n","   test auc: 0.9250280857086182\n","   test aupr: 0.6528249979019165\n","epoch 124 took 9.699046850204468s\n","   train loss: 0.017310991883277893\n","   train auc: 0.9700379371643066\n","   train aupr: 0.72892165184021\n","Testing\n","   test loss: 0.03279893100261688\n","   test auc: 0.9251031875610352\n","   test aupr: 0.6533033847808838\n","epoch 125 took 9.689420223236084s\n","   train loss: 0.01712968945503235\n","   train auc: 0.9701372981071472\n","   train aupr: 0.7294865846633911\n","Testing\n","   test loss: 0.038515347987413406\n","   test auc: 0.9251774549484253\n","   test aupr: 0.653791069984436\n","epoch 126 took 9.671058177947998s\n","   train loss: 0.015531697310507298\n","   train auc: 0.9702683091163635\n","   train aupr: 0.73023521900177\n","Testing\n","   test loss: 0.03332333266735077\n","   test auc: 0.9252296686172485\n","   test aupr: 0.6542116403579712\n","epoch 127 took 9.665727615356445s\n","   train loss: 0.014794332906603813\n","   train auc: 0.9704251885414124\n","   train aupr: 0.7312366962432861\n","Testing\n","   test loss: 0.030787326395511627\n","   test auc: 0.9253798127174377\n","   test aupr: 0.6548521518707275\n","epoch 128 took 9.68972373008728s\n","   train loss: 0.014747573994100094\n","   train auc: 0.9705767035484314\n","   train aupr: 0.7321615815162659\n","Testing\n","   test loss: 0.035533491522073746\n","   test auc: 0.9254777431488037\n","   test aupr: 0.6554749608039856\n","epoch 129 took 9.677394151687622s\n","   train loss: 0.014460867270827293\n","   train auc: 0.9707293510437012\n","   train aupr: 0.7331060767173767\n","Testing\n","   test loss: 0.02960132248699665\n","   test auc: 0.925586998462677\n","   test aupr: 0.6561402082443237\n","epoch 130 took 9.667937278747559s\n","   train loss: 0.01426017191261053\n","   train auc: 0.970884382724762\n","   train aupr: 0.734102725982666\n","Testing\n","   test loss: 0.03242659196257591\n","   test auc: 0.9257380962371826\n","   test aupr: 0.6569271087646484\n","epoch 131 took 9.685385942459106s\n","   train loss: 0.0141147430986166\n","   train auc: 0.9710457921028137\n","   train aupr: 0.7351656556129456\n","Testing\n","   test loss: 0.03257874399423599\n","   test auc: 0.9258435964584351\n","   test aupr: 0.6576359272003174\n","epoch 132 took 9.69424033164978s\n","   train loss: 0.013954444788396358\n","   train auc: 0.9711970090866089\n","   train aupr: 0.7361502051353455\n","Testing\n","   test loss: 0.03373365104198456\n","   test auc: 0.925902247428894\n","   test aupr: 0.6582464575767517\n","epoch 133 took 9.678857564926147s\n","   train loss: 0.01396257895976305\n","   train auc: 0.9713497161865234\n","   train aupr: 0.7371692657470703\n","Testing\n","   test loss: 0.03525012731552124\n","   test auc: 0.9259337186813354\n","   test aupr: 0.6587873101234436\n","epoch 134 took 9.702828407287598s\n","   train loss: 0.01379888691008091\n","   train auc: 0.9715055823326111\n","   train aupr: 0.7382011413574219\n","Testing\n","   test loss: 0.03583493083715439\n","   test auc: 0.9259380102157593\n","   test aupr: 0.6592683792114258\n","epoch 135 took 9.685563325881958s\n","   train loss: 0.01390706468373537\n","   train auc: 0.9716579914093018\n","   train aupr: 0.7392218112945557\n","Testing\n","   test loss: 0.02754046395421028\n","   test auc: 0.9260553121566772\n","   test aupr: 0.6599635481834412\n","epoch 136 took 9.686524391174316s\n","   train loss: 0.013683994300663471\n","   train auc: 0.9718092679977417\n","   train aupr: 0.7402142882347107\n","Testing\n","   test loss: 0.028873832896351814\n","   test auc: 0.9262648820877075\n","   test aupr: 0.660853922367096\n","epoch 137 took 9.713890314102173s\n","   train loss: 0.013519876636564732\n","   train auc: 0.971953809261322\n","   train aupr: 0.7411658763885498\n","Testing\n","   test loss: 0.04155256226658821\n","   test auc: 0.9262746572494507\n","   test aupr: 0.6613540053367615\n","epoch 138 took 9.699957370758057s\n","   train loss: 0.013698958791792393\n","   train auc: 0.9721022248268127\n","   train aupr: 0.7421643733978271\n","Testing\n","   test loss: 0.040147025138139725\n","   test auc: 0.9261434078216553\n","   test aupr: 0.6615750193595886\n","epoch 139 took 9.703619480133057s\n","   train loss: 0.013477723114192486\n","   train auc: 0.9722420573234558\n","   train aupr: 0.7430546283721924\n","Testing\n","   test loss: 0.03947941213846207\n","   test auc: 0.926047682762146\n","   test aupr: 0.6618780493736267\n","epoch 140 took 9.693792343139648s\n","   train loss: 0.013798373751342297\n","   train auc: 0.9723911881446838\n","   train aupr: 0.7440598607063293\n","Testing\n","   test loss: 0.03295683488249779\n","   test auc: 0.9260486960411072\n","   test aupr: 0.6623735427856445\n","epoch 141 took 9.68947696685791s\n","   train loss: 0.014032816514372826\n","   train auc: 0.9725334048271179\n","   train aupr: 0.7450199723243713\n","Testing\n","   test loss: 0.03769823908805847\n","   test auc: 0.9260559678077698\n","   test aupr: 0.6628566980361938\n","epoch 142 took 9.690900802612305s\n","   train loss: 0.013933056965470314\n","   train auc: 0.9726598262786865\n","   train aupr: 0.7458144426345825\n","Testing\n","   test loss: 0.03042312152683735\n","   test auc: 0.9261182546615601\n","   test aupr: 0.6634840369224548\n","epoch 143 took 9.690489530563354s\n","   train loss: 0.013469252735376358\n","   train auc: 0.9727939367294312\n","   train aupr: 0.7466891407966614\n","Testing\n","   test loss: 0.03403107821941376\n","   test auc: 0.9262024760246277\n","   test aupr: 0.6641601324081421\n","epoch 144 took 9.680920124053955s\n","   train loss: 0.013630271889269352\n","   train auc: 0.9729387760162354\n","   train aupr: 0.7476840019226074\n","Testing\n","   test loss: 0.04755276069045067\n","   test auc: 0.9261036515235901\n","   test aupr: 0.6644171476364136\n","epoch 145 took 9.689955472946167s\n","   train loss: 0.013866941444575787\n","   train auc: 0.9730637669563293\n","   train aupr: 0.748489499092102\n","Testing\n","   test loss: 0.03304881602525711\n","   test auc: 0.9260308742523193\n","   test aupr: 0.664715051651001\n","epoch 146 took 9.694775581359863s\n","   train loss: 0.014476917684078217\n","   train auc: 0.9731886982917786\n","   train aupr: 0.7492972612380981\n","Testing\n","   test loss: 0.03828466311097145\n","   test auc: 0.9260210394859314\n","   test aupr: 0.6651659607887268\n","epoch 147 took 9.70811128616333s\n","   train loss: 0.013828246854245663\n","   train auc: 0.9733103513717651\n","   train aupr: 0.7500566840171814\n","Testing\n","   test loss: 0.02329086884856224\n","   test auc: 0.9261525273323059\n","   test aupr: 0.6658239960670471\n","epoch 148 took 9.703261613845825s\n","   train loss: 0.015701057389378548\n","   train auc: 0.9734265804290771\n","   train aupr: 0.7507873773574829\n","Testing\n","   test loss: 0.05303468927741051\n","   test auc: 0.9261044263839722\n","   test aupr: 0.6659078001976013\n","epoch 149 took 9.673329830169678s\n","   train loss: 0.017050620168447495\n","   train auc: 0.973510205745697\n","   train aupr: 0.751250684261322\n","Testing\n","   test loss: 0.03603684529662132\n","   test auc: 0.925940215587616\n","   test aupr: 0.6657708287239075\n","epoch 150 took 9.65030574798584s\n","   train loss: 0.04077348858118057\n","   train auc: 0.9734833240509033\n","   train aupr: 0.7511684894561768\n","Testing\n","   test loss: 0.07564190030097961\n","   test auc: 0.9255920648574829\n","   test aupr: 0.6643305420875549\n","epoch 151 took 9.573518753051758s\n","   train loss: 0.07017943263053894\n","   train auc: 0.972575843334198\n","   train aupr: 0.7476699352264404\n","Testing\n","   test loss: 0.08067925274372101\n","   test auc: 0.9250708818435669\n","   test aupr: 0.6609424352645874\n","epoch 152 took 9.533899307250977s\n","   train loss: 0.06639228761196136\n","   train auc: 0.9717829823493958\n","   train aupr: 0.7435409426689148\n","Testing\n","   test loss: 0.07924694567918777\n","   test auc: 0.9246054887771606\n","   test aupr: 0.6568607091903687\n","epoch 153 took 9.542569875717163s\n","   train loss: 0.06559478491544724\n","   train auc: 0.9711693525314331\n","   train aupr: 0.7393003106117249\n","Testing\n","   test loss: 0.07987309247255325\n","   test auc: 0.9240541458129883\n","   test aupr: 0.652788519859314\n","epoch 154 took 9.487303495407104s\n","   train loss: 0.06628096103668213\n","   train auc: 0.9705789089202881\n","   train aupr: 0.7354903817176819\n","Testing\n","   test loss: 0.08064354956150055\n","   test auc: 0.9235255718231201\n","   test aupr: 0.6490475535392761\n","epoch 155 took 9.495349168777466s\n","   train loss: 0.0656304657459259\n","   train auc: 0.9699524641036987\n","   train aupr: 0.7312807440757751\n","Testing\n","   test loss: 0.08266819268465042\n","   test auc: 0.9228777289390564\n","   test aupr: 0.6451756954193115\n","epoch 156 took 9.485227823257446s\n","   train loss: 0.06529002636671066\n","   train auc: 0.9694065451622009\n","   train aupr: 0.72709721326828\n","Testing\n","   test loss: 0.08182503283023834\n","   test auc: 0.922274649143219\n","   test aupr: 0.6414712071418762\n","epoch 157 took 9.494136095046997s\n","   train loss: 0.06571602821350098\n","   train auc: 0.9688701033592224\n","   train aupr: 0.7231898903846741\n","Testing\n","   test loss: 0.09423931688070297\n","   test auc: 0.9213621616363525\n","   test aupr: 0.6375160217285156\n","epoch 158 took 9.496612787246704s\n","   train loss: 0.06356128305196762\n","   train auc: 0.9682889580726624\n","   train aupr: 0.719350278377533\n","Testing\n","   test loss: 0.086502805352211\n","   test auc: 0.9202114939689636\n","   test aupr: 0.6335034966468811\n","epoch 159 took 9.504698753356934s\n","   train loss: 0.06236300989985466\n","   train auc: 0.9676983952522278\n","   train aupr: 0.715549111366272\n","Testing\n","   test loss: 0.07849523425102234\n","   test auc: 0.9193261861801147\n","   test aupr: 0.629857063293457\n","epoch 160 took 9.533564567565918s\n","   train loss: 0.057188715785741806\n","   train auc: 0.9671053886413574\n","   train aupr: 0.712073802947998\n","Testing\n","   test loss: 0.09299307316541672\n","   test auc: 0.9181833863258362\n","   test aupr: 0.6265918016433716\n","epoch 161 took 9.535723447799683s\n","   train loss: 0.05508490279316902\n","   train auc: 0.9665030837059021\n","   train aupr: 0.7090657353401184\n","Testing\n","   test loss: 0.0674256980419159\n","   test auc: 0.9172333478927612\n","   test aupr: 0.6238594651222229\n","epoch 162 took 9.535408020019531s\n","   train loss: 0.050124410539865494\n","   train auc: 0.9659765958786011\n","   train aupr: 0.7062419056892395\n","Testing\n","   test loss: 0.06771393120288849\n","   test auc: 0.9165970683097839\n","   test aupr: 0.6214539408683777\n","epoch 163 took 9.541471719741821s\n","   train loss: 0.04903891682624817\n","   train auc: 0.9655296206474304\n","   train aupr: 0.7038429975509644\n","Testing\n","   test loss: 0.10708848387002945\n","   test auc: 0.9155898690223694\n","   test aupr: 0.6191208362579346\n","epoch 164 took 9.539647102355957s\n","   train loss: 0.049035005271434784\n","   train auc: 0.9649918079376221\n","   train aupr: 0.7013186812400818\n","Testing\n","   test loss: 0.07622773200273514\n","   test auc: 0.9145510196685791\n","   test aupr: 0.6169938445091248\n","epoch 165 took 9.550356388092041s\n","   train loss: 0.04168238863348961\n","   train auc: 0.9645892977714539\n","   train aupr: 0.6993388533592224\n","Testing\n","   test loss: 0.06372940540313721\n","   test auc: 0.9138733744621277\n","   test aupr: 0.6153485774993896\n","epoch 166 took 9.533524513244629s\n","   train loss: 0.040603432804346085\n","   train auc: 0.9642811417579651\n","   train aupr: 0.6977652311325073\n","Testing\n","   test loss: 0.05997959151864052\n","   test auc: 0.9134369492530823\n","   test aupr: 0.6139689087867737\n","epoch 167 took 9.542571306228638s\n","   train loss: 0.040161848068237305\n","   train auc: 0.9639816284179688\n","   train aupr: 0.6961876153945923\n","Testing\n","   test loss: 0.06097697839140892\n","   test auc: 0.9130599498748779\n","   test aupr: 0.6126226782798767\n","epoch 168 took 9.542381286621094s\n","   train loss: 0.03734287619590759\n","   train auc: 0.963729202747345\n","   train aupr: 0.6948012113571167\n","Testing\n","   test loss: 0.06444705277681351\n","   test auc: 0.9125989079475403\n","   test aupr: 0.6112186908721924\n","epoch 169 took 9.544144868850708s\n","   train loss: 0.03811015188694\n","   train auc: 0.9634818434715271\n","   train aupr: 0.6934932470321655\n","Testing\n","   test loss: 0.06748384982347488\n","   test auc: 0.9120074510574341\n","   test aupr: 0.609796404838562\n","epoch 170 took 9.555582761764526s\n","   train loss: 0.035539448261260986\n","   train auc: 0.9632664918899536\n","   train aupr: 0.6922983527183533\n","Testing\n","   test loss: 0.07642653584480286\n","   test auc: 0.9111951589584351\n","   test aupr: 0.6081826686859131\n","epoch 171 took 9.559269189834595s\n","   train loss: 0.0349966362118721\n","   train auc: 0.9630906581878662\n","   train aupr: 0.6912787556648254\n","Testing\n","   test loss: 0.059673648327589035\n","   test auc: 0.9105640649795532\n","   test aupr: 0.606681227684021\n","epoch 172 took 9.563830137252808s\n","   train loss: 0.03460429981350899\n","   train auc: 0.9629061818122864\n","   train aupr: 0.6902259588241577\n","Testing\n","   test loss: 0.07806657999753952\n","   test auc: 0.9099741578102112\n","   test aupr: 0.6052497625350952\n","epoch 173 took 9.5654878616333s\n","   train loss: 0.03386669605970383\n","   train auc: 0.962748646736145\n","   train aupr: 0.6892677545547485\n","Testing\n","   test loss: 0.0677439495921135\n","   test auc: 0.9092432260513306\n","   test aupr: 0.6037963032722473\n","epoch 174 took 9.580030918121338s\n","   train loss: 0.03305935114622116\n","   train auc: 0.9626186490058899\n","   train aupr: 0.6884556412696838\n","Testing\n","   test loss: 0.05598596855998039\n","   test auc: 0.9088371992111206\n","   test aupr: 0.6026906371116638\n","epoch 175 took 9.574793338775635s\n","   train loss: 0.03415709361433983\n","   train auc: 0.9624602198600769\n","   train aupr: 0.6875362396240234\n","Testing\n","   test loss: 0.06320000439882278\n","   test auc: 0.9084999561309814\n","   test aupr: 0.6017104983329773\n","epoch 176 took 9.554144859313965s\n","   train loss: 0.031242651864886284\n","   train auc: 0.9623323082923889\n","   train aupr: 0.6867450475692749\n","Testing\n","   test loss: 0.06863971054553986\n","   test auc: 0.9079484939575195\n","   test aupr: 0.6005446910858154\n","epoch 177 took 9.565632343292236s\n","   train loss: 0.032856956124305725\n","   train auc: 0.9622365236282349\n","   train aupr: 0.6860968470573425\n","Testing\n","   test loss: 0.0563785582780838\n","   test auc: 0.9075251817703247\n","   test aupr: 0.5995200872421265\n","epoch 178 took 9.57565689086914s\n","   train loss: 0.03145802393555641\n","   train auc: 0.9621086120605469\n","   train aupr: 0.6853575706481934\n","Testing\n","   test loss: 0.07069598138332367\n","   test auc: 0.9070645570755005\n","   test aupr: 0.5984198451042175\n","epoch 179 took 9.568109035491943s\n","   train loss: 0.03269222006201744\n","   train auc: 0.9620087146759033\n","   train aupr: 0.684723436832428\n","Testing\n","   test loss: 0.06261201947927475\n","   test auc: 0.9065409302711487\n","   test aupr: 0.597377359867096\n","epoch 180 took 9.545655012130737s\n","   train loss: 0.03261348977684975\n","   train auc: 0.9618639349937439\n","   train aupr: 0.6839495301246643\n","Testing\n","   test loss: 0.05855438858270645\n","   test auc: 0.9062173366546631\n","   test aupr: 0.5965340733528137\n","epoch 181 took 9.562202215194702s\n","   train loss: 0.03016473911702633\n","   train auc: 0.9617595076560974\n","   train aupr: 0.6832778453826904\n","Testing\n","   test loss: 0.05736968293786049\n","   test auc: 0.9059596657752991\n","   test aupr: 0.5956644415855408\n","epoch 182 took 9.549458980560303s\n","   train loss: 0.028833063319325447\n","   train auc: 0.9617019295692444\n","   train aupr: 0.6828085780143738\n","Testing\n","   test loss: 0.058991868048906326\n","   test auc: 0.9056831002235413\n","   test aupr: 0.5948598384857178\n","epoch 183 took 9.564806699752808s\n","   train loss: 0.028117820620536804\n","   train auc: 0.9616630673408508\n","   train aupr: 0.6824765801429749\n","Testing\n","   test loss: 0.05975046381354332\n","   test auc: 0.9053800106048584\n","   test aupr: 0.5940276384353638\n","epoch 184 took 9.541495323181152s\n","   train loss: 0.028334803879261017\n","   train auc: 0.961627721786499\n","   train aupr: 0.6821661591529846\n","Testing\n","   test loss: 0.05808739364147186\n","   test auc: 0.9050877690315247\n","   test aupr: 0.593248724937439\n","epoch 185 took 9.570109844207764s\n","   train loss: 0.02902781032025814\n","   train auc: 0.9615761041641235\n","   train aupr: 0.6817622780799866\n","Testing\n","   test loss: 0.07442586123943329\n","   test auc: 0.9045745134353638\n","   test aupr: 0.5921533703804016\n","epoch 186 took 9.584728717803955s\n","   train loss: 0.028309965506196022\n","   train auc: 0.9615246057510376\n","   train aupr: 0.6813710331916809\n","Testing\n","   test loss: 0.0737229660153389\n","   test auc: 0.9038568735122681\n","   test aupr: 0.5907526612281799\n","epoch 187 took 9.569029808044434s\n","   train loss: 0.02882491424679756\n","   train auc: 0.9614830613136292\n","   train aupr: 0.6810587644577026\n","Testing\n","   test loss: 0.04479852691292763\n","   test auc: 0.9036032557487488\n","   test aupr: 0.5900048613548279\n","epoch 188 took 9.557194471359253s\n","   train loss: 0.0284907016903162\n","   train auc: 0.9614218473434448\n","   train aupr: 0.6806282997131348\n","Testing\n","   test loss: 0.05399855226278305\n","   test auc: 0.903593122959137\n","   test aupr: 0.5896453261375427\n","epoch 189 took 9.56348967552185s\n","   train loss: 0.027106883004307747\n","   train auc: 0.9614017009735107\n","   train aupr: 0.6804186701774597\n","Testing\n","   test loss: 0.0620739609003067\n","   test auc: 0.9033230543136597\n","   test aupr: 0.58890700340271\n","epoch 190 took 9.565119981765747s\n","   train loss: 0.02717684954404831\n","   train auc: 0.9613833427429199\n","   train aupr: 0.6802246570587158\n","Testing\n","   test loss: 0.055265069007873535\n","   test auc: 0.9030649662017822\n","   test aupr: 0.5881423950195312\n","epoch 191 took 9.560771226882935s\n","   train loss: 0.02672223187983036\n","   train auc: 0.9613624811172485\n","   train aupr: 0.6800024509429932\n","Testing\n","   test loss: 0.05848967283964157\n","   test auc: 0.9028447866439819\n","   test aupr: 0.5875120759010315\n","epoch 192 took 9.561127662658691s\n","   train loss: 0.02577570639550686\n","   train auc: 0.9613582491874695\n","   train aupr: 0.6798518896102905\n","Testing\n","   test loss: 0.06177038326859474\n","   test auc: 0.9025331139564514\n","   test aupr: 0.5867496132850647\n","epoch 193 took 9.565929889678955s\n","   train loss: 0.02600148692727089\n","   train auc: 0.9613549709320068\n","   train aupr: 0.6797348856925964\n","Testing\n","   test loss: 0.05146555230021477\n","   test auc: 0.902315080165863\n","   test aupr: 0.5861599445343018\n","epoch 194 took 9.548428297042847s\n","   train loss: 0.026070762425661087\n","   train auc: 0.961351752281189\n","   train aupr: 0.6796094179153442\n","Testing\n","   test loss: 0.044437944889068604\n","   test auc: 0.9023281335830688\n","   test aupr: 0.5860057473182678\n","epoch 195 took 9.557034969329834s\n","   train loss: 0.02532731182873249\n","   train auc: 0.9613651633262634\n","   train aupr: 0.6795693039894104\n","Testing\n","   test loss: 0.05781342461705208\n","   test auc: 0.9022523164749146\n","   test aupr: 0.5856958627700806\n","epoch 196 took 9.557940483093262s\n","   train loss: 0.025709887966513634\n","   train auc: 0.9613707065582275\n","   train aupr: 0.6794888973236084\n","Testing\n","   test loss: 0.0553307868540287\n","   test auc: 0.9020460844039917\n","   test aupr: 0.5851138830184937\n","epoch 197 took 9.557589769363403s\n","   train loss: 0.027229217812418938\n","   train auc: 0.9613575339317322\n","   train aupr: 0.6792963743209839\n","Testing\n","   test loss: 0.04972323030233383\n","   test auc: 0.9019591212272644\n","   test aupr: 0.5847209692001343\n","epoch 198 took 9.560333490371704s\n","   train loss: 0.02474709413945675\n","   train auc: 0.9613670110702515\n","   train aupr: 0.6792322993278503\n","Testing\n","   test loss: 0.05519598349928856\n","   test auc: 0.9018582701683044\n","   test aupr: 0.584370493888855\n","epoch 199 took 9.557182550430298s\n","   train loss: 0.024048510938882828\n","   train auc: 0.961396336555481\n","   train aupr: 0.6792839169502258\n","Testing\n","   test loss: 0.05535406619310379\n","   test auc: 0.901688277721405\n","   test aupr: 0.5839165449142456\n","epoch 200 took 9.57433533668518s\n","   train loss: 0.024802232161164284\n","   train auc: 0.9614226818084717\n","   train aupr: 0.6793025732040405\n","Testing\n","   test loss: 0.05413501709699631\n","   test auc: 0.901538610458374\n","   test aupr: 0.583440899848938\n","epoch 201 took 9.577121019363403s\n","   train loss: 0.024050263687968254\n","   train auc: 0.9614382982254028\n","   train aupr: 0.6792762875556946\n","Testing\n","   test loss: 0.04725758731365204\n","   test auc: 0.9014915823936462\n","   test aupr: 0.5831570029258728\n","epoch 202 took 9.578980207443237s\n","   train loss: 0.02402259223163128\n","   train auc: 0.9614753723144531\n","   train aupr: 0.6793510317802429\n","Testing\n","   test loss: 0.04757744073867798\n","   test auc: 0.901519775390625\n","   test aupr: 0.5830440521240234\n","epoch 203 took 9.580780506134033s\n","   train loss: 0.023674054071307182\n","   train auc: 0.9615089297294617\n","   train aupr: 0.6794121861457825\n","Testing\n","   test loss: 0.0467861071228981\n","   test auc: 0.9015637636184692\n","   test aupr: 0.5829471349716187\n","epoch 204 took 9.5781991481781s\n","   train loss: 0.023788170889019966\n","   train auc: 0.9615458250045776\n","   train aupr: 0.6794759035110474\n","Testing\n","   test loss: 0.05500490218400955\n","   test auc: 0.9015149474143982\n","   test aupr: 0.5826998353004456\n","epoch 205 took 9.578609704971313s\n","   train loss: 0.02325623854994774\n","   train auc: 0.9615872502326965\n","   train aupr: 0.679562509059906\n","Testing\n","   test loss: 0.04693163186311722\n","   test auc: 0.9014715552330017\n","   test aupr: 0.5824701189994812\n","epoch 206 took 9.571574926376343s\n","   train loss: 0.02414332889020443\n","   train auc: 0.9616184234619141\n","   train aupr: 0.6796214580535889\n","Testing\n","   test loss: 0.04637795314192772\n","   test auc: 0.9015260934829712\n","   test aupr: 0.5824177861213684\n","epoch 207 took 9.578349351882935s\n","   train loss: 0.023616420105099678\n","   train auc: 0.9616595506668091\n","   train aupr: 0.6797082424163818\n","Testing\n","   test loss: 0.06138060614466667\n","   test auc: 0.9013945460319519\n","   test aupr: 0.58211749792099\n","epoch 208 took 9.581376075744629s\n","   train loss: 0.02291383594274521\n","   train auc: 0.9617066383361816\n","   train aupr: 0.6798328161239624\n","Testing\n","   test loss: 0.04857144132256508\n","   test auc: 0.9012447595596313\n","   test aupr: 0.5818029642105103\n","epoch 209 took 9.595494747161865s\n","   train loss: 0.0231986865401268\n","   train auc: 0.9617475867271423\n","   train aupr: 0.679906964302063\n","Testing\n","   test loss: 0.05067775398492813\n","   test auc: 0.9012212753295898\n","   test aupr: 0.5816024541854858\n","epoch 210 took 9.567204236984253s\n","   train loss: 0.022649148479104042\n","   train auc: 0.9617931246757507\n","   train aupr: 0.6800564527511597\n","Testing\n","   test loss: 0.041945867240428925\n","   test auc: 0.9012726545333862\n","   test aupr: 0.5815422534942627\n","epoch 211 took 9.566941976547241s\n","   train loss: 0.023404011502861977\n","   train auc: 0.9618403911590576\n","   train aupr: 0.6801897883415222\n","Testing\n","   test loss: 0.04172647371888161\n","   test auc: 0.9014151692390442\n","   test aupr: 0.5816881060600281\n","epoch 212 took 9.558199644088745s\n","   train loss: 0.022768141701817513\n","   train auc: 0.9618880748748779\n","   train aupr: 0.680334746837616\n","Testing\n","   test loss: 0.04875754937529564\n","   test auc: 0.9014752507209778\n","   test aupr: 0.5817024111747742\n","epoch 213 took 9.567181825637817s\n","   train loss: 0.022310204803943634\n","   train auc: 0.9619358777999878\n","   train aupr: 0.6804832816123962\n","Testing\n","   test loss: 0.056766387075185776\n","   test auc: 0.9013715386390686\n","   test aupr: 0.5814493298530579\n","epoch 214 took 9.567312479019165s\n","   train loss: 0.022932186722755432\n","   train auc: 0.9619817137718201\n","   train aupr: 0.680614709854126\n","Testing\n","   test loss: 0.05557476729154587\n","   test auc: 0.9012043476104736\n","   test aupr: 0.5810945630073547\n","epoch 215 took 9.584225177764893s\n","   train loss: 0.021676894277334213\n","   train auc: 0.9620347619056702\n","   train aupr: 0.6808111667633057\n","Testing\n","   test loss: 0.0551275797188282\n","   test auc: 0.901051938533783\n","   test aupr: 0.5807701945304871\n","epoch 216 took 9.57319951057434s\n","   train loss: 0.021779349073767662\n","   train auc: 0.962095320224762\n","   train aupr: 0.6810349225997925\n","Testing\n","   test loss: 0.04152446985244751\n","   test auc: 0.9010417461395264\n","   test aupr: 0.5807163715362549\n","epoch 217 took 9.584930896759033s\n","   train loss: 0.020680859684944153\n","   train auc: 0.9621626734733582\n","   train aupr: 0.6813330054283142\n","Testing\n","   test loss: 0.05363456904888153\n","   test auc: 0.9010350704193115\n","   test aupr: 0.580696165561676\n","epoch 218 took 9.585187435150146s\n","   train loss: 0.021635496988892555\n","   train auc: 0.9622248411178589\n","   train aupr: 0.6815857291221619\n","Testing\n","   test loss: 0.04894208163022995\n","   test auc: 0.9009730219841003\n","   test aupr: 0.5805426239967346\n","epoch 219 took 9.579897403717041s\n","   train loss: 0.022150354459881783\n","   train auc: 0.9622787833213806\n","   train aupr: 0.6817823052406311\n","Testing\n","   test loss: 0.042639896273612976\n","   test auc: 0.9010072946548462\n","   test aupr: 0.5805875062942505\n","epoch 220 took 9.569322109222412s\n","   train loss: 0.02081991359591484\n","   train auc: 0.9623423218727112\n","   train aupr: 0.6820651292800903\n","Testing\n","   test loss: 0.03666972741484642\n","   test auc: 0.9011519551277161\n","   test aupr: 0.5808519721031189\n","epoch 221 took 9.575414657592773s\n","   train loss: 0.021094314754009247\n","   train auc: 0.9623996019363403\n","   train aupr: 0.6823028922080994\n","Testing\n","   test loss: 0.03871157020330429\n","   test auc: 0.9013262987136841\n","   test aupr: 0.581183671951294\n","epoch 222 took 9.580777883529663s\n","   train loss: 0.020559310913085938\n","   train auc: 0.9624717235565186\n","   train aupr: 0.6826417446136475\n","Testing\n","   test loss: 0.044834114611148834\n","   test auc: 0.9014180898666382\n","   test aupr: 0.581405520439148\n","epoch 223 took 9.571816444396973s\n","   train loss: 0.020518306642770767\n","   train auc: 0.9625381827354431\n","   train aupr: 0.68294358253479\n","Testing\n","   test loss: 0.045673731714487076\n","   test auc: 0.9014496207237244\n","   test aupr: 0.5815062522888184\n","epoch 224 took 9.585206031799316s\n","   train loss: 0.028035087510943413\n","   train auc: 0.962601363658905\n","   train aupr: 0.6832410097122192\n","Testing\n","   test loss: 0.080551378428936\n","   test auc: 0.9011062383651733\n","   test aupr: 0.5785483121871948\n","epoch 225 took 9.518039464950562s\n","   train loss: 0.04355941712856293\n","   train auc: 0.9623405337333679\n","   train aupr: 0.6821057200431824\n","Testing\n","   test loss: 0.0489799939095974\n","   test auc: 0.9007772207260132\n","   test aupr: 0.5757244229316711\n","epoch 226 took 9.533626794815063s\n","   train loss: 0.027531979605555534\n","   train auc: 0.9622260928153992\n","   train aupr: 0.681458592414856\n","Testing\n","   test loss: 0.043247099965810776\n","   test auc: 0.9008301496505737\n","   test aupr: 0.5756588578224182\n","epoch 227 took 9.541533946990967s\n","   train loss: 0.02477036416530609\n","   train auc: 0.9622392058372498\n","   train aupr: 0.6813563704490662\n","Testing\n","   test loss: 0.050439901649951935\n","   test auc: 0.9008513689041138\n","   test aupr: 0.5756380558013916\n","epoch 228 took 9.546177387237549s\n","   train loss: 0.023847557604312897\n","   train auc: 0.9622645974159241\n","   train aupr: 0.6813387274742126\n","Testing\n","   test loss: 0.04424215853214264\n","   test auc: 0.9008657932281494\n","   test aupr: 0.5755943655967712\n","epoch 229 took 9.583741188049316s\n","   train loss: 0.022954560816287994\n","   train auc: 0.9622981548309326\n","   train aupr: 0.6813881993293762\n","Testing\n","   test loss: 0.044534437358379364\n","   test auc: 0.9009393453598022\n","   test aupr: 0.5756751298904419\n","epoch 230 took 9.550853967666626s\n","   train loss: 0.023080667480826378\n","   train auc: 0.9623494148254395\n","   train aupr: 0.6815512776374817\n","Testing\n","   test loss: 0.0469580739736557\n","   test auc: 0.9009866118431091\n","   test aupr: 0.5757230520248413\n","epoch 231 took 9.566718816757202s\n","   train loss: 0.022451946511864662\n","   train auc: 0.96238112449646\n","   train aupr: 0.6816189885139465\n","Testing\n","   test loss: 0.043889448046684265\n","   test auc: 0.9010297060012817\n","   test aupr: 0.575793981552124\n","epoch 232 took 9.569337606430054s\n","   train loss: 0.02194286696612835\n","   train auc: 0.9624356031417847\n","   train aupr: 0.6818289160728455\n","Testing\n","   test loss: 0.05000009015202522\n","   test auc: 0.9010297060012817\n","   test aupr: 0.5758368372917175\n","epoch 233 took 9.560795783996582s\n","   train loss: 0.021982943639159203\n","   train auc: 0.9624866843223572\n","   train aupr: 0.6820027232170105\n","Testing\n","   test loss: 0.044780828058719635\n","   test auc: 0.901027262210846\n","   test aupr: 0.5758753418922424\n","epoch 234 took 9.566944360733032s\n","   train loss: 0.02087860368192196\n","   train auc: 0.9625434875488281\n","   train aupr: 0.6822227239608765\n","Testing\n","   test loss: 0.04148097336292267\n","   test auc: 0.901098370552063\n","   test aupr: 0.5760505199432373\n","epoch 235 took 9.571529388427734s\n","   train loss: 0.02058054506778717\n","   train auc: 0.9626064896583557\n","   train aupr: 0.6824876070022583\n","Testing\n","   test loss: 0.056142907589673996\n","   test auc: 0.9010527729988098\n","   test aupr: 0.576041579246521\n","epoch 236 took 9.560070991516113s\n","   train loss: 0.020389577373862267\n","   train auc: 0.9626758098602295\n","   train aupr: 0.682805597782135\n","Testing\n","   test loss: 0.05094842612743378\n","   test auc: 0.9009252786636353\n","   test aupr: 0.5759104490280151\n","epoch 237 took 9.55408000946045s\n","   train loss: 0.019966531544923782\n","   train auc: 0.9627506732940674\n","   train aupr: 0.6831599473953247\n","Testing\n","   test loss: 0.046244267374277115\n","   test auc: 0.900891900062561\n","   test aupr: 0.5759561657905579\n","epoch 238 took 9.572191953659058s\n","   train loss: 0.0199031513184309\n","   train auc: 0.9628220200538635\n","   train aupr: 0.6834916472434998\n","Testing\n","   test loss: 0.04099677503108978\n","   test auc: 0.9009519219398499\n","   test aupr: 0.5761449337005615\n","epoch 239 took 9.573862791061401s\n","   train loss: 0.019693246111273766\n","   train auc: 0.9628906846046448\n","   train aupr: 0.6838080286979675\n","Testing\n","   test loss: 0.04857463017106056\n","   test auc: 0.900989294052124\n","   test aupr: 0.5762689113616943\n","epoch 240 took 9.578521013259888s\n","   train loss: 0.019836368039250374\n","   train auc: 0.9629676342010498\n","   train aupr: 0.6841796040534973\n","Testing\n","   test loss: 0.05569928511977196\n","   test auc: 0.9008885622024536\n","   test aupr: 0.5761610269546509\n","epoch 241 took 9.576918840408325s\n","   train loss: 0.019673652946949005\n","   train auc: 0.9630336761474609\n","   train aupr: 0.684478223323822\n","Testing\n","   test loss: 0.0473279170691967\n","   test auc: 0.900799036026001\n","   test aupr: 0.576115071773529\n","epoch 242 took 9.58132266998291s\n","   train loss: 0.020091628655791283\n","   train auc: 0.9631134271621704\n","   train aupr: 0.6848601698875427\n","Testing\n","   test loss: 0.04232990741729736\n","   test auc: 0.9008356928825378\n","   test aupr: 0.5763199925422668\n","epoch 243 took 9.578114748001099s\n","   train loss: 0.019435105845332146\n","   train auc: 0.9631775617599487\n","   train aupr: 0.685118556022644\n","Testing\n","   test loss: 0.04157666862010956\n","   test auc: 0.9009093046188354\n","   test aupr: 0.576590895652771\n","epoch 244 took 9.558310508728027s\n","   train loss: 0.019082751125097275\n","   train auc: 0.9632559418678284\n","   train aupr: 0.6855403184890747\n","Testing\n","   test loss: 0.03554309159517288\n","   test auc: 0.9010428786277771\n","   test aupr: 0.576948344707489\n","epoch 245 took 9.567171573638916s\n","   train loss: 0.019526494666934013\n","   train auc: 0.9633303880691528\n","   train aupr: 0.6859091520309448\n","Testing\n","   test loss: 0.03652326017618179\n","   test auc: 0.9012143611907959\n","   test aupr: 0.5773884654045105\n","epoch 246 took 9.561133861541748s\n","   train loss: 0.022754814475774765\n","   train auc: 0.9633753895759583\n","   train aupr: 0.6860644817352295\n","Testing\n","   test loss: 0.043214913457632065\n","   test auc: 0.9013218283653259\n","   test aupr: 0.5777102708816528\n","epoch 247 took 9.557795763015747s\n","   train loss: 0.020023295655846596\n","   train auc: 0.9634301662445068\n","   train aupr: 0.6863337755203247\n","Testing\n","   test loss: 0.042048726230859756\n","   test auc: 0.9013957381248474\n","   test aupr: 0.5779203772544861\n","epoch 248 took 9.541780233383179s\n","   train loss: 0.019996771588921547\n","   train auc: 0.963498055934906\n","   train aupr: 0.6866474151611328\n","Testing\n","   test loss: 0.05166931822896004\n","   test auc: 0.9013949036598206\n","   test aupr: 0.57798832654953\n","epoch 249 took 9.545833826065063s\n","   train loss: 0.01966402307152748\n","   train auc: 0.9635596871376038\n","   train aupr: 0.6869350075721741\n","Testing\n","   test loss: 0.052264969795942307\n","   test auc: 0.9013046622276306\n","   test aupr: 0.5779261589050293\n","epoch 250 took 9.60117506980896s\n","   train loss: 0.01917780190706253\n","   train auc: 0.9636340737342834\n","   train aupr: 0.6873122453689575\n","Testing\n","   test loss: 0.045715589076280594\n","   test auc: 0.901274561882019\n","   test aupr: 0.5779613852500916\n","epoch 251 took 9.55939507484436s\n","   train loss: 0.01852189004421234\n","   train auc: 0.9637088775634766\n","   train aupr: 0.6876856684684753\n","Testing\n","   test loss: 0.04295298084616661\n","   test auc: 0.9013181328773499\n","   test aupr: 0.578133225440979\n","epoch 252 took 9.547083854675293s\n","   train loss: 0.018424753099679947\n","   train auc: 0.9637898206710815\n","   train aupr: 0.688130259513855\n","Testing\n","   test loss: 0.0453316792845726\n","   test auc: 0.9013598561286926\n","   test aupr: 0.5783002972602844\n","epoch 253 took 9.551788330078125s\n","   train loss: 0.01832514815032482\n","   train auc: 0.9638679027557373\n","   train aupr: 0.6885419487953186\n","Testing\n","   test loss: 0.044549744576215744\n","   test auc: 0.9013891816139221\n","   test aupr: 0.5784514546394348\n","epoch 254 took 9.563114166259766s\n","   train loss: 0.018216794356703758\n","   train auc: 0.96394944190979\n","   train aupr: 0.6889938712120056\n","Testing\n","   test loss: 0.047271728515625\n","   test auc: 0.9013997316360474\n","   test aupr: 0.5785737633705139\n","epoch 255 took 9.56356143951416s\n","   train loss: 0.01821991056203842\n","   train auc: 0.9640175700187683\n","   train aupr: 0.6893336176872253\n","Testing\n","   test loss: 0.0476340726017952\n","   test auc: 0.9013850688934326\n","   test aupr: 0.5786471962928772\n","epoch 256 took 9.571667194366455s\n","   train loss: 0.018268533051013947\n","   train auc: 0.9640960693359375\n","   train aupr: 0.6897667050361633\n","Testing\n","   test loss: 0.050841156393289566\n","   test auc: 0.9013432264328003\n","   test aupr: 0.5786710381507874\n","epoch 257 took 9.560638189315796s\n","   train loss: 0.018108196556568146\n","   train auc: 0.9641732573509216\n","   train aupr: 0.6901831030845642\n","Testing\n","   test loss: 0.04518015682697296\n","   test auc: 0.9013249278068542\n","   test aupr: 0.5787310600280762\n","epoch 258 took 9.564175605773926s\n","   train loss: 0.01805184595286846\n","   train auc: 0.964248538017273\n","   train aupr: 0.6905903816223145\n","Testing\n","   test loss: 0.046590205281972885\n","   test auc: 0.9013389945030212\n","   test aupr: 0.578856885433197\n","epoch 259 took 9.571356058120728s\n","   train loss: 0.018155904486775398\n","   train auc: 0.9643298387527466\n","   train aupr: 0.691059947013855\n","Testing\n","   test loss: 0.04885030537843704\n","   test auc: 0.9013246297836304\n","   test aupr: 0.5789278149604797\n","epoch 260 took 9.566588401794434s\n","   train loss: 0.018036242574453354\n","   train auc: 0.9644065499305725\n","   train aupr: 0.6914740204811096\n","Testing\n","   test loss: 0.046263858675956726\n","   test auc: 0.9013115167617798\n","   test aupr: 0.5790119767189026\n","epoch 261 took 9.572527170181274s\n","   train loss: 0.017960814759135246\n","   train auc: 0.964482843875885\n","   train aupr: 0.6918929815292358\n","Testing\n","   test loss: 0.04825570061802864\n","   test auc: 0.9013020992279053\n","   test aupr: 0.5791135430335999\n","epoch 262 took 9.578490972518921s\n","   train loss: 0.018084673210978508\n","   train auc: 0.9645577669143677\n","   train aupr: 0.692305326461792\n","Testing\n","   test loss: 0.04390901327133179\n","   test auc: 0.9013132452964783\n","   test aupr: 0.5792470574378967\n","epoch 263 took 9.585193872451782s\n","   train loss: 0.017985602840781212\n","   train auc: 0.9646317958831787\n","   train aupr: 0.6927025318145752\n","Testing\n","   test loss: 0.045227885246276855\n","   test auc: 0.9013459086418152\n","   test aupr: 0.5794183015823364\n","epoch 264 took 9.549679279327393s\n","   train loss: 0.01791187934577465\n","   train auc: 0.9647051692008972\n","   train aupr: 0.6931079030036926\n","Testing\n","   test loss: 0.04696232080459595\n","   test auc: 0.9013550877571106\n","   test aupr: 0.5795506238937378\n","epoch 265 took 9.579914331436157s\n","   train loss: 0.017855381593108177\n","   train auc: 0.9647827744483948\n","   train aupr: 0.6935550570487976\n","Testing\n","   test loss: 0.04752422869205475\n","   test auc: 0.9013474583625793\n","   test aupr: 0.5796473622322083\n","epoch 266 took 9.583353757858276s\n","   train loss: 0.017970314249396324\n","   train auc: 0.964857816696167\n","   train aupr: 0.6939688324928284\n","Testing\n","   test loss: 0.04633995145559311\n","   test auc: 0.9013485312461853\n","   test aupr: 0.5797485709190369\n","epoch 267 took 9.57962441444397s\n","   train loss: 0.01782742328941822\n","   train auc: 0.9649320244789124\n","   train aupr: 0.6943771839141846\n","Testing\n","   test loss: 0.04780438169836998\n","   test auc: 0.9013457894325256\n","   test aupr: 0.5798570513725281\n","epoch 268 took 9.566478967666626s\n","   train loss: 0.017915068194270134\n","   train auc: 0.9650044441223145\n","   train aupr: 0.6947783827781677\n","Testing\n","   test loss: 0.04762853682041168\n","   test auc: 0.9013333916664124\n","   test aupr: 0.5799437165260315\n","epoch 269 took 9.587320804595947s\n","   train loss: 0.017877427861094475\n","   train auc: 0.9650812745094299\n","   train aupr: 0.6952087879180908\n","Testing\n","   test loss: 0.043290793895721436\n","   test auc: 0.9013558030128479\n","   test aupr: 0.5800856947898865\n","epoch 270 took 9.574541330337524s\n","   train loss: 0.017861977219581604\n","   train auc: 0.965149998664856\n","   train aupr: 0.6955952644348145\n","Testing\n","   test loss: 0.0454435870051384\n","   test auc: 0.9013919830322266\n","   test aupr: 0.5802507996559143\n","epoch 271 took 9.579702138900757s\n","   train loss: 0.017728395760059357\n","   train auc: 0.9652218818664551\n","   train aupr: 0.6960083246231079\n","Testing\n","   test loss: 0.044773124158382416\n","   test auc: 0.9014186859130859\n","   test aupr: 0.5803949236869812\n","epoch 272 took 9.593995332717896s\n","   train loss: 0.01767893321812153\n","   train auc: 0.9652945399284363\n","   train aupr: 0.6964123845100403\n","Testing\n","   test loss: 0.046738941222429276\n","   test auc: 0.901435911655426\n","   test aupr: 0.5805374383926392\n","epoch 273 took 9.59072232246399s\n","   train loss: 0.017613733187317848\n","   train auc: 0.9653640389442444\n","   train aupr: 0.6967822313308716\n","Testing\n","   test loss: 0.04484529793262482\n","   test auc: 0.9014519453048706\n","   test aupr: 0.5806878805160522\n","epoch 274 took 9.58167314529419s\n","   train loss: 0.017680563032627106\n","   train auc: 0.9654362797737122\n","   train aupr: 0.6971843838691711\n","Testing\n","   test loss: 0.04682159051299095\n","   test auc: 0.9014635682106018\n","   test aupr: 0.5808408856391907\n","epoch 275 took 9.574078798294067s\n","   train loss: 0.017681535333395004\n","   train auc: 0.9655073881149292\n","   train aupr: 0.6975821852684021\n","Testing\n","   test loss: 0.045770447701215744\n","   test auc: 0.9014701843261719\n","   test aupr: 0.5809756517410278\n","epoch 276 took 9.585414171218872s\n","   train loss: 0.017634181305766106\n","   train auc: 0.9655762910842896\n","   train aupr: 0.6979730129241943\n","Testing\n","   test loss: 0.044218700379133224\n","   test auc: 0.901497483253479\n","   test aupr: 0.5811337232589722\n","epoch 277 took 9.561426639556885s\n","   train loss: 0.017653368413448334\n","   train auc: 0.9656472206115723\n","   train aupr: 0.6983795762062073\n","Testing\n","   test loss: 0.048607587814331055\n","   test auc: 0.9015036225318909\n","   test aupr: 0.5812502503395081\n","epoch 278 took 9.56270718574524s\n","   train loss: 0.01758185587823391\n","   train auc: 0.9657149910926819\n","   train aupr: 0.6987606883049011\n","Testing\n","   test loss: 0.04851794242858887\n","   test auc: 0.9014793038368225\n","   test aupr: 0.5813156962394714\n","epoch 279 took 9.575084924697876s\n","   train loss: 0.017497191205620766\n","   train auc: 0.9657835364341736\n","   train aupr: 0.6991389989852905\n","Testing\n","   test loss: 0.04769347235560417\n","   test auc: 0.9014575481414795\n","   test aupr: 0.5813909769058228\n","epoch 280 took 9.584778547286987s\n","   train loss: 0.017483225092291832\n","   train auc: 0.965858519077301\n","   train aupr: 0.6995822191238403\n","Testing\n","   test loss: 0.044169679284095764\n","   test auc: 0.901462972164154\n","   test aupr: 0.5815297961235046\n","epoch 281 took 9.57106900215149s\n","   train loss: 0.017431341111660004\n","   train auc: 0.9659233093261719\n","   train aupr: 0.6999402046203613\n","Testing\n","   test loss: 0.04732871428132057\n","   test auc: 0.9014723896980286\n","   test aupr: 0.5816869139671326\n","epoch 282 took 9.585733413696289s\n","   train loss: 0.01748533733189106\n","   train auc: 0.9659976959228516\n","   train aupr: 0.7003768086433411\n","Testing\n","   test loss: 0.04642276093363762\n","   test auc: 0.9014707207679749\n","   test aupr: 0.5818146467208862\n","epoch 283 took 9.599651336669922s\n","   train loss: 0.017360879108309746\n","   train auc: 0.9660652875900269\n","   train aupr: 0.7007620334625244\n","Testing\n","   test loss: 0.04826180636882782\n","   test auc: 0.9014613032341003\n","   test aupr: 0.5819268822669983\n","epoch 284 took 9.58046579360962s\n","   train loss: 0.017438091337680817\n","   train auc: 0.9661351442337036\n","   train aupr: 0.7011798620223999\n","Testing\n","   test loss: 0.04667763411998749\n","   test auc: 0.9014536738395691\n","   test aupr: 0.5820379853248596\n","epoch 285 took 9.576905965805054s\n","   train loss: 0.017329050227999687\n","   train auc: 0.9662009477615356\n","   train aupr: 0.7015419006347656\n","Testing\n","   test loss: 0.045407649129629135\n","   test auc: 0.9014638662338257\n","   test aupr: 0.5821788311004639\n","epoch 286 took 9.582152843475342s\n","   train loss: 0.017435844987630844\n","   train auc: 0.96626877784729\n","   train aupr: 0.7019379138946533\n","Testing\n","   test loss: 0.05088454857468605\n","   test auc: 0.9014451503753662\n","   test aupr: 0.5822558999061584\n","epoch 287 took 9.58139944076538s\n","   train loss: 0.017368335276842117\n","   train auc: 0.9663322567939758\n","   train aupr: 0.7022884488105774\n","Testing\n","   test loss: 0.045479290187358856\n","   test auc: 0.9014306664466858\n","   test aupr: 0.5823394060134888\n","epoch 288 took 9.580062866210938s\n","   train loss: 0.01729951612651348\n","   train auc: 0.9664000868797302\n","   train aupr: 0.7026822566986084\n","Testing\n","   test loss: 0.04674733430147171\n","   test auc: 0.9014387726783752\n","   test aupr: 0.582474946975708\n","epoch 289 took 9.572247505187988s\n","   train loss: 0.017228275537490845\n","   train auc: 0.9664679765701294\n","   train aupr: 0.7030931115150452\n","Testing\n","   test loss: 0.0433015450835228\n","   test auc: 0.9014651775360107\n","   test aupr: 0.5826504230499268\n","epoch 290 took 9.588606834411621s\n","   train loss: 0.017241213470697403\n","   train auc: 0.9665375351905823\n","   train aupr: 0.7034876942634583\n","Testing\n","   test loss: 0.044906556606292725\n","   test auc: 0.9014995098114014\n","   test aupr: 0.5828574299812317\n","epoch 291 took 9.593293190002441s\n","   train loss: 0.017222091555595398\n","   train auc: 0.9666004180908203\n","   train aupr: 0.7038300633430481\n","Testing\n","   test loss: 0.05197811871767044\n","   test auc: 0.9014747142791748\n","   test aupr: 0.5829530358314514\n","epoch 292 took 9.586124420166016s\n","   train loss: 0.017151467502117157\n","   train auc: 0.966669499874115\n","   train aupr: 0.7042452692985535\n","Testing\n","   test loss: 0.047862883657217026\n","   test auc: 0.9014317989349365\n","   test aupr: 0.5829948782920837\n","epoch 293 took 9.5880765914917s\n","   train loss: 0.017102502286434174\n","   train auc: 0.9667314887046814\n","   train aupr: 0.704591691493988\n","Testing\n","   test loss: 0.04707087576389313\n","   test auc: 0.9014219641685486\n","   test aupr: 0.5830971002578735\n","epoch 294 took 9.585045099258423s\n","   train loss: 0.01738567277789116\n","   train auc: 0.9667993187904358\n","   train aupr: 0.7049975395202637\n","Testing\n","   test loss: 0.04508042335510254\n","   test auc: 0.901432991027832\n","   test aupr: 0.5832345485687256\n","epoch 295 took 9.589224338531494s\n","   train loss: 0.017283735796809196\n","   train auc: 0.9668631553649902\n","   train aupr: 0.7053611874580383\n","Testing\n","   test loss: 0.05069582909345627\n","   test auc: 0.9014159440994263\n","   test aupr: 0.5833237171173096\n","epoch 296 took 9.60402512550354s\n","   train loss: 0.017031028866767883\n","   train auc: 0.9669238924980164\n","   train aupr: 0.7057042717933655\n","Testing\n","   test loss: 0.04839455708861351\n","   test auc: 0.9013803005218506\n","   test aupr: 0.5833916068077087\n","epoch 297 took 9.586933135986328s\n","   train loss: 0.01708904094994068\n","   train auc: 0.9669930934906006\n","   train aupr: 0.70613032579422\n","Testing\n","   test loss: 0.05221380665898323\n","   test auc: 0.9013347625732422\n","   test aupr: 0.5834484696388245\n","epoch 298 took 9.619088411331177s\n","   train loss: 0.017163632437586784\n","   train auc: 0.9670581221580505\n","   train aupr: 0.706514835357666\n","Testing\n","   test loss: 0.04663706198334694\n","   test auc: 0.9013034105300903\n","   test aupr: 0.5835212469100952\n","epoch 299 took 9.60733437538147s\n","   train loss: 0.017004022374749184\n","   train auc: 0.9671221375465393\n","   train aupr: 0.70688396692276\n","Testing\n","   test loss: 0.0476946197450161\n","   test auc: 0.9012995958328247\n","   test aupr: 0.5836400389671326\n","epoch 300 took 9.588057279586792s\n","   train loss: 0.016953658312559128\n","   train auc: 0.9671853184700012\n","   train aupr: 0.7072611451148987\n","Testing\n","   test loss: 0.04675789177417755\n","   test auc: 0.9012959003448486\n","   test aupr: 0.583756685256958\n","epoch 301 took 9.588659763336182s\n","   train loss: 0.016772350296378136\n","   train auc: 0.9672473073005676\n","   train aupr: 0.707618772983551\n","Testing\n","   test loss: 0.04760744422674179\n","   test auc: 0.9012924432754517\n","   test aupr: 0.5838916301727295\n","epoch 302 took 9.590386867523193s\n","   train loss: 0.016815586015582085\n","   train auc: 0.9673082828521729\n","   train aupr: 0.7079689502716064\n","Testing\n","   test loss: 0.043676599860191345\n","   test auc: 0.90130615234375\n","   test aupr: 0.5840553045272827\n","epoch 303 took 9.606035709381104s\n","   train loss: 0.016874458640813828\n","   train auc: 0.9673771858215332\n","   train aupr: 0.7083970308303833\n","Testing\n","   test loss: 0.047388795763254166\n","   test auc: 0.9013198614120483\n","   test aupr: 0.5842010974884033\n","epoch 304 took 9.588430881500244s\n","   train loss: 0.017004206776618958\n","   train auc: 0.9674393534660339\n","   train aupr: 0.7087643146514893\n","Testing\n","   test loss: 0.05752173811197281\n","   test auc: 0.9012523889541626\n","   test aupr: 0.584213376045227\n","epoch 305 took 9.602701663970947s\n","   train loss: 0.016827834770083427\n","   train auc: 0.9674988985061646\n","   train aupr: 0.7091106176376343\n","Testing\n","   test loss: 0.05151183158159256\n","   test auc: 0.9011603593826294\n","   test aupr: 0.584193766117096\n","epoch 306 took 9.614725828170776s\n","   train loss: 0.01679338701069355\n","   train auc: 0.9675542116165161\n","   train aupr: 0.7094165682792664\n","Testing\n","   test loss: 0.045341167598962784\n","   test auc: 0.901138424873352\n","   test aupr: 0.5843079090118408\n","epoch 307 took 9.584118366241455s\n","   train loss: 0.016804924234747887\n","   train auc: 0.9676246047019958\n","   train aupr: 0.7098618149757385\n","Testing\n","   test loss: 0.049072422087192535\n","   test auc: 0.9011316299438477\n","   test aupr: 0.5844342708587646\n","epoch 308 took 9.603732824325562s\n","   train loss: 0.01657089591026306\n","   train auc: 0.9676816463470459\n","   train aupr: 0.7101840972900391\n","Testing\n","   test loss: 0.04957272484898567\n","   test auc: 0.9011054635047913\n","   test aupr: 0.5845243334770203\n","epoch 309 took 9.626035451889038s\n","   train loss: 0.016615459695458412\n","   train auc: 0.9677470922470093\n","   train aupr: 0.7105823755264282\n","Testing\n","   test loss: 0.05421563610434532\n","   test auc: 0.9010463953018188\n","   test aupr: 0.5845680832862854\n","epoch 310 took 9.627969026565552s\n","   train loss: 0.016749095171689987\n","   train auc: 0.9678075313568115\n","   train aupr: 0.7109445929527283\n","Testing\n","   test loss: 0.053617868572473526\n","   test auc: 0.9009647369384766\n","   test aupr: 0.5845507383346558\n","epoch 311 took 9.607083559036255s\n","   train loss: 0.01662454754114151\n","   train auc: 0.9678703546524048\n","   train aupr: 0.7113156318664551\n","Testing\n","   test loss: 0.0472574420273304\n","   test auc: 0.9009234309196472\n","   test aupr: 0.5846037864685059\n","epoch 312 took 9.608483076095581s\n","   train loss: 0.016693467274308205\n","   train auc: 0.9679350256919861\n","   train aupr: 0.7117211818695068\n","Testing\n","   test loss: 0.04964835196733475\n","   test auc: 0.9009013772010803\n","   test aupr: 0.5846863389015198\n","epoch 313 took 9.60627031326294s\n","   train loss: 0.01674525812268257\n","   train auc: 0.9679907560348511\n","   train aupr: 0.7120362520217896\n","Testing\n","   test loss: 0.04912577196955681\n","   test auc: 0.9008687138557434\n","   test aupr: 0.584758460521698\n","epoch 314 took 9.59896731376648s\n","   train loss: 0.0164046511054039\n","   train auc: 0.9680542349815369\n","   train aupr: 0.7124351859092712\n","Testing\n","   test loss: 0.04715029150247574\n","   test auc: 0.9008505940437317\n","   test aupr: 0.5848689675331116\n","epoch 315 took 9.600807428359985s\n","   train loss: 0.01651720143854618\n","   train auc: 0.9681136012077332\n","   train aupr: 0.7127860188484192\n","Testing\n","   test loss: 0.04596128687262535\n","   test auc: 0.9008512496948242\n","   test aupr: 0.5850224494934082\n","epoch 316 took 9.597764253616333s\n","   train loss: 0.016473498195409775\n","   train auc: 0.9681763648986816\n","   train aupr: 0.7131766080856323\n","Testing\n","   test loss: 0.04995211213827133\n","   test auc: 0.9008340835571289\n","   test aupr: 0.585159957408905\n","epoch 317 took 9.601207733154297s\n","   train loss: 0.016410572454333305\n","   train auc: 0.968237578868866\n","   train aupr: 0.7135443091392517\n","Testing\n","   test loss: 0.04818059876561165\n","   test auc: 0.9008058309555054\n","   test aupr: 0.5852724313735962\n","epoch 318 took 9.617720365524292s\n","   train loss: 0.016392111778259277\n","   train auc: 0.9682976007461548\n","   train aupr: 0.7138985395431519\n","Testing\n","   test loss: 0.050649162381887436\n","   test auc: 0.9007779359817505\n","   test aupr: 0.5853689908981323\n","epoch 319 took 9.604715347290039s\n","   train loss: 0.01631176844239235\n","   train auc: 0.9683558344841003\n","   train aupr: 0.7142593264579773\n","Testing\n","   test loss: 0.044264055788517\n","   test auc: 0.9007746577262878\n","   test aupr: 0.5855004787445068\n","epoch 320 took 9.600344181060791s\n","   train loss: 0.01631510630249977\n","   train auc: 0.9684157967567444\n","   train aupr: 0.7146214246749878\n","Testing\n","   test loss: 0.049574218690395355\n","   test auc: 0.9007740020751953\n","   test aupr: 0.5856430530548096\n","epoch 321 took 9.593466520309448s\n","   train loss: 0.016520485281944275\n","   train auc: 0.9684749841690063\n","   train aupr: 0.7149821519851685\n","Testing\n","   test loss: 0.052713096141815186\n","   test auc: 0.9007260203361511\n","   test aupr: 0.5856871604919434\n","epoch 322 took 9.606654405593872s\n","   train loss: 0.01653999648988247\n","   train auc: 0.9685322642326355\n","   train aupr: 0.7153208255767822\n","Testing\n","   test loss: 0.04449930414557457\n","   test auc: 0.900710940361023\n","   test aupr: 0.5857961773872375\n","epoch 323 took 9.596196174621582s\n","   train loss: 0.01632206328213215\n","   train auc: 0.968595027923584\n","   train aupr: 0.7157134413719177\n","Testing\n","   test loss: 0.04308746010065079\n","   test auc: 0.9007466435432434\n","   test aupr: 0.5860174894332886\n","epoch 324 took 9.591254949569702s\n","   train loss: 0.016173934563994408\n","   train auc: 0.9686501026153564\n","   train aupr: 0.7160422801971436\n","Testing\n","   test loss: 0.05001755803823471\n","   test auc: 0.9007521867752075\n","   test aupr: 0.5861802101135254\n","epoch 325 took 9.614386558532715s\n","   train loss: 0.01616114005446434\n","   train auc: 0.9687083959579468\n","   train aupr: 0.7163986563682556\n","Testing\n","   test loss: 0.0523027740418911\n","   test auc: 0.9007098078727722\n","   test aupr: 0.5862431526184082\n","epoch 326 took 9.597179889678955s\n","   train loss: 0.01619335077702999\n","   train auc: 0.9687694907188416\n","   train aupr: 0.7167770266532898\n","Testing\n","   test loss: 0.04868543893098831\n","   test auc: 0.9006733298301697\n","   test aupr: 0.5863276124000549\n","epoch 327 took 9.590067386627197s\n","   train loss: 0.016192497685551643\n","   train auc: 0.9688270092010498\n","   train aupr: 0.7171143293380737\n","Testing\n","   test loss: 0.049003858119249344\n","   test auc: 0.9006540775299072\n","   test aupr: 0.5864536762237549\n","epoch 328 took 9.595581293106079s\n","   train loss: 0.01611444726586342\n","   train auc: 0.9688854217529297\n","   train aupr: 0.7174824476242065\n","Testing\n","   test loss: 0.05144760385155678\n","   test auc: 0.9006208181381226\n","   test aupr: 0.5865454077720642\n","epoch 329 took 9.605100631713867s\n","   train loss: 0.01602359674870968\n","   train auc: 0.9689433574676514\n","   train aupr: 0.7178341746330261\n","Testing\n","   test loss: 0.05020345747470856\n","   test auc: 0.9005829691886902\n","   test aupr: 0.5866268873214722\n","epoch 330 took 9.598296165466309s\n","   train loss: 0.016060305759310722\n","   train auc: 0.9690002799034119\n","   train aupr: 0.7181796431541443\n","Testing\n","   test loss: 0.05735810101032257\n","   test auc: 0.9005135297775269\n","   test aupr: 0.5866381525993347\n","epoch 331 took 9.582941770553589s\n","   train loss: 0.0160282664000988\n","   train auc: 0.9690585136413574\n","   train aupr: 0.718539834022522\n","Testing\n","   test loss: 0.047435954213142395\n","   test auc: 0.9004573225975037\n","   test aupr: 0.5866774320602417\n","epoch 332 took 9.593437910079956s\n","   train loss: 0.016061076894402504\n","   train auc: 0.9691181778907776\n","   train aupr: 0.7189069390296936\n","Testing\n","   test loss: 0.055184345692396164\n","   test auc: 0.9004124999046326\n","   test aupr: 0.5867477059364319\n","epoch 333 took 9.595808982849121s\n","   train loss: 0.01590680330991745\n","   train auc: 0.9691733121871948\n","   train aupr: 0.7192286252975464\n","Testing\n","   test loss: 0.04396749287843704\n","   test auc: 0.9003873467445374\n","   test aupr: 0.5868569016456604\n","epoch 334 took 9.607182741165161s\n","   train loss: 0.01604793779551983\n","   train auc: 0.9692339301109314\n","   train aupr: 0.7196231484413147\n","Testing\n","   test loss: 0.05149758234620094\n","   test auc: 0.9003822803497314\n","   test aupr: 0.5870161652565002\n","epoch 335 took 9.61359453201294s\n","   train loss: 0.015921959653496742\n","   train auc: 0.9692885875701904\n","   train aupr: 0.7199576497077942\n","Testing\n","   test loss: 0.05413041263818741\n","   test auc: 0.9003270268440247\n","   test aupr: 0.5870659351348877\n","epoch 336 took 9.629565000534058s\n","   train loss: 0.016095144674181938\n","   train auc: 0.9693448543548584\n","   train aupr: 0.7203089594841003\n","Testing\n","   test loss: 0.05523692071437836\n","   test auc: 0.9002475142478943\n","   test aupr: 0.5870699882507324\n","epoch 337 took 9.619481325149536s\n","   train loss: 0.016122808679938316\n","   train auc: 0.9693989157676697\n","   train aupr: 0.7206425070762634\n","Testing\n","   test loss: 0.050374586135149\n","   test auc: 0.9001887440681458\n","   test aupr: 0.587133526802063\n","epoch 338 took 9.60239052772522s\n","   train loss: 0.015845533460378647\n","   train auc: 0.969453752040863\n","   train aupr: 0.720974326133728\n","Testing\n","   test loss: 0.051082249730825424\n","   test auc: 0.9001531004905701\n","   test aupr: 0.5872264504432678\n","epoch 339 took 9.613895416259766s\n","   train loss: 0.015899838879704475\n","   train auc: 0.9695110321044922\n","   train aupr: 0.7213394045829773\n","Testing\n","   test loss: 0.05077887326478958\n","   test auc: 0.9001171588897705\n","   test aupr: 0.5873132348060608\n","epoch 340 took 9.610801935195923s\n","   train loss: 0.016097541898489\n","   train auc: 0.9695687890052795\n","   train aupr: 0.7217069268226624\n","Testing\n","   test loss: 0.05751959607005119\n","   test auc: 0.9000498652458191\n","   test aupr: 0.5873535871505737\n","epoch 341 took 9.592189311981201s\n","   train loss: 0.015972420573234558\n","   train auc: 0.9696176052093506\n","   train aupr: 0.7219969630241394\n","Testing\n","   test loss: 0.05336984246969223\n","   test auc: 0.8999684453010559\n","   test aupr: 0.5873720645904541\n","epoch 342 took 9.597923755645752s\n","   train loss: 0.015663765370845795\n","   train auc: 0.9696749448776245\n","   train aupr: 0.7223592400550842\n","Testing\n","   test loss: 0.05162449926137924\n","   test auc: 0.8999165892601013\n","   test aupr: 0.5874367952346802\n","epoch 343 took 9.606750965118408s\n","   train loss: 0.015768246725201607\n","   train auc: 0.9697290062904358\n","   train aupr: 0.7226923704147339\n","Testing\n","   test loss: 0.04420609772205353\n","   test auc: 0.8999113440513611\n","   test aupr: 0.5875855088233948\n","epoch 344 took 9.610832691192627s\n","   train loss: 0.01581510156393051\n","   train auc: 0.9697842597961426\n","   train aupr: 0.7230433225631714\n","Testing\n","   test loss: 0.05581473559141159\n","   test auc: 0.8998817205429077\n","   test aupr: 0.587675929069519\n","epoch 345 took 9.609125137329102s\n","   train loss: 0.01571979746222496\n","   train auc: 0.9698429107666016\n","   train aupr: 0.7234145998954773\n","Testing\n","   test loss: 0.056200310587882996\n","   test auc: 0.8997942209243774\n","   test aupr: 0.5876646637916565\n","epoch 346 took 9.611948251724243s\n","   train loss: 0.01563190296292305\n","   train auc: 0.9698954820632935\n","   train aupr: 0.723735511302948\n","Testing\n","   test loss: 0.04547206684947014\n","   test auc: 0.8997578024864197\n","   test aupr: 0.5877593755722046\n","epoch 347 took 9.617317914962769s\n","   train loss: 0.01577480137348175\n","   train auc: 0.9699498414993286\n","   train aupr: 0.7240718007087708\n","Testing\n","   test loss: 0.053602080792188644\n","   test auc: 0.8997303247451782\n","   test aupr: 0.5878753662109375\n","epoch 348 took 9.610349416732788s\n","   train loss: 0.015655359253287315\n","   train auc: 0.9700081944465637\n","   train aupr: 0.7244465351104736\n","Testing\n","   test loss: 0.053820833563804626\n","   test auc: 0.8996649980545044\n","   test aupr: 0.587922990322113\n","epoch 349 took 9.613462448120117s\n","   train loss: 0.015643110498785973\n","   train auc: 0.970058023929596\n","   train aupr: 0.7247439622879028\n","Testing\n","   test loss: 0.05611123517155647\n","   test auc: 0.8995895385742188\n","   test aupr: 0.5879433155059814\n","epoch 350 took 9.614163637161255s\n","   train loss: 0.01562072616070509\n","   train auc: 0.9701134562492371\n","   train aupr: 0.7250954508781433\n","Testing\n","   test loss: 0.05567947402596474\n","   test auc: 0.8995059132575989\n","   test aupr: 0.5879415273666382\n","epoch 351 took 9.609310865402222s\n","   train loss: 0.01558505930006504\n","   train auc: 0.970166027545929\n","   train aupr: 0.7254206538200378\n","Testing\n","   test loss: 0.04947158321738243\n","   test auc: 0.8994550704956055\n","   test aupr: 0.5880086421966553\n","epoch 352 took 9.607953548431396s\n","   train loss: 0.015650097280740738\n","   train auc: 0.9702171087265015\n","   train aupr: 0.7257287502288818\n","Testing\n","   test loss: 0.05187751352787018\n","   test auc: 0.8994202017784119\n","   test aupr: 0.5881108641624451\n","epoch 353 took 9.610856056213379s\n","   train loss: 0.015432344749569893\n","   train auc: 0.970274806022644\n","   train aupr: 0.7261127829551697\n","Testing\n","   test loss: 0.05274687707424164\n","   test auc: 0.8993685841560364\n","   test aupr: 0.5881685614585876\n","epoch 354 took 9.621883392333984s\n","   train loss: 0.015588020905852318\n","   train auc: 0.9703274369239807\n","   train aupr: 0.7264336943626404\n","Testing\n","   test loss: 0.05680407956242561\n","   test auc: 0.8992924094200134\n","   test aupr: 0.5881825089454651\n","epoch 355 took 9.620184659957886s\n","   train loss: 0.015502684749662876\n","   train auc: 0.9703807830810547\n","   train aupr: 0.7267753481864929\n","Testing\n","   test loss: 0.054304782301187515\n","   test auc: 0.8992106318473816\n","   test aupr: 0.5881853103637695\n","epoch 356 took 9.624313116073608s\n","   train loss: 0.015491398051381111\n","   train auc: 0.9704293608665466\n","   train aupr: 0.7270621657371521\n","Testing\n","   test loss: 0.044026587158441544\n","   test auc: 0.899192214012146\n","   test aupr: 0.5882993936538696\n","epoch 357 took 9.611250162124634s\n","   train loss: 0.015376648865640163\n","   train auc: 0.9704815149307251\n","   train aupr: 0.7273843288421631\n","Testing\n","   test loss: 0.04828331246972084\n","   test auc: 0.8992035388946533\n","   test aupr: 0.5884760618209839\n","epoch 358 took 9.615447282791138s\n","   train loss: 0.015498792752623558\n","   train auc: 0.9705372452735901\n","   train aupr: 0.727739691734314\n","Testing\n","   test loss: 0.04709310084581375\n","   test auc: 0.899200975894928\n","   test aupr: 0.5886346101760864\n","epoch 359 took 9.608258485794067s\n","   train loss: 0.015357482247054577\n","   train auc: 0.9705920219421387\n","   train aupr: 0.7280924320220947\n","Testing\n","   test loss: 0.054328806698322296\n","   test auc: 0.8991662263870239\n","   test aupr: 0.5887427926063538\n","epoch 360 took 9.597331762313843s\n","   train loss: 0.015365910716354847\n","   train auc: 0.9706441164016724\n","   train aupr: 0.7284068465232849\n","Testing\n","   test loss: 0.04570203274488449\n","   test auc: 0.8991403579711914\n","   test aupr: 0.5888543725013733\n","epoch 361 took 9.58290982246399s\n","   train loss: 0.015246045775711536\n","   train auc: 0.9706994295120239\n","   train aupr: 0.728766143321991\n","Testing\n","   test loss: 0.05106949061155319\n","   test auc: 0.89913010597229\n","   test aupr: 0.5889869928359985\n","epoch 362 took 9.607811450958252s\n","   train loss: 0.015209139324724674\n","   train auc: 0.9707521796226501\n","   train aupr: 0.7291004061698914\n","Testing\n","   test loss: 0.049799203872680664\n","   test auc: 0.8991041779518127\n","   test aupr: 0.5890988707542419\n","epoch 363 took 9.627101421356201s\n","   train loss: 0.015225314535200596\n","   train auc: 0.9708012938499451\n","   train aupr: 0.7293902039527893\n","Testing\n","   test loss: 0.05243084579706192\n","   test auc: 0.8990718126296997\n","   test aupr: 0.5892062187194824\n","epoch 364 took 9.619688749313354s\n","   train loss: 0.01515998411923647\n","   train auc: 0.970853865146637\n","   train aupr: 0.7297183871269226\n","Testing\n","   test loss: 0.05204301327466965\n","   test auc: 0.8990288376808167\n","   test aupr: 0.5892931222915649\n","epoch 365 took 9.619972705841064s\n","   train loss: 0.015295549295842648\n","   train auc: 0.9709101915359497\n","   train aupr: 0.7301027774810791\n","Testing\n","   test loss: 0.049209125339984894\n","   test auc: 0.8989989161491394\n","   test aupr: 0.589408278465271\n","epoch 366 took 9.62276816368103s\n","   train loss: 0.01530152652412653\n","   train auc: 0.9709601402282715\n","   train aupr: 0.7304142713546753\n","Testing\n","   test loss: 0.050909969955682755\n","   test auc: 0.8989747762680054\n","   test aupr: 0.589530348777771\n","epoch 367 took 9.648576974868774s\n","   train loss: 0.015218435786664486\n","   train auc: 0.9710091352462769\n","   train aupr: 0.7307133078575134\n","Testing\n","   test loss: 0.05586950108408928\n","   test auc: 0.8989185094833374\n","   test aupr: 0.5895962119102478\n","epoch 368 took 9.627649307250977s\n","   train loss: 0.015175717882812023\n","   train auc: 0.9710598587989807\n","   train aupr: 0.7310351729393005\n","Testing\n","   test loss: 0.04550668224692345\n","   test auc: 0.8988880515098572\n","   test aupr: 0.5897154211997986\n","epoch 369 took 9.645421028137207s\n","   train loss: 0.015078521333634853\n","   train auc: 0.9711080193519592\n","   train aupr: 0.7313298583030701\n","Testing\n","   test loss: 0.05548221245408058\n","   test auc: 0.8988581895828247\n","   test aupr: 0.5898236632347107\n","epoch 370 took 9.630707025527954s\n","   train loss: 0.015028764493763447\n","   train auc: 0.9711632132530212\n","   train aupr: 0.7316968441009521\n","Testing\n","   test loss: 0.05230506509542465\n","   test auc: 0.898796796798706\n","   test aupr: 0.5898595452308655\n","epoch 371 took 9.636114597320557s\n","   train loss: 0.015100921504199505\n","   train auc: 0.9712134599685669\n","   train aupr: 0.7320107221603394\n","Testing\n","   test loss: 0.05328523367643356\n","   test auc: 0.8987451195716858\n","   test aupr: 0.5899195075035095\n","epoch 372 took 9.600743055343628s\n","   train loss: 0.015258767642080784\n","   train auc: 0.9712637662887573\n","   train aupr: 0.732333779335022\n","Testing\n","   test loss: 0.051451973617076874\n","   test auc: 0.8986996412277222\n","   test aupr: 0.5899930000305176\n","epoch 373 took 9.606384754180908s\n","   train loss: 0.015222336165606976\n","   train auc: 0.9713158011436462\n","   train aupr: 0.7326699495315552\n","Testing\n","   test loss: 0.043782200664281845\n","   test auc: 0.8987012505531311\n","   test aupr: 0.5901414155960083\n","epoch 374 took 9.611704349517822s\n","   train loss: 0.015255128033459187\n","   train auc: 0.9713654518127441\n","   train aupr: 0.7329850196838379\n","Testing\n","   test loss: 0.05133003368973732\n","   test auc: 0.8986996412277222\n","   test aupr: 0.5902899503707886\n","epoch 375 took 9.616823673248291s\n","   train loss: 0.01506927702575922\n","   train auc: 0.9714149832725525\n","   train aupr: 0.7332991361618042\n","Testing\n","   test loss: 0.052691392600536346\n","   test auc: 0.8986622095108032\n","   test aupr: 0.5903634428977966\n","epoch 376 took 9.596795320510864s\n","   train loss: 0.015053181909024715\n","   train auc: 0.9714664220809937\n","   train aupr: 0.7336257100105286\n","Testing\n","   test loss: 0.05251655355095863\n","   test auc: 0.8986181020736694\n","   test aupr: 0.5904175043106079\n","epoch 377 took 9.617949724197388s\n","   train loss: 0.015070208348333836\n","   train auc: 0.9715108275413513\n","   train aupr: 0.733887255191803\n","Testing\n","   test loss: 0.04874983802437782\n","   test auc: 0.8985931873321533\n","   test aupr: 0.5905117392539978\n","epoch 378 took 9.630914211273193s\n","   train loss: 0.014799335040152073\n","   train auc: 0.9715610146522522\n","   train aupr: 0.7342182993888855\n","Testing\n","   test loss: 0.04991154000163078\n","   test auc: 0.8985779881477356\n","   test aupr: 0.5906351804733276\n","epoch 379 took 9.611489534378052s\n","   train loss: 0.014998365193605423\n","   train auc: 0.971611738204956\n","   train aupr: 0.7345524430274963\n","Testing\n","   test loss: 0.04841425269842148\n","   test auc: 0.8985647559165955\n","   test aupr: 0.5907710194587708\n","epoch 380 took 9.624723434448242s\n","   train loss: 0.014867802150547504\n","   train auc: 0.9716603755950928\n","   train aupr: 0.7348688840866089\n","Testing\n","   test loss: 0.0531776137650013\n","   test auc: 0.8985362648963928\n","   test aupr: 0.5908678770065308\n","epoch 381 took 9.617187261581421s\n","   train loss: 0.01481457520276308\n","   train auc: 0.971708357334137\n","   train aupr: 0.7351676821708679\n","Testing\n","   test loss: 0.047654151916503906\n","   test auc: 0.8985128998756409\n","   test aupr: 0.5909749865531921\n","epoch 382 took 9.615602493286133s\n","   train loss: 0.014746008440852165\n","   train auc: 0.9717556834220886\n","   train aupr: 0.7354686260223389\n","Testing\n","   test loss: 0.04782940819859505\n","   test auc: 0.8985124826431274\n","   test aupr: 0.5911308526992798\n","epoch 383 took 9.620176076889038s\n","   train loss: 0.014897740446031094\n","   train auc: 0.9718066453933716\n","   train aupr: 0.7358071208000183\n","Testing\n","   test loss: 0.04521995410323143\n","   test auc: 0.8985208868980408\n","   test aupr: 0.5912987589836121\n","epoch 384 took 9.61677598953247s\n","   train loss: 0.014923326671123505\n","   train auc: 0.971855640411377\n","   train aupr: 0.7361301779747009\n","Testing\n","   test loss: 0.051418740302324295\n","   test auc: 0.8985093832015991\n","   test aupr: 0.5914358496665955\n","epoch 385 took 9.623526573181152s\n","   train loss: 0.014824696816504002\n","   train auc: 0.9719055891036987\n","   train aupr: 0.7364528775215149\n","Testing\n","   test loss: 0.05009350925683975\n","   test auc: 0.8984788060188293\n","   test aupr: 0.5915364027023315\n","epoch 386 took 9.603650331497192s\n","   train loss: 0.014772162772715092\n","   train auc: 0.971954345703125\n","   train aupr: 0.73676997423172\n","Testing\n","   test loss: 0.042648494243621826\n","   test auc: 0.89848792552948\n","   test aupr: 0.5916963219642639\n","epoch 387 took 9.6280038356781s\n","   train loss: 0.014816774055361748\n","   train auc: 0.9719995856285095\n","   train aupr: 0.7370541095733643\n","Testing\n","   test loss: 0.056207045912742615\n","   test auc: 0.8984708189964294\n","   test aupr: 0.5918065309524536\n","epoch 388 took 9.60271406173706s\n","   train loss: 0.014781971462070942\n","   train auc: 0.972047746181488\n","   train aupr: 0.7373616099357605\n","Testing\n","   test loss: 0.05333052575588226\n","   test auc: 0.8984056711196899\n","   test aupr: 0.5918421745300293\n","epoch 389 took 9.63301706314087s\n","   train loss: 0.01473158411681652\n","   train auc: 0.9720947742462158\n","   train aupr: 0.7376625537872314\n","Testing\n","   test loss: 0.04284494370222092\n","   test auc: 0.8983973264694214\n","   test aupr: 0.5919868350028992\n","epoch 390 took 9.653881311416626s\n","   train loss: 0.014814726077020168\n","   train auc: 0.9721420407295227\n","   train aupr: 0.7379666566848755\n","Testing\n","   test loss: 0.046190761029720306\n","   test auc: 0.8984187245368958\n","   test aupr: 0.5921815037727356\n","epoch 391 took 9.622750520706177s\n","   train loss: 0.014685732312500477\n","   train auc: 0.9721885323524475\n","   train aupr: 0.7382729053497314\n","Testing\n","   test loss: 0.04683548957109451\n","   test auc: 0.8984262943267822\n","   test aupr: 0.592348039150238\n","epoch 392 took 9.62607216835022s\n","   train loss: 0.014622547663748264\n","   train auc: 0.9722381234169006\n","   train aupr: 0.7386086583137512\n","Testing\n","   test loss: 0.05138486996293068\n","   test auc: 0.8984118103981018\n","   test aupr: 0.5924779176712036\n","epoch 393 took 9.615470886230469s\n","   train loss: 0.014697168953716755\n","   train auc: 0.9722867608070374\n","   train aupr: 0.7389390468597412\n","Testing\n","   test loss: 0.05173175036907196\n","   test auc: 0.8983765244483948\n","   test aupr: 0.5925617218017578\n","epoch 394 took 9.617655754089355s\n","   train loss: 0.014635465107858181\n","   train auc: 0.9723302125930786\n","   train aupr: 0.7392075061798096\n","Testing\n","   test loss: 0.05028151720762253\n","   test auc: 0.898347795009613\n","   test aupr: 0.5926578044891357\n","epoch 395 took 9.60503602027893s\n","   train loss: 0.01461053267121315\n","   train auc: 0.9723774790763855\n","   train aupr: 0.7395157217979431\n","Testing\n","   test loss: 0.0518605038523674\n","   test auc: 0.8983170986175537\n","   test aupr: 0.5927548408508301\n","epoch 396 took 9.616596937179565s\n","   train loss: 0.014871264807879925\n","   train auc: 0.9724217653274536\n","   train aupr: 0.7397977113723755\n","Testing\n","   test loss: 0.058862488716840744\n","   test auc: 0.8982478380203247\n","   test aupr: 0.5927751064300537\n","epoch 397 took 9.640835046768188s\n","   train loss: 0.014754152856767178\n","   train auc: 0.9724665284156799\n","   train aupr: 0.740081250667572\n","Testing\n","   test loss: 0.05084832385182381\n","   test auc: 0.8981872797012329\n","   test aupr: 0.5927839279174805\n","epoch 398 took 9.634097337722778s\n","   train loss: 0.014572282321751118\n","   train auc: 0.972513735294342\n","   train aupr: 0.7403883934020996\n","Testing\n","   test loss: 0.05767268314957619\n","   test auc: 0.8981308341026306\n","   test aupr: 0.5927969813346863\n","epoch 399 took 9.621308326721191s\n","   train loss: 0.014646057039499283\n","   train auc: 0.9725595116615295\n","   train aupr: 0.7406866550445557\n","Testing\n","   test loss: 0.054654303938150406\n","   test auc: 0.8980607986450195\n","   test aupr: 0.5928077101707458\n","epoch 400 took 9.607293128967285s\n","   train loss: 0.014540919102728367\n","   train auc: 0.9726050496101379\n","   train aupr: 0.7409811615943909\n","Testing\n","   test loss: 0.053529027849435806\n","   test auc: 0.898004949092865\n","   test aupr: 0.5928534865379333\n","epoch 401 took 9.614280223846436s\n","   train loss: 0.014475710690021515\n","   train auc: 0.972650945186615\n","   train aupr: 0.7412927150726318\n","Testing\n","   test loss: 0.051936183124780655\n","   test auc: 0.8979600071907043\n","   test aupr: 0.5929223299026489\n","epoch 402 took 9.616607427597046s\n","   train loss: 0.014541806653141975\n","   train auc: 0.9726967811584473\n","   train aupr: 0.7415952086448669\n","Testing\n","   test loss: 0.056505221873521805\n","   test auc: 0.8979027271270752\n","   test aupr: 0.5929668545722961\n","epoch 403 took 9.599852085113525s\n","   train loss: 0.014664209447801113\n","   train auc: 0.9727436900138855\n","   train aupr: 0.7419048547744751\n","Testing\n","   test loss: 0.04799683764576912\n","   test auc: 0.8978637456893921\n","   test aupr: 0.5930423140525818\n","epoch 404 took 9.631212711334229s\n","   train loss: 0.014938435517251492\n","   train auc: 0.9727886319160461\n","   train aupr: 0.7421939373016357\n","Testing\n","   test loss: 0.05137735232710838\n","   test auc: 0.8978413939476013\n","   test aupr: 0.5931554436683655\n","epoch 405 took 9.643135786056519s\n","   train loss: 0.014640280045568943\n","   train auc: 0.9728299379348755\n","   train aupr: 0.7424479126930237\n","Testing\n","   test loss: 0.05305931717157364\n","   test auc: 0.8977982997894287\n","   test aupr: 0.5932285785675049\n","epoch 406 took 9.611338138580322s\n","   train loss: 0.01443689875304699\n","   train auc: 0.9728752970695496\n","   train aupr: 0.7427492737770081\n","Testing\n","   test loss: 0.053352199494838715\n","   test auc: 0.8977516889572144\n","   test aupr: 0.5932918190956116\n","epoch 407 took 9.611212491989136s\n","   train loss: 0.014436716213822365\n","   train auc: 0.972920835018158\n","   train aupr: 0.7430543303489685\n","Testing\n","   test loss: 0.052548862993717194\n","   test auc: 0.8977097272872925\n","   test aupr: 0.593352735042572\n","epoch 408 took 9.631500720977783s\n","   train loss: 0.014365545473992825\n","   train auc: 0.9729632139205933\n","   train aupr: 0.7433199882507324\n","Testing\n","   test loss: 0.05068580061197281\n","   test auc: 0.8976773023605347\n","   test aupr: 0.593429684638977\n","epoch 409 took 9.634554862976074s\n","   train loss: 0.014382326044142246\n","   train auc: 0.9730075001716614\n","   train aupr: 0.7436050176620483\n","Testing\n","   test loss: 0.054156046360731125\n","   test auc: 0.8976374268531799\n","   test aupr: 0.5935055017471313\n","epoch 410 took 9.648437738418579s\n","   train loss: 0.01450889278203249\n","   train auc: 0.9730544090270996\n","   train aupr: 0.7439293265342712\n","Testing\n","   test loss: 0.04857999086380005\n","   test auc: 0.897606611251831\n","   test aupr: 0.593605637550354\n","epoch 411 took 9.64543867111206s\n","   train loss: 0.014359562657773495\n","   train auc: 0.973094642162323\n","   train aupr: 0.7441797852516174\n","Testing\n","   test loss: 0.04344044625759125\n","   test auc: 0.8976178765296936\n","   test aupr: 0.5937831401824951\n","epoch 412 took 9.634872674942017s\n","   train loss: 0.014562849886715412\n","   train auc: 0.9731396436691284\n","   train aupr: 0.7444794774055481\n","Testing\n","   test loss: 0.05396734178066254\n","   test auc: 0.8976049423217773\n","   test aupr: 0.5939228534698486\n","epoch 413 took 9.637882947921753s\n","   train loss: 0.014411433599889278\n","   train auc: 0.9731821417808533\n","   train aupr: 0.7447570562362671\n","Testing\n","   test loss: 0.051070962101221085\n","   test auc: 0.8975638747215271\n","   test aupr: 0.5940191745758057\n","epoch 414 took 9.62332034111023s\n","   train loss: 0.014332963153719902\n","   train auc: 0.9732277393341064\n","   train aupr: 0.7450660467147827\n","Testing\n","   test loss: 0.04678028076887131\n","   test auc: 0.8975511789321899\n","   test aupr: 0.5941492319107056\n","epoch 415 took 9.612918853759766s\n","   train loss: 0.014286654070019722\n","   train auc: 0.9732677936553955\n","   train aupr: 0.7453222870826721\n","Testing\n","   test loss: 0.051624178886413574\n","   test auc: 0.8975350856781006\n","   test aupr: 0.5942696928977966\n","epoch 416 took 9.618412971496582s\n","   train loss: 0.01428093109279871\n","   train auc: 0.9733103513717651\n","   train aupr: 0.7455973625183105\n","Testing\n","   test loss: 0.05299806967377663\n","   test auc: 0.897495687007904\n","   test aupr: 0.5943530201911926\n","epoch 417 took 9.631980419158936s\n","   train loss: 0.014288878068327904\n","   train auc: 0.973353922367096\n","   train aupr: 0.7458903789520264\n","Testing\n","   test loss: 0.0629793331027031\n","   test auc: 0.8974135518074036\n","   test aupr: 0.5943530797958374\n","epoch 418 took 9.610103845596313s\n","   train loss: 0.014268188737332821\n","   train auc: 0.9733992218971252\n","   train aupr: 0.7462096214294434\n","Testing\n","   test loss: 0.0541238971054554\n","   test auc: 0.8973283767700195\n","   test aupr: 0.5943536162376404\n","epoch 419 took 9.608080387115479s\n","   train loss: 0.01431459840387106\n","   train auc: 0.9734397530555725\n","   train aupr: 0.7464619278907776\n","Testing\n","   test loss: 0.05209954082965851\n","   test auc: 0.8972846269607544\n","   test aupr: 0.5944170355796814\n","epoch 420 took 9.614725589752197s\n","   train loss: 0.0142270028591156\n","   train auc: 0.9734847545623779\n","   train aupr: 0.7467717528343201\n","Testing\n","   test loss: 0.0547756589949131\n","   test auc: 0.8972381353378296\n","   test aupr: 0.5944737792015076\n","epoch 421 took 9.611976861953735s\n","   train loss: 0.014326433651149273\n","   train auc: 0.973525881767273\n","   train aupr: 0.7470405697822571\n","Testing\n","   test loss: 0.053010813891887665\n","   test auc: 0.8971887826919556\n","   test aupr: 0.5945314168930054\n","epoch 422 took 9.618174076080322s\n","   train loss: 0.014357528649270535\n","   train auc: 0.9735660552978516\n","   train aupr: 0.7473033666610718\n","Testing\n","   test loss: 0.04785719886422157\n","   test auc: 0.8971661925315857\n","   test aupr: 0.5946366786956787\n","epoch 423 took 9.610872030258179s\n","   train loss: 0.014267145656049252\n","   train auc: 0.97360759973526\n","   train aupr: 0.7475823760032654\n","Testing\n","   test loss: 0.04799193888902664\n","   test auc: 0.8971614837646484\n","   test aupr: 0.5947758555412292\n","epoch 424 took 9.618207931518555s\n","   train loss: 0.014120680280029774\n","   train auc: 0.9736495018005371\n","   train aupr: 0.7478629350662231\n","Testing\n","   test loss: 0.06025536358356476\n","   test auc: 0.8971129059791565\n","   test aupr: 0.5948348641395569\n","epoch 425 took 9.615707159042358s\n","   train loss: 0.014210929162800312\n","   train auc: 0.9736900329589844\n","   train aupr: 0.7481231093406677\n","Testing\n","   test loss: 0.04535343125462532\n","   test auc: 0.897078275680542\n","   test aupr: 0.5949087142944336\n","epoch 426 took 9.61935043334961s\n","   train loss: 0.014129619114100933\n","   train auc: 0.9737319946289062\n","   train aupr: 0.7484077215194702\n","Testing\n","   test loss: 0.054758571088314056\n","   test auc: 0.8970580697059631\n","   test aupr: 0.5949992537498474\n","epoch 427 took 9.613499402999878s\n","   train loss: 0.014192122034728527\n","   train auc: 0.9737735986709595\n","   train aupr: 0.7486895322799683\n","Testing\n","   test loss: 0.05049765482544899\n","   test auc: 0.8970205783843994\n","   test aupr: 0.5950656533241272\n","epoch 428 took 9.60403823852539s\n","   train loss: 0.0141425970941782\n","   train auc: 0.9738166332244873\n","   train aupr: 0.7489849925041199\n","Testing\n","   test loss: 0.051493290811777115\n","   test auc: 0.89699786901474\n","   test aupr: 0.5951561331748962\n","epoch 429 took 9.614035844802856s\n","   train loss: 0.014131062664091587\n","   train auc: 0.9738595485687256\n","   train aupr: 0.7492828965187073\n","Testing\n","   test loss: 0.0535656213760376\n","   test auc: 0.8969640731811523\n","   test aupr: 0.5952128767967224\n","epoch 430 took 9.645375967025757s\n","   train loss: 0.01406957022845745\n","   train auc: 0.9738999009132385\n","   train aupr: 0.7495497465133667\n","Testing\n","   test loss: 0.05491907149553299\n","   test auc: 0.8969156742095947\n","   test aupr: 0.5952527523040771\n","epoch 431 took 9.636782884597778s\n","   train loss: 0.014026944525539875\n","   train auc: 0.9739387035369873\n","   train aupr: 0.7497957944869995\n","Testing\n","   test loss: 0.06488071382045746\n","   test auc: 0.896828293800354\n","   test aupr: 0.5952309966087341\n","epoch 432 took 9.647552490234375s\n","   train loss: 0.014050143770873547\n","   train auc: 0.9739806652069092\n","   train aupr: 0.750088095664978\n","Testing\n","   test loss: 0.04846622049808502\n","   test auc: 0.8967657685279846\n","   test aupr: 0.5952597856521606\n","epoch 433 took 9.642508029937744s\n","   train loss: 0.014067355543375015\n","   train auc: 0.9740225672721863\n","   train aupr: 0.7503758668899536\n","Testing\n","   test loss: 0.055953867733478546\n","   test auc: 0.8967300653457642\n","   test aupr: 0.5953332185745239\n","epoch 434 took 9.650063753128052s\n","   train loss: 0.014082145877182484\n","   train auc: 0.9740616083145142\n","   train aupr: 0.7506294250488281\n","Testing\n","   test loss: 0.04908502474427223\n","   test auc: 0.8966964483261108\n","   test aupr: 0.5954069495201111\n","epoch 435 took 9.643033504486084s\n","   train loss: 0.014316726475954056\n","   train auc: 0.9741010069847107\n","   train aupr: 0.7508967518806458\n","Testing\n","   test loss: 0.05490275099873543\n","   test auc: 0.8966653347015381\n","   test aupr: 0.5954957604408264\n","epoch 436 took 9.615280628204346s\n","   train loss: 0.014288893900811672\n","   train auc: 0.9741397500038147\n","   train aupr: 0.7511513233184814\n","Testing\n","   test loss: 0.05133555829524994\n","   test auc: 0.8966261148452759\n","   test aupr: 0.5955576300621033\n","epoch 437 took 9.611661434173584s\n","   train loss: 0.014074721373617649\n","   train auc: 0.9741783738136292\n","   train aupr: 0.7514072060585022\n","Testing\n","   test loss: 0.05859183520078659\n","   test auc: 0.8965730667114258\n","   test aupr: 0.5955839157104492\n","epoch 438 took 9.608208179473877s\n","   train loss: 0.013996307738125324\n","   train auc: 0.9742184281349182\n","   train aupr: 0.7516757249832153\n","Testing\n","   test loss: 0.04931013286113739\n","   test auc: 0.896530270576477\n","   test aupr: 0.595629096031189\n","epoch 439 took 9.610787868499756s\n","   train loss: 0.01405099593102932\n","   train auc: 0.97425776720047\n","   train aupr: 0.7519428730010986\n","Testing\n","   test loss: 0.05319500342011452\n","   test auc: 0.8965065479278564\n","   test aupr: 0.5957192182540894\n","epoch 440 took 9.609647989273071s\n","   train loss: 0.014026753604412079\n","   train auc: 0.9743000268936157\n","   train aupr: 0.7522362470626831\n","Testing\n","   test loss: 0.047941047698259354\n","   test auc: 0.8964881300926208\n","   test aupr: 0.5958204865455627\n","epoch 441 took 9.61860990524292s\n","   train loss: 0.013934729620814323\n","   train auc: 0.9743376970291138\n","   train aupr: 0.7524817585945129\n","Testing\n","   test loss: 0.05118440091609955\n","   test auc: 0.8964738845825195\n","   test aupr: 0.5959304571151733\n","epoch 442 took 9.613634586334229s\n","   train loss: 0.014031689614057541\n","   train auc: 0.974376380443573\n","   train aupr: 0.7527427077293396\n","Testing\n","   test loss: 0.04360339790582657\n","   test auc: 0.8964771628379822\n","   test aupr: 0.5960784554481506\n","epoch 443 took 9.615138292312622s\n","   train loss: 0.015815237537026405\n","   train auc: 0.9744139909744263\n","   train aupr: 0.7529932856559753\n","Testing\n","   test loss: 0.04256900027394295\n","   test auc: 0.896510660648346\n","   test aupr: 0.5962225198745728\n","epoch 444 took 9.641561269760132s\n","   train loss: 0.014979095198214054\n","   train auc: 0.974446177482605\n","   train aupr: 0.7531772255897522\n","Testing\n","   test loss: 0.05050152167677879\n","   test auc: 0.8965181112289429\n","   test aupr: 0.5963245630264282\n","epoch 445 took 9.595999240875244s\n","   train loss: 0.014267866499722004\n","   train auc: 0.9744831323623657\n","   train aupr: 0.7534114122390747\n","Testing\n","   test loss: 0.06067655235528946\n","   test auc: 0.8964565992355347\n","   test aupr: 0.5963520407676697\n","epoch 446 took 9.60563850402832s\n","   train loss: 0.013988981954753399\n","   train auc: 0.974520742893219\n","   train aupr: 0.7536593675613403\n","Testing\n","   test loss: 0.05256326496601105\n","   test auc: 0.8963907957077026\n","   test aupr: 0.5963709354400635\n","epoch 447 took 9.623257398605347s\n","   train loss: 0.0139665761962533\n","   train auc: 0.9745590686798096\n","   train aupr: 0.7539184093475342\n","Testing\n","   test loss: 0.04426269233226776\n","   test auc: 0.8963856101036072\n","   test aupr: 0.5965022444725037\n","epoch 448 took 9.628987789154053s\n","   train loss: 0.01428813487291336\n","   train auc: 0.9745950102806091\n","   train aupr: 0.7541540861129761\n","Testing\n","   test loss: 0.05451222509145737\n","   test auc: 0.8963684439659119\n","   test aupr: 0.596605658531189\n","epoch 449 took 9.61730694770813s\n","   train loss: 0.014062219299376011\n","   train auc: 0.9746327996253967\n","   train aupr: 0.7544045448303223\n","Testing\n","   test loss: 0.051885657012462616\n","   test auc: 0.8963263034820557\n","   test aupr: 0.5966583490371704\n","epoch 450 took 9.620591640472412s\n","   train loss: 0.013945918530225754\n","   train auc: 0.9746712446212769\n","   train aupr: 0.754666805267334\n","Testing\n","   test loss: 0.057471953332424164\n","   test auc: 0.896276593208313\n","   test aupr: 0.5967031121253967\n","epoch 451 took 9.616371393203735s\n","   train loss: 0.0139168631285429\n","   train auc: 0.9747090339660645\n","   train aupr: 0.75491863489151\n","Testing\n","   test loss: 0.05486420542001724\n","   test auc: 0.8962182402610779\n","   test aupr: 0.5967371463775635\n","epoch 452 took 9.618082046508789s\n","   train loss: 0.013943861238658428\n","   train auc: 0.974746823310852\n","   train aupr: 0.7551716566085815\n","Testing\n","   test loss: 0.06003447622060776\n","   test auc: 0.8961508870124817\n","   test aupr: 0.5967504978179932\n","epoch 453 took 9.626789331436157s\n","   train loss: 0.013877433724701405\n","   train auc: 0.9747838377952576\n","   train aupr: 0.7554253935813904\n","Testing\n","   test loss: 0.05288967117667198\n","   test auc: 0.8960908055305481\n","   test aupr: 0.5967680215835571\n","epoch 454 took 9.61974310874939s\n","   train loss: 0.01382908970117569\n","   train auc: 0.9748199582099915\n","   train aupr: 0.7556599378585815\n","Testing\n","   test loss: 0.05516323819756508\n","   test auc: 0.8960472941398621\n","   test aupr: 0.5968263149261475\n","epoch 455 took 9.612215995788574s\n","   train loss: 0.013841644860804081\n","   train auc: 0.9748607873916626\n","   train aupr: 0.7559546232223511\n","Testing\n","   test loss: 0.05465555936098099\n","   test auc: 0.8959983587265015\n","   test aupr: 0.5968815088272095\n","epoch 456 took 9.617343425750732s\n","   train loss: 0.013858644291758537\n","   train auc: 0.9748977422714233\n","   train aupr: 0.7562063932418823\n","Testing\n","   test loss: 0.05173778161406517\n","   test auc: 0.8959612250328064\n","   test aupr: 0.5969472527503967\n","epoch 457 took 9.618703126907349s\n","   train loss: 0.013832754455506802\n","   train auc: 0.9749343395233154\n","   train aupr: 0.7564527988433838\n","Testing\n","   test loss: 0.05542202666401863\n","   test auc: 0.8959203362464905\n","   test aupr: 0.5970057249069214\n","epoch 458 took 9.620539426803589s\n","   train loss: 0.01392329577356577\n","   train auc: 0.9749693870544434\n","   train aupr: 0.7566855549812317\n","Testing\n","   test loss: 0.05416825786232948\n","   test auc: 0.8958743214607239\n","   test aupr: 0.5970512628555298\n","epoch 459 took 9.625046491622925s\n","   train loss: 0.013769419863820076\n","   train auc: 0.9750053286552429\n","   train aupr: 0.7569268345832825\n","Testing\n","   test loss: 0.05053550750017166\n","   test auc: 0.8958451747894287\n","   test aupr: 0.5971249938011169\n","epoch 460 took 9.647649765014648s\n","   train loss: 0.01381753571331501\n","   train auc: 0.9750435948371887\n","   train aupr: 0.7571980357170105\n","Testing\n","   test loss: 0.060534171760082245\n","   test auc: 0.89579176902771\n","   test aupr: 0.5971593260765076\n","epoch 461 took 9.611819982528687s\n","   train loss: 0.013878078199923038\n","   train auc: 0.9750795960426331\n","   train aupr: 0.757438600063324\n","Testing\n","   test loss: 0.05112914368510246\n","   test auc: 0.8957377076148987\n","   test aupr: 0.5971912741661072\n","epoch 462 took 9.62041449546814s\n","   train loss: 0.013780517503619194\n","   train auc: 0.9751160144805908\n","   train aupr: 0.7576870322227478\n","Testing\n","   test loss: 0.057763807475566864\n","   test auc: 0.8956925868988037\n","   test aupr: 0.5972403287887573\n","epoch 463 took 9.617295026779175s\n","   train loss: 0.013734289444983006\n","   train auc: 0.9751533269882202\n","   train aupr: 0.7579444646835327\n","Testing\n","   test loss: 0.05704972520470619\n","   test auc: 0.8956298232078552\n","   test aupr: 0.5972532033920288\n","epoch 464 took 9.624942064285278s\n","   train loss: 0.013746100477874279\n","   train auc: 0.9751876592636108\n","   train aupr: 0.7581707239151001\n","Testing\n","   test loss: 0.05634860321879387\n","   test auc: 0.8955691456794739\n","   test aupr: 0.5972698926925659\n","epoch 465 took 9.629428148269653s\n","   train loss: 0.013784732669591904\n","   train auc: 0.9752253293991089\n","   train aupr: 0.7584359645843506\n","Testing\n","   test loss: 0.05889251083135605\n","   test auc: 0.8955041170120239\n","   test aupr: 0.5972771048545837\n","epoch 466 took 9.624109983444214s\n","   train loss: 0.013819482177495956\n","   train auc: 0.9752601385116577\n","   train aupr: 0.7586674094200134\n","Testing\n","   test loss: 0.06079383194446564\n","   test auc: 0.8954265117645264\n","   test aupr: 0.5972700715065002\n","epoch 467 took 9.638369083404541s\n","   train loss: 0.013684422709047794\n","   train auc: 0.9752956628799438\n","   train aupr: 0.7589095830917358\n","Testing\n","   test loss: 0.061977487057447433\n","   test auc: 0.8953391313552856\n","   test aupr: 0.5972484350204468\n","epoch 468 took 9.619271993637085s\n","   train loss: 0.013790632598102093\n","   train auc: 0.9753316044807434\n","   train aupr: 0.7591567635536194\n","Testing\n","   test loss: 0.05465628206729889\n","   test auc: 0.8952740430831909\n","   test aupr: 0.5972636938095093\n","epoch 469 took 9.63286828994751s\n","   train loss: 0.01373371947556734\n","   train auc: 0.9753681421279907\n","   train aupr: 0.7594135999679565\n","Testing\n","   test loss: 0.05008045956492424\n","   test auc: 0.895247220993042\n","   test aupr: 0.5973519086837769\n","epoch 470 took 9.640411615371704s\n","   train loss: 0.013824773952364922\n","   train auc: 0.9754016399383545\n","   train aupr: 0.7596397995948792\n","Testing\n","   test loss: 0.054889384657144547\n","   test auc: 0.89521723985672\n","   test aupr: 0.5974279642105103\n","epoch 471 took 9.63611102104187s\n","   train loss: 0.013711337931454182\n","   train auc: 0.9754372239112854\n","   train aupr: 0.7598860859870911\n","Testing\n","   test loss: 0.054046571254730225\n","   test auc: 0.8951734900474548\n","   test aupr: 0.5974791049957275\n","epoch 472 took 9.612147331237793s\n","   train loss: 0.01362619549036026\n","   train auc: 0.9754737615585327\n","   train aupr: 0.7601451277732849\n","Testing\n","   test loss: 0.06135442107915878\n","   test auc: 0.8951075673103333\n","   test aupr: 0.5974952578544617\n","epoch 473 took 9.638453960418701s\n","   train loss: 0.013743857853114605\n","   train auc: 0.9755074381828308\n","   train aupr: 0.7603664398193359\n","Testing\n","   test loss: 0.054281674325466156\n","   test auc: 0.8950433135032654\n","   test aupr: 0.5975109338760376\n","epoch 474 took 9.63186264038086s\n","   train loss: 0.013594078831374645\n","   train auc: 0.9755432605743408\n","   train aupr: 0.7606126070022583\n","Testing\n","   test loss: 0.05806849151849747\n","   test auc: 0.8949885964393616\n","   test aupr: 0.597541868686676\n","epoch 475 took 9.616933345794678s\n","   train loss: 0.013657103292644024\n","   train auc: 0.9755788445472717\n","   train aupr: 0.7608625888824463\n","Testing\n","   test loss: 0.050415560603141785\n","   test auc: 0.8949469327926636\n","   test aupr: 0.5975973606109619\n","epoch 476 took 9.645793914794922s\n","   train loss: 0.01367763802409172\n","   train auc: 0.9756123423576355\n","   train aupr: 0.7610878348350525\n","Testing\n","   test loss: 0.05531669035553932\n","   test auc: 0.8949132561683655\n","   test aupr: 0.5976625084877014\n","epoch 477 took 9.630459785461426s\n","   train loss: 0.013618589378893375\n","   train auc: 0.9756469130516052\n","   train aupr: 0.7613233923912048\n","Testing\n","   test loss: 0.059696827083826065\n","   test auc: 0.8948532938957214\n","   test aupr: 0.5976774096488953\n","epoch 478 took 9.632426261901855s\n","   train loss: 0.013570618815720081\n","   train auc: 0.9756810665130615\n","   train aupr: 0.7615551352500916\n","Testing\n","   test loss: 0.06197496876120567\n","   test auc: 0.894771158695221\n","   test aupr: 0.5976583361625671\n","epoch 479 took 9.614347219467163s\n","   train loss: 0.01353274006396532\n","   train auc: 0.9757156372070312\n","   train aupr: 0.7617955207824707\n","Testing\n","   test loss: 0.06202978268265724\n","   test auc: 0.8946835398674011\n","   test aupr: 0.5976213812828064\n","epoch 480 took 9.62408447265625s\n","   train loss: 0.01361228246241808\n","   train auc: 0.9757505059242249\n","   train aupr: 0.7620352506637573\n","Testing\n","   test loss: 0.06096559390425682\n","   test auc: 0.8946033120155334\n","   test aupr: 0.5976060032844543\n","epoch 481 took 9.637169122695923s\n","   train loss: 0.01359864417463541\n","   train auc: 0.9757853150367737\n","   train aupr: 0.7622795104980469\n","Testing\n","   test loss: 0.058874551206827164\n","   test auc: 0.8945311307907104\n","   test aupr: 0.5976056456565857\n","epoch 482 took 9.63208794593811s\n","   train loss: 0.013619575649499893\n","   train auc: 0.9758185744285583\n","   train aupr: 0.7625067234039307\n","Testing\n","   test loss: 0.05907274782657623\n","   test auc: 0.8944615125656128\n","   test aupr: 0.597599446773529\n","epoch 483 took 9.628774881362915s\n","   train loss: 0.013636584393680096\n","   train auc: 0.9758514165878296\n","   train aupr: 0.7627320289611816\n","Testing\n","   test loss: 0.05755307897925377\n","   test auc: 0.894397497177124\n","   test aupr: 0.5976071953773499\n","epoch 484 took 9.631014823913574s\n","   train loss: 0.013585926033556461\n","   train auc: 0.9758875966072083\n","   train aupr: 0.7629837393760681\n","Testing\n","   test loss: 0.05600469559431076\n","   test auc: 0.8943427205085754\n","   test aupr: 0.5976257920265198\n","epoch 485 took 9.625611782073975s\n","   train loss: 0.013506867922842503\n","   train auc: 0.9759204983711243\n","   train aupr: 0.7632125020027161\n","Testing\n","   test loss: 0.06421171873807907\n","   test auc: 0.8942643404006958\n","   test aupr: 0.5975933074951172\n","epoch 486 took 9.616901397705078s\n","   train loss: 0.013604873791337013\n","   train auc: 0.9759522080421448\n","   train aupr: 0.7634231448173523\n","Testing\n","   test loss: 0.0587884820997715\n","   test auc: 0.8941800594329834\n","   test aupr: 0.5975626111030579\n","epoch 487 took 9.627623558044434s\n","   train loss: 0.013530087657272816\n","   train auc: 0.9759875535964966\n","   train aupr: 0.7636749744415283\n","Testing\n","   test loss: 0.0572427473962307\n","   test auc: 0.8941176533699036\n","   test aupr: 0.5975782871246338\n","epoch 488 took 9.61613154411316s\n","   train loss: 0.013495354913175106\n","   train auc: 0.9760196805000305\n","   train aupr: 0.7638859152793884\n","Testing\n","   test loss: 0.06123703345656395\n","   test auc: 0.8940459489822388\n","   test aupr: 0.5975773930549622\n","epoch 489 took 9.619500637054443s\n","   train loss: 0.01359467301517725\n","   train auc: 0.9760512113571167\n","   train aupr: 0.7640966773033142\n","Testing\n","   test loss: 0.062108058482408524\n","   test auc: 0.893957793712616\n","   test aupr: 0.5975428819656372\n","epoch 490 took 9.609795093536377s\n","   train loss: 0.013464661315083504\n","   train auc: 0.9760871529579163\n","   train aupr: 0.7643576860427856\n","Testing\n","   test loss: 0.0636281967163086\n","   test auc: 0.893864095211029\n","   test aupr: 0.597497820854187\n","epoch 491 took 9.630529880523682s\n","   train loss: 0.01361020840704441\n","   train auc: 0.9761195182800293\n","   train aupr: 0.7645803093910217\n","Testing\n","   test loss: 0.049960196018218994\n","   test auc: 0.8938131332397461\n","   test aupr: 0.5975177884101868\n","epoch 492 took 9.625804901123047s\n","   train loss: 0.0142287602648139\n","   train auc: 0.9761513471603394\n","   train aupr: 0.7647837400436401\n","Testing\n","   test loss: 0.04836017265915871\n","   test auc: 0.8938099145889282\n","   test aupr: 0.5976170301437378\n","epoch 493 took 9.617592811584473s\n","   train loss: 0.013694293797016144\n","   train auc: 0.9761826992034912\n","   train aupr: 0.764994740486145\n","Testing\n","   test loss: 0.059507451951503754\n","   test auc: 0.8937755227088928\n","   test aupr: 0.5976665616035461\n","epoch 494 took 9.653497695922852s\n","   train loss: 0.01348171941936016\n","   train auc: 0.9762158989906311\n","   train aupr: 0.765225350856781\n","Testing\n","   test loss: 0.056025028228759766\n","   test auc: 0.8937188386917114\n","   test aupr: 0.5976728200912476\n","epoch 495 took 9.629294157028198s\n","   train loss: 0.013490723446011543\n","   train auc: 0.9762464165687561\n","   train aupr: 0.7654283046722412\n","Testing\n","   test loss: 0.0655415952205658\n","   test auc: 0.8936420679092407\n","   test aupr: 0.5976442694664001\n","epoch 496 took 9.635096311569214s\n","   train loss: 0.01355515792965889\n","   train auc: 0.9762800931930542\n","   train aupr: 0.7656670808792114\n","Testing\n","   test loss: 0.05658095329999924\n","   test auc: 0.8935684561729431\n","   test aupr: 0.5976258516311646\n","epoch 497 took 9.644787073135376s\n","   train loss: 0.01350814662873745\n","   train auc: 0.9763110876083374\n","   train aupr: 0.7658812403678894\n","Testing\n","   test loss: 0.05200964957475662\n","   test auc: 0.8935361504554749\n","   test aupr: 0.5976757407188416\n","epoch 498 took 9.636703729629517s\n","   train loss: 0.013389592058956623\n","   train auc: 0.9763435125350952\n","   train aupr: 0.7661082148551941\n","Testing\n","   test loss: 0.06017639487981796\n","   test auc: 0.8934902548789978\n","   test aupr: 0.5976981520652771\n","epoch 499 took 9.636945247650146s\n","   train loss: 0.013397382572293282\n","   train auc: 0.9763767719268799\n","   train aupr: 0.7663467526435852\n","Testing\n","   test loss: 0.06054036319255829\n","   test auc: 0.8934158086776733\n","   test aupr: 0.5976756811141968\n","epoch 500 took 9.63943886756897s\n","   train loss: 0.0134041178971529\n","   train auc: 0.9764081239700317\n","   train aupr: 0.7665601372718811\n","Testing\n","   test loss: 0.0545881986618042\n","   test auc: 0.8933582305908203\n","   test aupr: 0.5976866483688354\n","Saving the intermediate model weights...\n","Done\n","epoch 501 took 9.6486177444458s\n","   train loss: 0.013287989422678947\n","   train auc: 0.9764384031295776\n","   train aupr: 0.766760528087616\n","Testing\n","   test loss: 0.05878889188170433\n","   test auc: 0.893310010433197\n","   test aupr: 0.5977208614349365\n","epoch 502 took 9.645275115966797s\n","   train loss: 0.013277681544423103\n","   train auc: 0.9764727354049683\n","   train aupr: 0.7670108675956726\n","Testing\n","   test loss: 0.06028957664966583\n","   test auc: 0.8932463526725769\n","   test aupr: 0.5977307558059692\n","epoch 503 took 9.634543895721436s\n","   train loss: 0.013281094841659069\n","   train auc: 0.9765030741691589\n","   train aupr: 0.767215371131897\n","Testing\n","   test loss: 0.05896759778261185\n","   test auc: 0.8931826949119568\n","   test aupr: 0.597740113735199\n","epoch 504 took 9.638484001159668s\n","   train loss: 0.013255159370601177\n","   train auc: 0.976535439491272\n","   train aupr: 0.7674407362937927\n","Testing\n","   test loss: 0.06217036023736\n","   test auc: 0.8931130766868591\n","   test aupr: 0.5977396368980408\n","epoch 505 took 9.644418001174927s\n","   train loss: 0.013216332532465458\n","   train auc: 0.9765681624412537\n","   train aupr: 0.7676741480827332\n","Testing\n","   test loss: 0.05954741686582565\n","   test auc: 0.8930420279502869\n","   test aupr: 0.5977375507354736\n","epoch 506 took 9.643566370010376s\n","   train loss: 0.013226046226918697\n","   train auc: 0.9766011834144592\n","   train aupr: 0.7679091095924377\n","Testing\n","   test loss: 0.060042619705200195\n","   test auc: 0.8929775953292847\n","   test aupr: 0.5977456569671631\n","epoch 507 took 9.664552927017212s\n","   train loss: 0.013260203413665295\n","   train auc: 0.9766337275505066\n","   train aupr: 0.7681427001953125\n","Testing\n","   test loss: 0.06052080914378166\n","   test auc: 0.8929105401039124\n","   test aupr: 0.5977469086647034\n","epoch 508 took 9.643221378326416s\n","   train loss: 0.013245153240859509\n","   train auc: 0.976662814617157\n","   train aupr: 0.7683375477790833\n","Testing\n","   test loss: 0.05549505725502968\n","   test auc: 0.892857551574707\n","   test aupr: 0.597771942615509\n","epoch 509 took 9.621994972229004s\n","   train loss: 0.01328352652490139\n","   train auc: 0.9766948223114014\n","   train aupr: 0.7685614824295044\n","Testing\n","   test loss: 0.05879729986190796\n","   test auc: 0.8928098678588867\n","   test aupr: 0.5978058576583862\n","epoch 510 took 9.639678955078125s\n","   train loss: 0.01321747712790966\n","   train auc: 0.9767262935638428\n","   train aupr: 0.7687824368476868\n","Testing\n","   test loss: 0.06320914626121521\n","   test auc: 0.8927393555641174\n","   test aupr: 0.5977984666824341\n","epoch 511 took 9.647510051727295s\n","   train loss: 0.013210543431341648\n","   train auc: 0.9767571687698364\n","   train aupr: 0.7689955234527588\n","Testing\n","   test loss: 0.05903058871626854\n","   test auc: 0.8926687836647034\n","   test aupr: 0.5977928638458252\n","epoch 512 took 9.625432252883911s\n","   train loss: 0.013278293423354626\n","   train auc: 0.9767882823944092\n","   train aupr: 0.7692112326622009\n","Testing\n","   test loss: 0.05802548676729202\n","   test auc: 0.8926127552986145\n","   test aupr: 0.5978127121925354\n","epoch 513 took 9.652954339981079s\n","   train loss: 0.013221025466918945\n","   train auc: 0.9768184423446655\n","   train aupr: 0.7694183588027954\n","Testing\n","   test loss: 0.05931701883673668\n","   test auc: 0.8925549983978271\n","   test aupr: 0.5978289246559143\n","epoch 514 took 9.607502460479736s\n","   train loss: 0.0131820784881711\n","   train auc: 0.9768508076667786\n","   train aupr: 0.7696577906608582\n","Testing\n","   test loss: 0.059002164751291275\n","   test auc: 0.8924958109855652\n","   test aupr: 0.5978384017944336\n","epoch 515 took 9.6183443069458s\n","   train loss: 0.013242014683783054\n","   train auc: 0.9768821001052856\n","   train aupr: 0.7698737382888794\n","Testing\n","   test loss: 0.059291575103998184\n","   test auc: 0.8924370408058167\n","   test aupr: 0.5978492498397827\n","epoch 516 took 9.624549865722656s\n","   train loss: 0.013258527964353561\n","   train auc: 0.9769129157066345\n","   train aupr: 0.7700920701026917\n","Testing\n","   test loss: 0.06213337555527687\n","   test auc: 0.8923696875572205\n","   test aupr: 0.5978453755378723\n","epoch 517 took 9.637821912765503s\n","   train loss: 0.01321768295019865\n","   train auc: 0.9769429564476013\n","   train aupr: 0.7703037858009338\n","Testing\n","   test loss: 0.061356544494628906\n","   test auc: 0.8922966718673706\n","   test aupr: 0.5978304147720337\n","epoch 518 took 9.609607458114624s\n","   train loss: 0.013215563260018826\n","   train auc: 0.976973831653595\n","   train aupr: 0.7705211639404297\n","Testing\n","   test loss: 0.0586167611181736\n","   test auc: 0.8922339677810669\n","   test aupr: 0.5978332757949829\n","epoch 519 took 9.621084690093994s\n","   train loss: 0.013238544575870037\n","   train auc: 0.9770027995109558\n","   train aupr: 0.7707177400588989\n","Testing\n","   test loss: 0.058854978531599045\n","   test auc: 0.8921786546707153\n","   test aupr: 0.5978482365608215\n","epoch 520 took 9.637737274169922s\n","   train loss: 0.013204180635511875\n","   train auc: 0.9770333766937256\n","   train aupr: 0.7709362506866455\n","Testing\n","   test loss: 0.06437434256076813\n","   test auc: 0.8921074271202087\n","   test aupr: 0.5978351831436157\n","epoch 521 took 9.611935377120972s\n","   train loss: 0.013245362788438797\n","   train auc: 0.9770628809928894\n","   train aupr: 0.7711409330368042\n","Testing\n","   test loss: 0.05914251506328583\n","   test auc: 0.8920369744300842\n","   test aupr: 0.597825288772583\n","epoch 522 took 9.601440906524658s\n","   train loss: 0.013178066350519657\n","   train auc: 0.9770944118499756\n","   train aupr: 0.771375298500061\n","Testing\n","   test loss: 0.0625600665807724\n","   test auc: 0.8919717073440552\n","   test aupr: 0.597825288772583\n","epoch 523 took 9.611490964889526s\n","   train loss: 0.013213062658905983\n","   train auc: 0.9771229028701782\n","   train aupr: 0.77156662940979\n","Testing\n","   test loss: 0.06130361929535866\n","   test auc: 0.8919005990028381\n","   test aupr: 0.5978128910064697\n","epoch 524 took 9.65319037437439s\n","   train loss: 0.013205748051404953\n","   train auc: 0.9771525859832764\n","   train aupr: 0.7717761397361755\n","Testing\n","   test loss: 0.05959584191441536\n","   test auc: 0.8918375372886658\n","   test aupr: 0.597813069820404\n","epoch 525 took 9.627763509750366s\n","   train loss: 0.013179095461964607\n","   train auc: 0.9771826267242432\n","   train aupr: 0.7719904780387878\n","Testing\n","   test loss: 0.05671793222427368\n","   test auc: 0.8917884230613708\n","   test aupr: 0.5978363752365112\n","epoch 526 took 9.617370843887329s\n","   train loss: 0.013187164440751076\n","   train auc: 0.9772109389305115\n","   train aupr: 0.77218097448349\n","Testing\n","   test loss: 0.060133274644613266\n","   test auc: 0.8917374014854431\n","   test aupr: 0.5978584885597229\n","epoch 527 took 9.605371475219727s\n","   train loss: 0.013232960365712643\n","   train auc: 0.9772406220436096\n","   train aupr: 0.7723921537399292\n","Testing\n","   test loss: 0.06155324727296829\n","   test auc: 0.8916741609573364\n","   test aupr: 0.5978603959083557\n","epoch 528 took 9.622778177261353s\n","   train loss: 0.013214033097028732\n","   train auc: 0.9772694706916809\n","   train aupr: 0.7725940942764282\n","Testing\n","   test loss: 0.06170069798827171\n","   test auc: 0.8916067481040955\n","   test aupr: 0.5978546738624573\n","epoch 529 took 9.606874704360962s\n","   train loss: 0.013151274994015694\n","   train auc: 0.9772996306419373\n","   train aupr: 0.7728071808815002\n","Testing\n","   test loss: 0.06060122698545456\n","   test auc: 0.8915425539016724\n","   test aupr: 0.5978533029556274\n","epoch 530 took 9.633402109146118s\n","   train loss: 0.013174549676477909\n","   train auc: 0.9773295521736145\n","   train aupr: 0.7730218172073364\n","Testing\n","   test loss: 0.06305386871099472\n","   test auc: 0.8914740085601807\n","   test aupr: 0.597845196723938\n","epoch 531 took 9.641472101211548s\n","   train loss: 0.013170548714697361\n","   train auc: 0.9773577451705933\n","   train aupr: 0.7732166051864624\n","Testing\n","   test loss: 0.06056263670325279\n","   test auc: 0.8914056420326233\n","   test aupr: 0.597835898399353\n","epoch 532 took 9.633140802383423s\n","   train loss: 0.013212249614298344\n","   train auc: 0.9773871302604675\n","   train aupr: 0.7734283208847046\n","Testing\n","   test loss: 0.0616597905755043\n","   test auc: 0.8913407921791077\n","   test aupr: 0.597833514213562\n","epoch 533 took 9.638079166412354s\n","   train loss: 0.01315842941403389\n","   train auc: 0.9774150848388672\n","   train aupr: 0.773622989654541\n","Testing\n","   test loss: 0.06191543489694595\n","   test auc: 0.891274094581604\n","   test aupr: 0.5978246927261353\n","epoch 534 took 9.625463724136353s\n","   train loss: 0.013185198418796062\n","   train auc: 0.9774432182312012\n","   train aupr: 0.7738137245178223\n","Testing\n","   test loss: 0.06228911131620407\n","   test auc: 0.8912049531936646\n","   test aupr: 0.5978091359138489\n","epoch 535 took 9.613857507705688s\n","   train loss: 0.013213755562901497\n","   train auc: 0.977472186088562\n","   train aupr: 0.7740218043327332\n","Testing\n","   test loss: 0.05612573400139809\n","   test auc: 0.8911517858505249\n","   test aupr: 0.5978230237960815\n","epoch 536 took 9.644195079803467s\n","   train loss: 0.013204795308411121\n","   train auc: 0.9774997234344482\n","   train aupr: 0.7742161154747009\n","Testing\n","   test loss: 0.05874566733837128\n","   test auc: 0.8911084532737732\n","   test aupr: 0.5978553295135498\n","epoch 537 took 9.644707202911377s\n","   train loss: 0.0131989736109972\n","   train auc: 0.9775280356407166\n","   train aupr: 0.7744174003601074\n","Testing\n","   test loss: 0.05753173306584358\n","   test auc: 0.891061544418335\n","   test aupr: 0.5978816747665405\n","epoch 538 took 9.645749807357788s\n","   train loss: 0.013207950629293919\n","   train auc: 0.9775565266609192\n","   train aupr: 0.7746184468269348\n","Testing\n","   test loss: 0.057325057685375214\n","   test auc: 0.8910189270973206\n","   test aupr: 0.5979151129722595\n","epoch 539 took 9.656592607498169s\n","   train loss: 0.013178777880966663\n","   train auc: 0.9775829911231995\n","   train aupr: 0.774802029132843\n","Testing\n","   test loss: 0.055034104734659195\n","   test auc: 0.8909822702407837\n","   test aupr: 0.5979591608047485\n","epoch 540 took 9.638554573059082s\n","   train loss: 0.013210888020694256\n","   train auc: 0.9776111245155334\n","   train aupr: 0.7749985456466675\n","Testing\n","   test loss: 0.062251899391412735\n","   test auc: 0.8909323215484619\n","   test aupr: 0.5979793667793274\n","epoch 541 took 9.630282402038574s\n","   train loss: 0.013120440766215324\n","   train auc: 0.9776408076286316\n","   train aupr: 0.775215208530426\n","Testing\n","   test loss: 0.05996427685022354\n","   test auc: 0.890869677066803\n","   test aupr: 0.5979773998260498\n","epoch 542 took 9.601173162460327s\n","   train loss: 0.013140423223376274\n","   train auc: 0.9776664972305298\n","   train aupr: 0.7753857970237732\n","Testing\n","   test loss: 0.06221279874444008\n","   test auc: 0.8908073902130127\n","   test aupr: 0.5979771018028259\n","epoch 543 took 9.658680438995361s\n","   train loss: 0.013183994218707085\n","   train auc: 0.9776950478553772\n","   train aupr: 0.7755945920944214\n","Testing\n","   test loss: 0.05911653861403465\n","   test auc: 0.8907488584518433\n","   test aupr: 0.5979847311973572\n","epoch 544 took 9.672892332077026s\n","   train loss: 0.01316007412970066\n","   train auc: 0.9777225255966187\n","   train aupr: 0.7757854461669922\n","Testing\n","   test loss: 0.06265232712030411\n","   test auc: 0.8906888365745544\n","   test aupr: 0.5979862809181213\n","epoch 545 took 9.657139539718628s\n","   train loss: 0.013140326365828514\n","   train auc: 0.9777491092681885\n","   train aupr: 0.7759665250778198\n","Testing\n","   test loss: 0.05992722138762474\n","   test auc: 0.8906269073486328\n","   test aupr: 0.5979818105697632\n","epoch 546 took 9.638499021530151s\n","   train loss: 0.01315145380795002\n","   train auc: 0.9777752757072449\n","   train aupr: 0.7761494517326355\n","Testing\n","   test loss: 0.06205384433269501\n","   test auc: 0.890567421913147\n","   test aupr: 0.5979852676391602\n","epoch 547 took 9.63402771949768s\n","   train loss: 0.013185900636017323\n","   train auc: 0.9778038263320923\n","   train aupr: 0.7763567566871643\n","Testing\n","   test loss: 0.05987193435430527\n","   test auc: 0.8905085921287537\n","   test aupr: 0.5979880094528198\n","epoch 548 took 9.655441761016846s\n","   train loss: 0.013175451196730137\n","   train auc: 0.9778314232826233\n","   train aupr: 0.7765525579452515\n","Testing\n","   test loss: 0.06017426773905754\n","   test auc: 0.8904533982276917\n","   test aupr: 0.5979955792427063\n","epoch 549 took 9.646131992340088s\n","   train loss: 0.013138229958713055\n","   train auc: 0.977858304977417\n","   train aupr: 0.7767394185066223\n","Testing\n","   test loss: 0.058153487741947174\n","   test auc: 0.8904024958610535\n","   test aupr: 0.5980099439620972\n","epoch 550 took 9.655030727386475s\n","   train loss: 0.013133813627064228\n","   train auc: 0.9778844118118286\n","   train aupr: 0.7769211530685425\n","Testing\n","   test loss: 0.060852840542793274\n","   test auc: 0.8903515338897705\n","   test aupr: 0.598023533821106\n","epoch 551 took 9.669608116149902s\n","   train loss: 0.013141286559402943\n","   train auc: 0.9779114723205566\n","   train aupr: 0.7771161198616028\n","Testing\n","   test loss: 0.06156220659613609\n","   test auc: 0.8902916312217712\n","   test aupr: 0.598023533821106\n","epoch 552 took 9.652405261993408s\n","   train loss: 0.013128625229001045\n","   train auc: 0.9779391884803772\n","   train aupr: 0.777314305305481\n","Testing\n","   test loss: 0.05908165127038956\n","   test auc: 0.8902361392974854\n","   test aupr: 0.598029613494873\n","epoch 553 took 9.648154497146606s\n","   train loss: 0.013144069351255894\n","   train auc: 0.9779656529426575\n","   train aupr: 0.777500569820404\n","Testing\n","   test loss: 0.06220252066850662\n","   test auc: 0.8901783227920532\n","   test aupr: 0.5980288982391357\n","epoch 554 took 9.606661081314087s\n","   train loss: 0.013120525516569614\n","   train auc: 0.9779928922653198\n","   train aupr: 0.7776964902877808\n","Testing\n","   test loss: 0.06021294370293617\n","   test auc: 0.8901180624961853\n","   test aupr: 0.5980256199836731\n","epoch 555 took 9.611711740493774s\n","   train loss: 0.013110430911183357\n","   train auc: 0.9780181050300598\n","   train aupr: 0.7778661251068115\n","Testing\n","   test loss: 0.058251913636922836\n","   test auc: 0.8900692462921143\n","   test aupr: 0.5980479717254639\n","epoch 556 took 9.626315593719482s\n","   train loss: 0.013184642419219017\n","   train auc: 0.9780452847480774\n","   train aupr: 0.7780603766441345\n","Testing\n","   test loss: 0.059066351503133774\n","   test auc: 0.8900235891342163\n","   test aupr: 0.5980725884437561\n","epoch 557 took 9.624748706817627s\n","   train loss: 0.013137011788785458\n","   train auc: 0.9780705571174622\n","   train aupr: 0.7782366871833801\n","Testing\n","   test loss: 0.06049680709838867\n","   test auc: 0.8899727463722229\n","   test aupr: 0.5980847477912903\n","epoch 558 took 9.611485958099365s\n","   train loss: 0.013102010823786259\n","   train auc: 0.9780963659286499\n","   train aupr: 0.7784132957458496\n","Testing\n","   test loss: 0.05852268636226654\n","   test auc: 0.8899233341217041\n","   test aupr: 0.5981013774871826\n","epoch 559 took 9.608052253723145s\n","   train loss: 0.013077114708721638\n","   train auc: 0.9781229496002197\n","   train aupr: 0.778601348400116\n","Testing\n","   test loss: 0.05958651751279831\n","   test auc: 0.8898769021034241\n","   test aupr: 0.5981205105781555\n","epoch 560 took 9.59776496887207s\n","   train loss: 0.013143463060259819\n","   train auc: 0.9781500697135925\n","   train aupr: 0.7788021564483643\n","Testing\n","   test loss: 0.061073414981365204\n","   test auc: 0.8898230791091919\n","   test aupr: 0.5981271266937256\n","epoch 561 took 9.607519388198853s\n","   train loss: 0.01309346966445446\n","   train auc: 0.9781756401062012\n","   train aupr: 0.7789762020111084\n","Testing\n","   test loss: 0.061096351593732834\n","   test auc: 0.8897639513015747\n","   test aupr: 0.5981254577636719\n","epoch 562 took 9.603036165237427s\n","   train loss: 0.013087962754070759\n","   train auc: 0.9782014489173889\n","   train aupr: 0.7791644930839539\n","Testing\n","   test loss: 0.06297387182712555\n","   test auc: 0.8897008895874023\n","   test aupr: 0.598114013671875\n","epoch 563 took 9.622600078582764s\n","   train loss: 0.01306166686117649\n","   train auc: 0.9782264232635498\n","   train aupr: 0.7793384790420532\n","Testing\n","   test loss: 0.0630829930305481\n","   test auc: 0.8896318078041077\n","   test aupr: 0.5980942249298096\n","epoch 564 took 9.628736734390259s\n","   train loss: 0.013116910122334957\n","   train auc: 0.9782516360282898\n","   train aupr: 0.7795131206512451\n","Testing\n","   test loss: 0.05759137123823166\n","   test auc: 0.8895778656005859\n","   test aupr: 0.5981014966964722\n","epoch 565 took 9.627137184143066s\n","   train loss: 0.01313818246126175\n","   train auc: 0.9782781004905701\n","   train aupr: 0.7797026634216309\n","Testing\n","   test loss: 0.062350351363420486\n","   test auc: 0.8895264863967896\n","   test aupr: 0.5981085300445557\n","epoch 566 took 9.626053810119629s\n","   train loss: 0.013065589591860771\n","   train auc: 0.9783024191856384\n","   train aupr: 0.7798658013343811\n","Testing\n","   test loss: 0.06074235588312149\n","   test auc: 0.8894683122634888\n","   test aupr: 0.5981028079986572\n","epoch 567 took 9.637524127960205s\n","   train loss: 0.01310029998421669\n","   train auc: 0.978329062461853\n","   train aupr: 0.7800614833831787\n","Testing\n","   test loss: 0.05884736403822899\n","   test auc: 0.8894190192222595\n","   test aupr: 0.5981115102767944\n","epoch 568 took 9.646274328231812s\n","   train loss: 0.013110043480992317\n","   train auc: 0.9783532619476318\n","   train aupr: 0.7802276611328125\n","Testing\n","   test loss: 0.05953417345881462\n","   test auc: 0.8893713355064392\n","   test aupr: 0.5981248617172241\n","epoch 569 took 9.661227703094482s\n","   train loss: 0.013145405799150467\n","   train auc: 0.9783787131309509\n","   train aupr: 0.7804086208343506\n","Testing\n","   test loss: 0.05886688083410263\n","   test auc: 0.8893239498138428\n","   test aupr: 0.5981414318084717\n","epoch 570 took 9.659712314605713s\n","   train loss: 0.013063468039035797\n","   train auc: 0.9784052968025208\n","   train aupr: 0.7806038856506348\n","Testing\n","   test loss: 0.061788078397512436\n","   test auc: 0.8892704248428345\n","   test aupr: 0.5981460213661194\n","epoch 571 took 9.65191912651062s\n","   train loss: 0.01314094290137291\n","   train auc: 0.9784297347068787\n","   train aupr: 0.780776858329773\n","Testing\n","   test loss: 0.06331309676170349\n","   test auc: 0.8892068862915039\n","   test aupr: 0.5981326699256897\n","epoch 572 took 9.644642114639282s\n","   train loss: 0.013095005415380001\n","   train auc: 0.9784525632858276\n","   train aupr: 0.7809323072433472\n","Testing\n","   test loss: 0.06266265362501144\n","   test auc: 0.889142632484436\n","   test aupr: 0.5981183648109436\n","epoch 573 took 9.630401611328125s\n","   train loss: 0.013088018633425236\n","   train auc: 0.9784777760505676\n","   train aupr: 0.7811103463172913\n","Testing\n","   test loss: 0.06525707989931107\n","   test auc: 0.889073371887207\n","   test aupr: 0.5980939269065857\n","epoch 574 took 9.627243995666504s\n","   train loss: 0.013045398518443108\n","   train auc: 0.9785027503967285\n","   train aupr: 0.7812892198562622\n","Testing\n","   test loss: 0.0630154013633728\n","   test auc: 0.8890035152435303\n","   test aupr: 0.5980697870254517\n","epoch 575 took 9.606929063796997s\n","   train loss: 0.013069267384707928\n","   train auc: 0.9785282015800476\n","   train aupr: 0.7814776301383972\n","Testing\n","   test loss: 0.062070947140455246\n","   test auc: 0.8889415264129639\n","   test aupr: 0.5980607271194458\n","epoch 576 took 9.627058982849121s\n","   train loss: 0.01304184552282095\n","   train auc: 0.9785522222518921\n","   train aupr: 0.7816414833068848\n","Testing\n","   test loss: 0.06203596293926239\n","   test auc: 0.8888826966285706\n","   test aupr: 0.5980550050735474\n","epoch 577 took 9.63175654411316s\n","   train loss: 0.01305653341114521\n","   train auc: 0.9785773754119873\n","   train aupr: 0.7818236947059631\n","Testing\n","   test loss: 0.061795055866241455\n","   test auc: 0.8888251781463623\n","   test aupr: 0.5980502367019653\n","epoch 578 took 9.640243530273438s\n","   train loss: 0.013077127747237682\n","   train auc: 0.9786025285720825\n","   train aupr: 0.7820048332214355\n","Testing\n","   test loss: 0.06170225888490677\n","   test auc: 0.8887687921524048\n","   test aupr: 0.598048746585846\n","epoch 579 took 9.611494779586792s\n","   train loss: 0.01310226321220398\n","   train auc: 0.9786252379417419\n","   train aupr: 0.7821617126464844\n","Testing\n","   test loss: 0.05879199877381325\n","   test auc: 0.8887202739715576\n","   test aupr: 0.5980606079101562\n","epoch 580 took 9.597630262374878s\n","   train loss: 0.013092032633721828\n","   train auc: 0.9786501526832581\n","   train aupr: 0.7823432683944702\n","Testing\n","   test loss: 0.060151152312755585\n","   test auc: 0.8886753916740417\n","   test aupr: 0.5980778932571411\n","epoch 581 took 9.629769325256348s\n","   train loss: 0.013074551708996296\n","   train auc: 0.9786747097969055\n","   train aupr: 0.7825157642364502\n","Testing\n","   test loss: 0.06357525289058685\n","   test auc: 0.8886193037033081\n","   test aupr: 0.5980740189552307\n","epoch 582 took 9.621958494186401s\n","   train loss: 0.013070832006633282\n","   train auc: 0.9786979556083679\n","   train aupr: 0.7826721668243408\n","Testing\n","   test loss: 0.059836987406015396\n","   test auc: 0.8885645270347595\n","   test aupr: 0.5980730652809143\n","epoch 583 took 9.620336294174194s\n","   train loss: 0.01306888833642006\n","   train auc: 0.978722333908081\n","   train aupr: 0.7828473448753357\n","Testing\n","   test loss: 0.06013288348913193\n","   test auc: 0.8885189890861511\n","   test aupr: 0.5980879068374634\n","epoch 584 took 9.624327421188354s\n","   train loss: 0.013017769902944565\n","   train auc: 0.9787447452545166\n","   train aupr: 0.7830016613006592\n","Testing\n","   test loss: 0.0596407912671566\n","   test auc: 0.8884735107421875\n","   test aupr: 0.598102331161499\n","epoch 585 took 9.632150411605835s\n","   train loss: 0.013080737553536892\n","   train auc: 0.9787681102752686\n","   train aupr: 0.7831637859344482\n","Testing\n","   test loss: 0.057416561990976334\n","   test auc: 0.8884349465370178\n","   test aupr: 0.598128080368042\n","epoch 586 took 9.613497734069824s\n","   train loss: 0.013079221360385418\n","   train auc: 0.9787928462028503\n","   train aupr: 0.7833420634269714\n","Testing\n","   test loss: 0.06043483689427376\n","   test auc: 0.8883925080299377\n","   test aupr: 0.5981467962265015\n","epoch 587 took 9.620058298110962s\n","   train loss: 0.013023841194808483\n","   train auc: 0.9788172841072083\n","   train aupr: 0.7835213541984558\n","Testing\n","   test loss: 0.0639355331659317\n","   test auc: 0.8883349895477295\n","   test aupr: 0.5981398820877075\n","epoch 588 took 9.62209153175354s\n","   train loss: 0.013057754375040531\n","   train auc: 0.9788417816162109\n","   train aupr: 0.7837008833885193\n","Testing\n","   test loss: 0.06330849230289459\n","   test auc: 0.8882714509963989\n","   test aupr: 0.5981219410896301\n","epoch 589 took 9.617017984390259s\n","   train loss: 0.01305724959820509\n","   train auc: 0.9788646101951599\n","   train aupr: 0.7838574051856995\n","Testing\n","   test loss: 0.0616699680685997\n","   test auc: 0.8882134556770325\n","   test aupr: 0.598112940788269\n","epoch 590 took 9.634043216705322s\n","   train loss: 0.013003743253648281\n","   train auc: 0.9788872003555298\n","   train aupr: 0.7840155363082886\n","Testing\n","   test loss: 0.06148204952478409\n","   test auc: 0.8881596326828003\n","   test aupr: 0.5981117486953735\n","epoch 591 took 9.618088722229004s\n","   train loss: 0.013063817285001278\n","   train auc: 0.978910505771637\n","   train aupr: 0.7841777205467224\n","Testing\n","   test loss: 0.05859597399830818\n","   test auc: 0.888114333152771\n","   test aupr: 0.5981249809265137\n","epoch 592 took 9.631905317306519s\n","   train loss: 0.013056107796728611\n","   train auc: 0.9789347052574158\n","   train aupr: 0.7843570113182068\n","Testing\n","   test loss: 0.05885763838887215\n","   test auc: 0.8880746364593506\n","   test aupr: 0.5981489419937134\n","epoch 593 took 9.624015092849731s\n","   train loss: 0.013047879561781883\n","   train auc: 0.9789586067199707\n","   train aupr: 0.7845284938812256\n","Testing\n","   test loss: 0.06049676984548569\n","   test auc: 0.8880301117897034\n","   test aupr: 0.5981638431549072\n","epoch 594 took 9.611266613006592s\n","   train loss: 0.013063532300293446\n","   train auc: 0.9789808988571167\n","   train aupr: 0.7846774458885193\n","Testing\n","   test loss: 0.0636303499341011\n","   test auc: 0.88797527551651\n","   test aupr: 0.5981592535972595\n","epoch 595 took 9.62269115447998s\n","   train loss: 0.013026558794081211\n","   train auc: 0.9790038466453552\n","   train aupr: 0.7848426699638367\n","Testing\n","   test loss: 0.05972319841384888\n","   test auc: 0.8879220485687256\n","   test aupr: 0.5981602072715759\n","epoch 596 took 9.651138067245483s\n","   train loss: 0.013020950369536877\n","   train auc: 0.9790274500846863\n","   train aupr: 0.7850131392478943\n","Testing\n","   test loss: 0.06133263558149338\n","   test auc: 0.8878746628761292\n","   test aupr: 0.5981687307357788\n","epoch 597 took 9.618860960006714s\n","   train loss: 0.01305634155869484\n","   train auc: 0.9790496826171875\n","   train aupr: 0.7851657867431641\n","Testing\n","   test loss: 0.06058993190526962\n","   test auc: 0.8878253698348999\n","   test aupr: 0.5981698632240295\n","epoch 598 took 9.634311437606812s\n","   train loss: 0.0130100566893816\n","   train auc: 0.9790716767311096\n","   train aupr: 0.785321831703186\n","Testing\n","   test loss: 0.06238841265439987\n","   test auc: 0.8877726197242737\n","   test aupr: 0.5981656312942505\n","epoch 599 took 9.636143922805786s\n","   train loss: 0.013048571534454823\n","   train auc: 0.9790962338447571\n","   train aupr: 0.7855044603347778\n","Testing\n","   test loss: 0.05803392827510834\n","   test auc: 0.8877271413803101\n","   test aupr: 0.5981748700141907\n","epoch 600 took 9.628501415252686s\n","   train loss: 0.013018397614359856\n","   train auc: 0.9791187047958374\n","   train aupr: 0.7856621742248535\n","Testing\n","   test loss: 0.060961902141571045\n","   test auc: 0.8876842260360718\n","   test aupr: 0.5981890559196472\n","epoch 601 took 9.614088773727417s\n","   train loss: 0.013034873642027378\n","   train auc: 0.9791412949562073\n","   train aupr: 0.7858185172080994\n","Testing\n","   test loss: 0.058693911880254745\n","   test auc: 0.8876403570175171\n","   test aupr: 0.5982005000114441\n","epoch 602 took 9.635864734649658s\n","   train loss: 0.013070200569927692\n","   train auc: 0.9791629910469055\n","   train aupr: 0.7859706282615662\n","Testing\n","   test loss: 0.05965700373053551\n","   test auc: 0.8875998258590698\n","   test aupr: 0.5982189774513245\n","epoch 603 took 9.632915258407593s\n","   train loss: 0.013010523281991482\n","   train auc: 0.9791848659515381\n","   train aupr: 0.7861254215240479\n","Testing\n","   test loss: 0.054902952164411545\n","   test auc: 0.8875694274902344\n","   test aupr: 0.5982573628425598\n","epoch 604 took 9.6456139087677s\n","   train loss: 0.013010348193347454\n","   train auc: 0.9792090654373169\n","   train aupr: 0.7862973809242249\n","Testing\n","   test loss: 0.05990298092365265\n","   test auc: 0.8875378966331482\n","   test aupr: 0.5982955694198608\n","epoch 605 took 9.648947477340698s\n","   train loss: 0.01299277599900961\n","   train auc: 0.9792305827140808\n","   train aupr: 0.7864471077919006\n","Testing\n","   test loss: 0.06566978991031647\n","   test auc: 0.8874810934066772\n","   test aupr: 0.5982851386070251\n","epoch 606 took 9.636209964752197s\n","   train loss: 0.013076692819595337\n","   train auc: 0.9792521595954895\n","   train aupr: 0.7866013050079346\n","Testing\n","   test loss: 0.06454943120479584\n","   test auc: 0.8874149918556213\n","   test aupr: 0.5982537865638733\n","epoch 607 took 9.637425661087036s\n","   train loss: 0.012975354678928852\n","   train auc: 0.9792733192443848\n","   train aupr: 0.7867533564567566\n","Testing\n","   test loss: 0.06016475334763527\n","   test auc: 0.8873620629310608\n","   test aupr: 0.5982482433319092\n","epoch 608 took 9.6244215965271s\n","   train loss: 0.013000552542507648\n","   train auc: 0.9792956709861755\n","   train aupr: 0.7869110107421875\n","Testing\n","   test loss: 0.059214428067207336\n","   test auc: 0.8873220682144165\n","   test aupr: 0.5982676148414612\n","epoch 609 took 9.639464378356934s\n","   train loss: 0.013060854747891426\n","   train auc: 0.979316771030426\n","   train aupr: 0.7870556116104126\n","Testing\n","   test loss: 0.05698699131608009\n","   test auc: 0.8872878551483154\n","   test aupr: 0.5982964038848877\n","epoch 610 took 9.62616753578186s\n","   train loss: 0.01299311500042677\n","   train auc: 0.9793395400047302\n","   train aupr: 0.7872254848480225\n","Testing\n","   test loss: 0.05976336821913719\n","   test auc: 0.8872513175010681\n","   test aupr: 0.5983219742774963\n","epoch 611 took 9.615332126617432s\n","   train loss: 0.012981421314179897\n","   train auc: 0.9793606400489807\n","   train aupr: 0.7873748540878296\n","Testing\n","   test loss: 0.06012850999832153\n","   test auc: 0.8872088193893433\n","   test aupr: 0.5983362793922424\n","epoch 612 took 9.623754978179932s\n","   train loss: 0.012976821511983871\n","   train auc: 0.9793823957443237\n","   train aupr: 0.7875292301177979\n","Testing\n","   test loss: 0.061246905475854874\n","   test auc: 0.8871637582778931\n","   test aupr: 0.5983477234840393\n","epoch 613 took 9.61312198638916s\n","   train loss: 0.012982121668756008\n","   train auc: 0.9794057011604309\n","   train aupr: 0.787702202796936\n","Testing\n","   test loss: 0.06371470540761948\n","   test auc: 0.8871106505393982\n","   test aupr: 0.5983448028564453\n","epoch 614 took 9.628059387207031s\n","   train loss: 0.012969075702130795\n","   train auc: 0.9794270396232605\n","   train aupr: 0.7878482341766357\n","Testing\n","   test loss: 0.05981455370783806\n","   test auc: 0.8870604634284973\n","   test aupr: 0.5983470678329468\n","epoch 615 took 9.628413200378418s\n","   train loss: 0.013000882230699062\n","   train auc: 0.9794489741325378\n","   train aupr: 0.7880077362060547\n","Testing\n","   test loss: 0.05940958112478256\n","   test auc: 0.8870198130607605\n","   test aupr: 0.598364531993866\n","epoch 616 took 9.625768661499023s\n","   train loss: 0.012998863123357296\n","   train auc: 0.9794701933860779\n","   train aupr: 0.7881587743759155\n","Testing\n","   test loss: 0.06111559644341469\n","   test auc: 0.8869761228561401\n","   test aupr: 0.5983734726905823\n","epoch 617 took 9.59766149520874s\n","   train loss: 0.012982165440917015\n","   train auc: 0.9794911742210388\n","   train aupr: 0.7883039116859436\n","Testing\n","   test loss: 0.05922907590866089\n","   test auc: 0.886933445930481\n","   test aupr: 0.598385751247406\n","epoch 618 took 9.612867593765259s\n","   train loss: 0.01294474583119154\n","   train auc: 0.9795140624046326\n","   train aupr: 0.7884770631790161\n","Testing\n","   test loss: 0.06285608559846878\n","   test auc: 0.8868860602378845\n","   test aupr: 0.5983887314796448\n","epoch 619 took 9.622719049453735s\n","   train loss: 0.012960828840732574\n","   train auc: 0.9795342087745667\n","   train aupr: 0.7886135578155518\n","Testing\n","   test loss: 0.061758868396282196\n","   test auc: 0.8868333101272583\n","   test aupr: 0.5983830094337463\n","epoch 620 took 9.612756490707397s\n","   train loss: 0.0129619799554348\n","   train auc: 0.9795539379119873\n","   train aupr: 0.7887474894523621\n","Testing\n","   test loss: 0.06118830293416977\n","   test auc: 0.8867854475975037\n","   test aupr: 0.5983880758285522\n","epoch 621 took 9.616436004638672s\n","   train loss: 0.01295365858823061\n","   train auc: 0.9795773029327393\n","   train aupr: 0.7889251708984375\n","Testing\n","   test loss: 0.06299076974391937\n","   test auc: 0.8867356777191162\n","   test aupr: 0.5983860492706299\n","epoch 622 took 9.622698783874512s\n","   train loss: 0.01297982968389988\n","   train auc: 0.9795970320701599\n","   train aupr: 0.7890623807907104\n","Testing\n","   test loss: 0.05963696539402008\n","   test auc: 0.8866888284683228\n","   test aupr: 0.5983889102935791\n","epoch 623 took 9.618579387664795s\n","   train loss: 0.01298266090452671\n","   train auc: 0.9796180725097656\n","   train aupr: 0.789210855960846\n","Testing\n","   test loss: 0.06081514060497284\n","   test auc: 0.8866466879844666\n","   test aupr: 0.598404049873352\n","epoch 624 took 9.619932174682617s\n","   train loss: 0.012933426536619663\n","   train auc: 0.9796401858329773\n","   train aupr: 0.7893744707107544\n","Testing\n","   test loss: 0.06438753008842468\n","   test auc: 0.8865950703620911\n","   test aupr: 0.5984033346176147\n","epoch 625 took 9.619770765304565s\n","   train loss: 0.012947745621204376\n","   train auc: 0.9796607494354248\n","   train aupr: 0.7895190119743347\n","Testing\n","   test loss: 0.06270282715559006\n","   test auc: 0.8865397572517395\n","   test aupr: 0.5983943939208984\n","epoch 626 took 9.622032880783081s\n","   train loss: 0.01296919584274292\n","   train auc: 0.9796806573867798\n","   train aupr: 0.7896605730056763\n","Testing\n","   test loss: 0.059812482446432114\n","   test auc: 0.8864949941635132\n","   test aupr: 0.5984019041061401\n","epoch 627 took 9.612916707992554s\n","   train loss: 0.012937094084918499\n","   train auc: 0.979701578617096\n","   train aupr: 0.7898085117340088\n","Testing\n","   test loss: 0.0633954256772995\n","   test auc: 0.8864476680755615\n","   test aupr: 0.5984034538269043\n","epoch 628 took 9.619845867156982s\n","   train loss: 0.012979592196643353\n","   train auc: 0.9797222018241882\n","   train aupr: 0.78995680809021\n","Testing\n","   test loss: 0.059882037341594696\n","   test auc: 0.8864015340805054\n","   test aupr: 0.5984073281288147\n","epoch 629 took 9.61692762374878s\n","   train loss: 0.012934131547808647\n","   train auc: 0.9797437191009521\n","   train aupr: 0.7901175618171692\n","Testing\n","   test loss: 0.06173465773463249\n","   test auc: 0.8863583207130432\n","   test aupr: 0.5984187126159668\n","epoch 630 took 9.61543607711792s\n","   train loss: 0.012942924164235592\n","   train auc: 0.9797637462615967\n","   train aupr: 0.7902541160583496\n","Testing\n","   test loss: 0.06182881072163582\n","   test auc: 0.8863098621368408\n","   test aupr: 0.5984195470809937\n","epoch 631 took 9.660643815994263s\n","   train loss: 0.012989932671189308\n","   train auc: 0.9797838926315308\n","   train aupr: 0.790398359298706\n","Testing\n","   test loss: 0.06377296149730682\n","   test auc: 0.8862577080726624\n","   test aupr: 0.5984134078025818\n","epoch 632 took 9.621068954467773s\n","   train loss: 0.012937952764332294\n","   train auc: 0.9798049330711365\n","   train aupr: 0.7905530333518982\n","Testing\n","   test loss: 0.061131782829761505\n","   test auc: 0.8862084150314331\n","   test aupr: 0.5984105467796326\n","epoch 633 took 9.62472677230835s\n","   train loss: 0.0129135986790061\n","   train auc: 0.9798256158828735\n","   train aupr: 0.7907010316848755\n","Testing\n","   test loss: 0.06220320612192154\n","   test auc: 0.8861624002456665\n","   test aupr: 0.5984124541282654\n","epoch 634 took 9.603372573852539s\n","   train loss: 0.012916929088532925\n","   train auc: 0.9798451662063599\n","   train aupr: 0.7908377051353455\n","Testing\n","   test loss: 0.058696720749139786\n","   test auc: 0.8861218690872192\n","   test aupr: 0.5984241962432861\n","epoch 635 took 9.647506713867188s\n","   train loss: 0.012937117367982864\n","   train auc: 0.9798662662506104\n","   train aupr: 0.7909954190254211\n","Testing\n","   test loss: 0.06090312451124191\n","   test auc: 0.8860833644866943\n","   test aupr: 0.5984395742416382\n","epoch 636 took 9.613486528396606s\n","   train loss: 0.012963537126779556\n","   train auc: 0.9798852205276489\n","   train aupr: 0.7911193370819092\n","Testing\n","   test loss: 0.060663118958473206\n","   test auc: 0.8860413432121277\n","   test aupr: 0.5984474420547485\n","epoch 637 took 9.616589307785034s\n","   train loss: 0.012884499505162239\n","   train auc: 0.9799067974090576\n","   train aupr: 0.7912822365760803\n","Testing\n","   test loss: 0.06057047098875046\n","   test auc: 0.8859997987747192\n","   test aupr: 0.598459005355835\n","epoch 638 took 9.608785152435303s\n","   train loss: 0.012914699502289295\n","   train auc: 0.9799256920814514\n","   train aupr: 0.7914136052131653\n","Testing\n","   test loss: 0.06004442647099495\n","   test auc: 0.8859602212905884\n","   test aupr: 0.598476767539978\n","epoch 639 took 9.610937118530273s\n","   train loss: 0.012986303307116032\n","   train auc: 0.9799457788467407\n","   train aupr: 0.791557252407074\n","Testing\n","   test loss: 0.06185564771294594\n","   test auc: 0.885917067527771\n","   test aupr: 0.598486065864563\n","epoch 640 took 9.624179363250732s\n","   train loss: 0.012947301380336285\n","   train auc: 0.979965329170227\n","   train aupr: 0.7916942834854126\n","Testing\n","   test loss: 0.06028706580400467\n","   test auc: 0.8858739137649536\n","   test aupr: 0.5984917283058167\n","epoch 641 took 9.628110885620117s\n","   train loss: 0.012926606461405754\n","   train auc: 0.9799851179122925\n","   train aupr: 0.7918347716331482\n","Testing\n","   test loss: 0.06171731650829315\n","   test auc: 0.8858311176300049\n","   test aupr: 0.5984982252120972\n","epoch 642 took 9.635387182235718s\n","   train loss: 0.012950267642736435\n","   train auc: 0.9800052046775818\n","   train aupr: 0.7919846177101135\n","Testing\n","   test loss: 0.05832541733980179\n","   test auc: 0.8857927918434143\n","   test aupr: 0.5985146164894104\n","epoch 643 took 9.647440195083618s\n","   train loss: 0.012871893122792244\n","   train auc: 0.9800249934196472\n","   train aupr: 0.7921237349510193\n","Testing\n","   test loss: 0.06560207158327103\n","   test auc: 0.8857460021972656\n","   test aupr: 0.5985170602798462\n","epoch 644 took 9.634606838226318s\n","   train loss: 0.012901253998279572\n","   train auc: 0.9800443053245544\n","   train aupr: 0.7922614812850952\n","Testing\n","   test loss: 0.05870441719889641\n","   test auc: 0.8856996893882751\n","   test aupr: 0.5985182523727417\n","epoch 645 took 9.645780086517334s\n","   train loss: 0.01290957909077406\n","   train auc: 0.9800649285316467\n","   train aupr: 0.7924071550369263\n","Testing\n","   test loss: 0.061718881130218506\n","   test auc: 0.8856618404388428\n","   test aupr: 0.5985346436500549\n","epoch 646 took 9.639456510543823s\n","   train loss: 0.012948013842105865\n","   train auc: 0.9800848364830017\n","   train aupr: 0.7925525903701782\n","Testing\n","   test loss: 0.06339850276708603\n","   test auc: 0.8856151103973389\n","   test aupr: 0.5985344052314758\n","epoch 647 took 9.626791715621948s\n","   train loss: 0.012941830791532993\n","   train auc: 0.9801024794578552\n","   train aupr: 0.7926695942878723\n","Testing\n","   test loss: 0.060843683779239655\n","   test auc: 0.88556969165802\n","   test aupr: 0.5985373854637146\n","epoch 648 took 9.616507768630981s\n","   train loss: 0.01303038839250803\n","   train auc: 0.9801228046417236\n","   train aupr: 0.7928243279457092\n","Testing\n","   test loss: 0.06110716238617897\n","   test auc: 0.8855285048484802\n","   test aupr: 0.5985486507415771\n","epoch 649 took 9.617231130599976s\n","   train loss: 0.012901592068374157\n","   train auc: 0.9801425933837891\n","   train aupr: 0.7929643988609314\n","Testing\n","   test loss: 0.05981013923883438\n","   test auc: 0.8854897022247314\n","   test aupr: 0.5985628366470337\n","epoch 650 took 9.624953985214233s\n","   train loss: 0.012897280976176262\n","   train auc: 0.9801607728004456\n","   train aupr: 0.7930893898010254\n","Testing\n","   test loss: 0.058551471680402756\n","   test auc: 0.8854570984840393\n","   test aupr: 0.5985872149467468\n","epoch 651 took 9.619359731674194s\n","   train loss: 0.012883289717137814\n","   train auc: 0.980181872844696\n","   train aupr: 0.7932510375976562\n","Testing\n","   test loss: 0.06023123487830162\n","   test auc: 0.8854234218597412\n","   test aupr: 0.5986093878746033\n","epoch 652 took 9.61857557296753s\n","   train loss: 0.012859247624874115\n","   train auc: 0.9802002310752869\n","   train aupr: 0.7933803200721741\n","Testing\n","   test loss: 0.06249106302857399\n","   test auc: 0.8853808045387268\n","   test aupr: 0.5986166596412659\n","epoch 653 took 9.610924482345581s\n","   train loss: 0.012906632386147976\n","   train auc: 0.9802190065383911\n","   train aupr: 0.7935168147087097\n","Testing\n","   test loss: 0.06062184274196625\n","   test auc: 0.8853374123573303\n","   test aupr: 0.5986216068267822\n","epoch 654 took 9.612624883651733s\n","   train loss: 0.012925802730023861\n","   train auc: 0.9802383780479431\n","   train aupr: 0.7936580181121826\n","Testing\n","   test loss: 0.05960266292095184\n","   test auc: 0.8853000402450562\n","   test aupr: 0.5986368656158447\n","epoch 655 took 9.604983806610107s\n","   train loss: 0.012853441759943962\n","   train auc: 0.9802566170692444\n","   train aupr: 0.793783962726593\n","Testing\n","   test loss: 0.05903984606266022\n","   test auc: 0.8852667212486267\n","   test aupr: 0.5986599326133728\n","epoch 656 took 9.606991529464722s\n","   train loss: 0.012878607958555222\n","   train auc: 0.9802774786949158\n","   train aupr: 0.7939426302909851\n","Testing\n","   test loss: 0.060110535472631454\n","   test auc: 0.8852325081825256\n","   test aupr: 0.5986810922622681\n","epoch 657 took 9.619847536087036s\n","   train loss: 0.01288600079715252\n","   train auc: 0.9802951812744141\n","   train aupr: 0.7940621376037598\n","Testing\n","   test loss: 0.05930697172880173\n","   test auc: 0.8851980566978455\n","   test aupr: 0.5987038612365723\n","epoch 658 took 9.659253358840942s\n","   train loss: 0.012931070290505886\n","   train auc: 0.9803139567375183\n","   train aupr: 0.7941967844963074\n","Testing\n","   test loss: 0.06055410951375961\n","   test auc: 0.8851646184921265\n","   test aupr: 0.598727822303772\n","epoch 659 took 9.606972455978394s\n","   train loss: 0.01290952693670988\n","   train auc: 0.9803332686424255\n","   train aupr: 0.7943354845046997\n","Testing\n","   test loss: 0.05902900919318199\n","   test auc: 0.8851308822631836\n","   test aupr: 0.5987492203712463\n","epoch 660 took 9.62005090713501s\n","   train loss: 0.012905708514153957\n","   train auc: 0.9803510308265686\n","   train aupr: 0.7944579720497131\n","Testing\n","   test loss: 0.060558661818504333\n","   test auc: 0.8850966095924377\n","   test aupr: 0.5987694263458252\n","epoch 661 took 9.618369102478027s\n","   train loss: 0.012883637100458145\n","   train auc: 0.9803707599639893\n","   train aupr: 0.794600784778595\n","Testing\n","   test loss: 0.06001581996679306\n","   test auc: 0.8850613236427307\n","   test aupr: 0.5987871289253235\n","epoch 662 took 9.609208822250366s\n","   train loss: 0.01290118508040905\n","   train auc: 0.980390191078186\n","   train aupr: 0.7947462797164917\n","Testing\n","   test loss: 0.05887637287378311\n","   test auc: 0.8850294351577759\n","   test aupr: 0.5988094806671143\n","epoch 663 took 9.59986400604248s\n","   train loss: 0.012856177054345608\n","   train auc: 0.9804083704948425\n","   train aupr: 0.7948698997497559\n","Testing\n","   test loss: 0.0607428103685379\n","   test auc: 0.8849961757659912\n","   test aupr: 0.5988289713859558\n","epoch 664 took 9.594019651412964s\n","   train loss: 0.012879271060228348\n","   train auc: 0.9804266691207886\n","   train aupr: 0.7950027585029602\n","Testing\n","   test loss: 0.06013106554746628\n","   test auc: 0.8849605917930603\n","   test aupr: 0.5988436937332153\n","epoch 665 took 9.59428334236145s\n","   train loss: 0.012869506143033504\n","   train auc: 0.9804450869560242\n","   train aupr: 0.7951328754425049\n","Testing\n","   test loss: 0.060658037662506104\n","   test auc: 0.8849250078201294\n","   test aupr: 0.5988585948944092\n","epoch 666 took 9.601492166519165s\n","   train loss: 0.012881397269666195\n","   train auc: 0.9804641008377075\n","   train aupr: 0.7952713370323181\n","Testing\n","   test loss: 0.06343526393175125\n","   test auc: 0.8848820924758911\n","   test aupr: 0.5988627076148987\n","epoch 667 took 9.595187902450562s\n","   train loss: 0.012871290557086468\n","   train auc: 0.9804823398590088\n","   train aupr: 0.7954021096229553\n","Testing\n","   test loss: 0.058373358100652695\n","   test auc: 0.8848445415496826\n","   test aupr: 0.5988748073577881\n","epoch 668 took 9.597403764724731s\n","   train loss: 0.012875592336058617\n","   train auc: 0.9804995656013489\n","   train aupr: 0.7955135703086853\n","Testing\n","   test loss: 0.057660315185785294\n","   test auc: 0.8848187327384949\n","   test aupr: 0.5989071130752563\n","epoch 669 took 9.61684536933899s\n","   train loss: 0.012829636223614216\n","   train auc: 0.9805186986923218\n","   train aupr: 0.7956627011299133\n","Testing\n","   test loss: 0.0621662363409996\n","   test auc: 0.8847845792770386\n","   test aupr: 0.5989239811897278\n","epoch 670 took 9.616428136825562s\n","   train loss: 0.012851319275796413\n","   train auc: 0.9805369973182678\n","   train aupr: 0.7957909107208252\n","Testing\n","   test loss: 0.06166238337755203\n","   test auc: 0.8847423195838928\n","   test aupr: 0.5989253520965576\n","epoch 671 took 9.601657629013062s\n","   train loss: 0.012824887409806252\n","   train auc: 0.9805545806884766\n","   train aupr: 0.795916736125946\n","Testing\n","   test loss: 0.060862503945827484\n","   test auc: 0.8847036957740784\n","   test aupr: 0.5989341139793396\n","epoch 672 took 9.610640048980713s\n","   train loss: 0.012861291877925396\n","   train auc: 0.9805737137794495\n","   train aupr: 0.7960594892501831\n","Testing\n","   test loss: 0.0603734590113163\n","   test auc: 0.8846673965454102\n","   test aupr: 0.5989475846290588\n","epoch 673 took 9.613803386688232s\n","   train loss: 0.012868204154074192\n","   train auc: 0.9805908799171448\n","   train aupr: 0.7961781620979309\n","Testing\n","   test loss: 0.06353113055229187\n","   test auc: 0.8846248984336853\n","   test aupr: 0.5989511609077454\n","epoch 674 took 9.599478483200073s\n","   train loss: 0.012924009002745152\n","   train auc: 0.9806099534034729\n","   train aupr: 0.796318531036377\n","Testing\n","   test loss: 0.061881277710199356\n","   test auc: 0.884580671787262\n","   test aupr: 0.5989498496055603\n","epoch 675 took 9.612552642822266s\n","   train loss: 0.012873305939137936\n","   train auc: 0.9806279540061951\n","   train aupr: 0.7964507341384888\n","Testing\n","   test loss: 0.06669522076845169\n","   test auc: 0.8845294713973999\n","   test aupr: 0.5989330410957336\n","epoch 676 took 9.620280265808105s\n","   train loss: 0.012873800471425056\n","   train auc: 0.9806445837020874\n","   train aupr: 0.7965637445449829\n","Testing\n","   test loss: 0.05816486105322838\n","   test auc: 0.8844871520996094\n","   test aupr: 0.5989329814910889\n","epoch 677 took 9.617381811141968s\n","   train loss: 0.012853112071752548\n","   train auc: 0.9806632399559021\n","   train aupr: 0.7966994643211365\n","Testing\n","   test loss: 0.05857174098491669\n","   test auc: 0.8844605684280396\n","   test aupr: 0.5989619493484497\n","epoch 678 took 9.625212669372559s\n","   train loss: 0.01282761711627245\n","   train auc: 0.980681300163269\n","   train aupr: 0.7968327403068542\n","Testing\n","   test loss: 0.061470191925764084\n","   test auc: 0.8844267129898071\n","   test aupr: 0.5989757180213928\n","epoch 679 took 9.622225046157837s\n","   train loss: 0.012890564277768135\n","   train auc: 0.9806991815567017\n","   train aupr: 0.796963095664978\n","Testing\n","   test loss: 0.05891150236129761\n","   test auc: 0.8843926191329956\n","   test aupr: 0.5989893674850464\n","epoch 680 took 9.621736288070679s\n","   train loss: 0.01284557394683361\n","   train auc: 0.9807159304618835\n","   train aupr: 0.7970800995826721\n","Testing\n","   test loss: 0.06434346735477448\n","   test auc: 0.8843533396720886\n","   test aupr: 0.5989933013916016\n","epoch 681 took 9.620020866394043s\n","   train loss: 0.01286361925303936\n","   train auc: 0.9807338118553162\n","   train aupr: 0.7972084879875183\n","Testing\n","   test loss: 0.06172342970967293\n","   test auc: 0.8843088150024414\n","   test aupr: 0.5989911556243896\n","epoch 682 took 9.624754667282104s\n","   train loss: 0.012859731912612915\n","   train auc: 0.9807519316673279\n","   train aupr: 0.7973430156707764\n","Testing\n","   test loss: 0.06246643513441086\n","   test auc: 0.8842678070068359\n","   test aupr: 0.5989969968795776\n","epoch 683 took 9.619585514068604s\n","   train loss: 0.012779859825968742\n","   train auc: 0.9807684421539307\n","   train aupr: 0.7974539995193481\n","Testing\n","   test loss: 0.059103820472955704\n","   test auc: 0.8842316269874573\n","   test aupr: 0.5990093946456909\n","epoch 684 took 9.613287210464478s\n","   train loss: 0.012855475768446922\n","   train auc: 0.9807860851287842\n","   train aupr: 0.7975807785987854\n","Testing\n","   test loss: 0.06143101677298546\n","   test auc: 0.884196400642395\n","   test aupr: 0.5990234017372131\n","epoch 685 took 9.639219284057617s\n","   train loss: 0.012837535701692104\n","   train auc: 0.9808045029640198\n","   train aupr: 0.7977167963981628\n","Testing\n","   test loss: 0.06405849009752274\n","   test auc: 0.8841504454612732\n","   test aupr: 0.5990182161331177\n","epoch 686 took 9.61185622215271s\n","   train loss: 0.012797410599887371\n","   train auc: 0.9808211326599121\n","   train aupr: 0.7978354692459106\n","Testing\n","   test loss: 0.05967371165752411\n","   test auc: 0.8841098546981812\n","   test aupr: 0.599021852016449\n","epoch 687 took 9.617531061172485s\n","   train loss: 0.012832408770918846\n","   train auc: 0.9808379411697388\n","   train aupr: 0.7979557514190674\n","Testing\n","   test loss: 0.061741724610328674\n","   test auc: 0.8840748071670532\n","   test aupr: 0.5990356802940369\n","epoch 688 took 9.598774433135986s\n","   train loss: 0.012814797461032867\n","   train auc: 0.9808564782142639\n","   train aupr: 0.7980939149856567\n","Testing\n","   test loss: 0.061976056545972824\n","   test auc: 0.8840358257293701\n","   test aupr: 0.5990430116653442\n","epoch 689 took 9.603236675262451s\n","   train loss: 0.012838623486459255\n","   train auc: 0.9808734059333801\n","   train aupr: 0.7982142567634583\n","Testing\n","   test loss: 0.060419175773859024\n","   test auc: 0.8839991688728333\n","   test aupr: 0.5990533232688904\n","epoch 690 took 9.606188774108887s\n","   train loss: 0.012818950228393078\n","   train auc: 0.9808897972106934\n","   train aupr: 0.7983313798904419\n","Testing\n","   test loss: 0.05989799648523331\n","   test auc: 0.8839648962020874\n","   test aupr: 0.5990661382675171\n","epoch 691 took 9.599788665771484s\n","   train loss: 0.012797336094081402\n","   train auc: 0.9809074401855469\n","   train aupr: 0.7984579801559448\n","Testing\n","   test loss: 0.05922812595963478\n","   test auc: 0.8839339017868042\n","   test aupr: 0.5990853309631348\n","epoch 692 took 9.609315156936646s\n","   train loss: 0.012785614468157291\n","   train auc: 0.9809242486953735\n","   train aupr: 0.7985806465148926\n","Testing\n","   test loss: 0.0596640482544899\n","   test auc: 0.8839043378829956\n","   test aupr: 0.5991092324256897\n","epoch 693 took 9.620344638824463s\n","   train loss: 0.01279750932008028\n","   train auc: 0.9809420108795166\n","   train aupr: 0.7987110018730164\n","Testing\n","   test loss: 0.05924547091126442\n","   test auc: 0.8838746547698975\n","   test aupr: 0.599132239818573\n"],"name":"stdout"}]}]}