{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GPU-UNet_Softmax_Epochs.ipynb","provenance":[{"file_id":"1MshBmCalzqGz5xnx1H0ErFeGsBx_02XT","timestamp":1596527489779},{"file_id":"1pvSj2cOo177dqWp1y6lbiwohGTGeGVm1","timestamp":1596124850987},{"file_id":"1ko8HCMMTNlaR1MCC-1f6QL1uAaqzHXwO","timestamp":1595928299309},{"file_id":"1cjKN36peUbhC36sgbu1cp5rwTPmzAtcg","timestamp":1595852695030}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ga1Mx266ltfH"},"source":["This script is used to perform lesion segmentation in fundus photographs. The lesions that can be segmented are hard exudates, soft exudates, microaneurysms and hemorrhages. The segmentation is based on a UNet, a CNN that takes an image as an input and that outputs a probability map indicating for every pixel the probability of belonging to a certain type of lesion or not."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9eyAksxJOFsi","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597744109940,"user_tz":-120,"elapsed":2265,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"981782e8-be2d-4698-dd49-4c35268db284"},"source":["# import necessary libraries\n","\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import precision_recall_curve\n","\n","from sklearn.model_selection import train_test_split\n","\n","import time\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D-lg7YM4d9uB","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597744111231,"user_tz":-120,"elapsed":3541,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"6854efaf-fb4f-4f38-bea5-5e5af1346fad"},"source":["# define how many gpus are available and set a memmory limit\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","print(\"Number of GPUs Available: \", len(gpus))\n","for i in range(1):\n","    tf.config.experimental.set_virtual_device_configuration(gpus[i], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7900)]) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of GPUs Available:  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BeXvl4eft7zY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597744115316,"user_tz":-120,"elapsed":7615,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"182b514a-0d2d-4cf0-f301-46368465719f"},"source":["strategy = tf.distribute.MirroredStrategy()\n","# the number of replicas that is created by the strategy should be equal to the number of GPU's available\n","print ('Number of synchronized replicas created: {}'.format(strategy.num_replicas_in_sync))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","Number of synchronized replicas created: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"M85RveB_uPv3","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1597744135380,"user_tz":-120,"elapsed":27670,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"69bc33da-c576-4c15-d389-f0d8df5b2a81"},"source":["# read in train and test data in case Google DRIVE is used\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x4SbTTqBltfN","colab":{}},"source":["# read in the train and test data for a certain lesion type\n","\n","# Basepath depends on the lesion\n","# LesionType = 'SoftExudates'\n","LesionType = 'HardExudates'\n","# LesionType = 'Microaneurysms'\n","# LesionType = 'Hemorrhages'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Na905WmrltfS","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1597744140809,"user_tz":-120,"elapsed":33083,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"155685ae-a663-417b-ef52-ca611ac517b6"},"source":["# Basepath for Google DRIVE:\n","Basepath = '/content/drive/My Drive/Stage_ENT_Studios/Data/IDRiD/' + LesionType + '/Arrays/'\n","\n","# Basepath for Jupyter notebooks:\n","# Basepath = 'C:/Users/lunam/Documents/1steMaster/Stage/Data_FinalArrays/IDRiD/'+ LesionType+'/Arrays/'\n","\n","# Basepath for KILI\n","# Basepath = '/home/kili/Desktop/Data_FinalArrays/IDRiD/'+ LesionType+'/Arrays/'\n","\n","# train data\n","train_images = np.load(Basepath + 'train_images_Final.npy')\n","print('Shape train images: {}'.format(train_images.shape))\n","\n","train_annotations =  np.load(Basepath + 'train_annotations_Final.npy')\n","# train_annotations = np.expand_dims(train_annotations, axis = 3)\n","print('Shape train annotations: {}'.format(train_annotations.shape))\n","\n","train_images, test_images, train_annotations, test_annotations = train_test_split(train_images, train_annotations, test_size=0.33, random_state=42)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape train images: (324, 256, 256, 3)\n","Shape train annotations: (324, 256, 256)\n","Shape test images: (156, 256, 256, 3)\n","Shape test annotations: (156, 256, 256)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WWjV59oPltfZ","colab":{}},"source":["# path to save the model and the tensorboard logs\n","\n","# Basepath for Google DRIVE:\n","base_path = '/content/drive/My Drive/Stage_ENT_Studios/Unet/Logs/'\n","\n","# Basepath for jupyter notebooks:\n","# 'C:/Users/lunam/Documents/1steMaster/Stage/Code_Final/DR_classification/FeatureBasedClassification/Lesion_Segmentation/UNet_Softmax_GPU/Logs/' + LesionType + '/'\n","\n","# Base path for Kili\n","# base_path = '/home/kili/Desktop/FeatureBasedClassification/Lesion_Segmentation/UNet_Softmax_GPU/Logs/' + LesionType + '/'\n","\n","# direction where the tensorboard files will be stored\n","log_dir_tens = base_path + 'Tensorboard_Logs/'\n","# direction where the trained models will be stored\n","log_dir_model = base_path + 'Trained_Model/'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FICWJHVxltfh","colab":{}},"source":["# The UNet network\n","def UNet(drop_prob = 0.1, init_filters = 64):\n","    '''This function defines the original UNet network'''\n","  \n","    # initialization of the weights\n","    W_init = tf.initializers.GlorotUniform()\n","    \n","    # Unet network\n","            \n","    # LEFT part\n","    # input layer\n","    # input_image = tf.keras.layers.InputLayer(input_shape = (512,512,3))\n","    input_image = tf.keras.layers.Input(shape = (256,256,3))\n","        \n","    # Convolutional block 1\n","    conv2d_1 = tf.keras.layers.Conv2D(init_filters, (3, 3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(input_image)\n","    conv2d_2 = tf.keras.layers.Conv2D(init_filters, (3, 3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_1)\n","    pool_1 = tf.keras.layers.MaxPool2D((2, 2), (2, 2))(conv2d_2)\n","    dropout_1 = tf.keras.layers.Dropout(rate = drop_prob)(pool_1)\n","            \n","    # Convolutional block 2\n","    conv2d_3 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_1)\n","    conv2d_4 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_3)\n","    pool_2 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_4)\n","    dropout_2 = tf.keras.layers.Dropout(rate = drop_prob)(pool_2 )\n","            \n","    # Convolutional block 3\n","    conv2d_5 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_2)\n","    conv2d_6 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_5)\n","    pool_3 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_6)\n","    dropout_3 = tf.keras.layers.Dropout(rate = drop_prob)(pool_3)\n","            \n","    # Convolutional block 4\n","    conv2d_7 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_3)\n","    conv2d_8 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_7)\n","    pool_4 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_8)\n","    dropout_4 = tf.keras.layers.Dropout(rate = drop_prob)(pool_4)\n","            \n","    # MIDDLE part\n","    conv2d_9 = tf.keras.layers.Conv2D(16*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_4)\n","    conv2d_10 = tf.keras.layers.Conv2D(16*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_9)\n","            \n","            \n","    # RIGHT part\n","    # Convolutional block 1\n","    upsampling_1 = tf.keras.layers.UpSampling2D((2,2))(conv2d_10)\n","    concat_1 = tf.keras.layers.Concatenate(3)([upsampling_1, conv2d_8])\n","    dropout_5 = tf.keras.layers.Dropout(rate = drop_prob)(concat_1)\n","    conv2d_11 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_5)\n","    conv2d_12 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_11)\n","            \n","    # Convolutional block 2\n","    upsampling_2 = tf.keras.layers.UpSampling2D((2,2))(conv2d_12)\n","    concat_2 = tf.keras.layers.Concatenate(3)([upsampling_2, conv2d_6])\n","    dropout_6 = tf.keras.layers.Dropout(rate = drop_prob)(concat_2)\n","    conv2d_13 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_6)\n","    conv2d_14 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_13)\n","        \n","            \n","    # Convolutional block 3\n","    upsampling_3 = tf.keras.layers.UpSampling2D((2,2))(conv2d_14)\n","    concat_3 = tf.keras.layers.Concatenate(3)([upsampling_3,conv2d_4])\n","    dropout_7 = tf.keras.layers.Dropout(rate = drop_prob)(concat_3)\n","    conv2d_15 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_7)\n","    conv2d_16 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_15)\n","            \n","    # Convolutional block 4\n","    upsampling_4 = tf.keras.layers.UpSampling2D((2,2))(conv2d_16)\n","    concat_4 = tf.keras.layers.Concatenate(3)([upsampling_4,conv2d_2])\n","    dropout_8 = tf.keras.layers.Dropout(rate = drop_prob)(concat_4)\n","    conv2d_17 = tf.keras.layers.Conv2D(init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_8)\n","    conv2d_18 = tf.keras.layers.Conv2D(init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_17)\n","            \n","    # ouput layer\n","    output_image = tf.keras.layers.Conv2D(2, (1,1), kernel_initializer= W_init, padding = 'same')(conv2d_18)\n","\n","    # define the model\n","    model = tf.keras.Model(inputs=input_image, outputs=output_image)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uXe-N1A0JgM5","colab":{}},"source":["# define some different losses\n","\n","# loss defined as in the original UNet paper\n","# The energy function is computed by a pixel-wise soft-max over the final feature map combined with the cross-entropy loss function\n","def loss_UNet(predicted_logits, real_annotations, GlobalBatchSize):\n","    # removes the last dimension of real_annotaitons (axis channel 1 has to be removed)\n","    real_annotations = tf.cast(real_annotations, tf.int32)\n","    loss = tf.nn.sparse_softmax_cross_entropy_with_logits (logits= predicted_logits, labels= real_annotations)\n","    loss = tf.reduce_mean(loss, axis = (1,2))   \n","    return tf.reduce_sum(loss) * (1. / GlobalBatchSize)\n","\n","\n","# here it is the softmax focal cross entropy\n","def loss_sfce(predicted_logits, real_annotations, GlobalBatchSize , alpha = 0.25, gamma = 2.0):\n","    '''\n","    This type of loss function tries to avoid data imbalance in image segmentation\n","    There are two parameters alpha and gamma, the default values are indicated\n","    gamma should always be greater than or equal to 0\n","    '''\n","\n","    pred_prob = tf.nn.softmax(predicted_logits)[:,:,:,1]\n","    real_annotations = tf.cast(real_annotations, tf.float32)\n","  \n","    # classic binary cross_entropy is calculated\n","    ce = K.binary_crossentropy(real_annotations, pred_prob, from_logits= False)\n","\n","    # binary cross-entropy is multiplied with two factors: alpha and modulating factor\n","    # convert the logits predictions into probabilities\n","    alpha_factor = 1.0\n","    modulating_factor = 1.0\n","  \n","    if alpha:\n","        alpha = tf.convert_to_tensor(alpha, dtype=K.floatx())\n","        alpha_factor = real_annotations * alpha + (1 - real_annotations) * (1 - alpha)\n","\n","\n","    p_t = (real_annotations * pred_prob) + ((1 - real_annotations) * (1 - pred_prob))\n","    if gamma:\n","        gamma = tf.convert_to_tensor(gamma, dtype=K.floatx())\n","        modulating_factor = tf.pow((1.0 - p_t), gamma)\n","\n","    # compute the final loss and return\n","    loss = alpha_factor * modulating_factor * ce\n","    loss = tf.reduce_mean(loss, axis =(1,2))\n","    return tf.reduce_sum(loss) * (1. / GlobalBatchSize)\n","\n","\n","# Asymmetric similarity loss function, to balance recall and precision\n","# the larger beta, the more important the recall becomes relative to the precision\n","def loss_asl(predicted_logits, real_annotations,GlobalBatchSize, beta = 2):\n","    real_annotations = tf.cast(real_annotations, tf.float32)\n","    pred_prob = tf.nn.softmax(predicted_logits)[:,:,:,1]\n","  \n","    prod_pos = pred_prob * real_annotations\n","    sum_prod_pos = tf.reduce_sum(tf.reduce_sum(prod_pos, axis = 2), axis = 1)\n","    prod_neg_pred = (1-pred_prob) * real_annotations\n","    sum_prod_neg_pred = tf.reduce_sum(tf.reduce_sum(prod_neg_pred, axis = 2), axis = 1)\n","    prod_neg_real = (pred_prob) * (1-real_annotations)\n","    sum_prod_neg_real = tf.reduce_sum(tf.reduce_sum(prod_neg_real, axis = 2), axis = 1)\n","\n","    beta = tf.convert_to_tensor(beta, dtype=K.floatx())\n","\n","    num = (1+beta**2) * sum_prod_pos\n","    denom = (1+beta**2) *sum_prod_pos + beta**2 * sum_prod_neg_pred + sum_prod_neg_real\n","\n","    loss = num/denom\n","    return tf.reduce_sum(loss) * (1. / GlobalBatchSize)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VQg7qWs9Wo9r","colab":{}},"source":["def train_network(TrainImages, TrainAnnotations, TestImages, TestAnnotations, \n","                  Drop_Prob = 0.1, Init_Filters = 64, batch_size = 3, loss_function = 'UNet_loss', optim = 'Adam', \n","                  learning_rate = tf.Variable(1e-5, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True, print_freq = 1):\n","    '''\n","    This function trains the UNet on the indicated train data with corresponding annotations\n","    At the end the trained model is being saved\n","    '''\n","    # setting up saver for the tensorboard logs\n","    if SaveResults:\n","        # creating summary which stores the results that can be visualised with tensorboard\n","        print(\"Setting up summary writer for tensorboard...\")\n","        summary_writer = tf.summary.create_file_writer(log_dir_tens)\n","\n","    # define the train and test batches that can be fed into the network\n","    # global batch size defines the batch size over all availabel GPU's\n","    print('Creating distributed data')\n","    Global_batch_size = batch_size * strategy.num_replicas_in_sync\n","    train_batch_data  = tf.data.Dataset.from_tensor_slices((TrainImages, TrainAnnotations)).shuffle(TrainImages.shape[0]).batch(Global_batch_size) \n","    test_batch_data = tf.data.Dataset.from_tensor_slices((TestImages, TestAnnotations)).batch(Global_batch_size) \n","\n","    # distribute the data over the different GPU's\n","    train_dist_data =  strategy.experimental_distribute_dataset(train_batch_data)\n","    test_dist_data =  strategy.experimental_distribute_dataset(test_batch_data)\n","\n","    # define the model that will be used for training and for testing\n","    # the model, optimisation and loss have to be distributed among GPU's\n","    tf.compat.v1.reset_default_graph()\n","    with strategy.scope():\n","        \n","        # model\n","        print('Defining the model')\n","        model = UNet(drop_prob = Drop_Prob, init_filters = Init_Filters)\n","    \n","        # loss\n","        print('Defining loss')\n","        def compute_loss(logits, annotations):\n","            if loss_function == 'UNet_loss':\n","                loss = loss_UNet(logits, annotations, Global_batch_size)\n","            elif loss_function == 'Sfce_loss':\n","                loss = loss_sfce(logits, annotations, Global_batch_size)\n","            elif loss_function == 'Asl_loss':\n","                loss = loss_asl(logits, annotations, Global_batch_size)\n","            return loss\n","\n","        # optimization\n","        # a decaying learning rate is used\n","        steps_per_epoch = int(TrainImages.shape[0]/Global_batch_size)\n","        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate= learning_rate, decay_steps= MAX_EPOCH*steps_per_epoch*0.25, decay_rate=0.2, staircase = True)\n","        print('Defining optimization')\n","        if optim == 'Adam':\n","            train_op = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n","        elif optim == 'sgd':\n","            train_op = tf.keras.optimizers.SGD(learning_rate = lr_schedule)\n","\n","        # defining the metrics\n","        print('Defining the metrics')\n","        train_roc_auc = tf.keras.metrics.AUC(curve = 'ROC')\n","        train_pr_auc = tf.keras.metrics.AUC(curve = 'PR')\n","        test_roc_auc = tf.keras.metrics.AUC(curve = 'ROC')\n","        test_pr_auc = tf.keras.metrics.AUC(curve = 'PR')\n","\n","    # one train step is a step in which one batch of data is fed to every GPU\n","    # one train step is a step in which one batch of data is fed to every GPU\n","    def Train_Step(input):\n","\n","        with tf.GradientTape() as tape:\n","            train_images_batch, train_annotations_batch = input\n","            # make prediction with model\n","            pred_logits = model(train_images_batch, training = True)\n","            # compute loss\n","            train_err = compute_loss(pred_logits, train_annotations_batch)\n","\n","        # update model\n","        train_weights = model.trainable_variables\n","        gradients = tape.gradient(train_err, train_weights)\n","        train_op.apply_gradients(zip(gradients, train_weights))\n","        \n","        # turn into probability map\n","        train_pos_prob = tf.nn.softmax(pred_logits)[:,:,:,1]\n","            \n","        # compute auc and aupr score\n","        temp_train_annotations = tf.reshape(train_annotations_batch,[-1])\n","        temp_train_positive_prob = tf.reshape(train_pos_prob,[-1])\n","        train_roc_auc.update_state(temp_train_annotations, temp_train_positive_prob)\n","        train_pr_auc.update_state(temp_train_annotations, temp_train_positive_prob)\n","\n","        # the error per replica is returned\n","        return train_err, train_roc_auc.result(), train_pr_auc.result()\n","\n","    # for the last epoch some testing has to be done, in a test step one batch of test data is fed to every GPU\n","    def Test_Step(input):\n","        test_images_batch, test_annotations_batch = input\n","\n","        # make prediction with model\n","        pred_logits = model(test_images_batch, training = False)\n","        # compute loss\n","        test_err = compute_loss(pred_logits, test_annotations_batch)\n","\n","        # turn into probability map\n","        test_pos_prob = tf.nn.softmax(pred_logits)[:,:,:,1]\n","        \n","        # compute auc and aupr score\n","        temp_test_annotations = tf.reshape(test_annotations_batch,[-1])\n","        temp_test_positive_prob = tf.reshape(test_pos_prob,[-1])\n","        test_roc_auc.update_state(temp_test_annotations, temp_test_positive_prob)\n","        test_pr_auc.update_state(temp_test_annotations, temp_test_positive_prob)\n","    \n","        # the error per replica is returned\n","        return test_err, test_roc_auc.result(), test_pr_auc.result()\n","\n","    @tf.function\n","    def distributed_train_step(dataset_inputs):\n","        per_replica_losses, per_replica_roc_auc, per_replica_pr_auc = strategy.run(Train_Step, args=(dataset_inputs,))\n","        return per_replica_losses, per_replica_roc_auc, per_replica_pr_auc\n","\n","    @tf.function\n","    def distributed_test_step(dataset_inputs):\n","        per_replica_losses, per_replica_roc_auc, per_replica_pr_auc = strategy.run(Test_Step, args=(dataset_inputs,))\n","        return per_replica_losses, per_replica_roc_auc, per_replica_pr_auc\n","    \n","  \n","    # the train and test steps now have to be performed with the distributed strategy\n","    print('Training')\n","    for epo in range(1,MAX_EPOCH+1):\n","        start_time = time.time()\n","    \n","        n_train_steps = 0\n","        total_train_loss = 0\n","        total_train_auc = 0\n","        total_train_aupr = 0\n","        # go over all global batches\n","        for train_input_data in train_dist_data:\n","            n_train_steps+=1\n","            per_replica_train_losses, per_replica_train_roc_auc, per_replica_train_pr_auc = distributed_train_step(train_input_data)\n","            total_train_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_train_losses, axis=None)\n","            total_train_auc += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_train_roc_auc, axis=None)\n","            total_train_aupr += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_train_pr_auc, axis=None)\n","\n","        # every print frequency the train and test resutls are printed out\n","        if epo % print_freq == 0 or epo == 1 or epo == (MAX_EPOCH):\n","            \n","            # calculate the final training results for this epoch\n","            total_train_loss = total_train_loss/n_train_steps\n","            total_train_auc =  total_train_auc/n_train_steps\n","            total_train_aupr =  total_train_aupr/n_train_steps \n","\n","            # print out the train results\n","            print('epoch {} took {}s'.format(epo, time.time() - start_time))\n","            print('   train loss: {}'.format(total_train_loss))\n","            print('   train auc: {}'.format(total_train_auc))\n","            print('   train aupr: {}'.format(total_train_aupr))\n","\n","            if SaveResults:\n","                # save these values to visualize them later with tensorboard\n","                with summary_writer.as_default():\n","                    tf.summary.scalar('train_loss', total_train_loss, step = epo)\n","                    tf.summary.scalar('train_roc_auc', total_train_auc, step = epo)\n","                    tf.summary.scalar('train_pr_auc', total_train_aupr, step = epo)\n","\n","\n","            # some testing has to be done at these print frequencies\n","            print('Testing')\n","            n_test_steps = 0\n","            total_test_loss = 0\n","            total_test_auc = 0\n","            total_test_aupr = 0\n","      \n","            for test_input_data in test_dist_data:\n","                n_test_steps+=1\n","                per_replica_test_losses, per_replica_test_roc_auc, per_replica_test_pr_auc = distributed_test_step(test_input_data)\n","                total_test_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_test_losses, axis=None)\n","                total_test_auc += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_test_roc_auc, axis=None)\n","                total_test_aupr += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_test_pr_auc, axis=None)\n","\n","            total_test_loss = total_test_loss/n_test_steps\n","            total_test_auc =  total_test_auc/n_test_steps\n","            total_test_aupr =  total_test_aupr/n_test_steps\n","\n","            # print out the test results      \n","            print('   validation loss: {}'.format(total_test_loss))\n","            print('   validation auc: {}'.format(total_test_auc))\n","            print('   validation aupr: {}'.format(total_test_aupr))\n","\n","            if SaveResults:\n","                with summary_writer.as_default():\n","                    tf.summary.scalar('validation_loss', total_test_loss, step = epo)\n","                    tf.summary.scalar('validation_roc_auc', total_test_auc, step = epo)\n","                    tf.summary.scalar('validation_pr_auc', total_test_aupr, step = epo)  \n","                    summary_writer.flush()     \n","        \n","\n","        if SaveResults:\n","            # storing the model weights at two time-points\n","            if epo == int(MAX_EPOCH/2):\n","                print('Saving the intermediate model weights...')\n","                model.save_weights(log_dir_model + 'UNet_Softmax_' + str(epo) +'_epochs')\n","                print('Done')\n","\n","        if epo == MAX_EPOCH:\n","            print('Saving the model weights...')\n","            model.save_weights(log_dir_model + 'UNet_Softmax_' + str(epo) +'_epochs')\n","            print('Done')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AY41_1b53yRn","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3a6fcd23-b4da-45c8-e0ef-a4cbadc3f1f6"},"source":["train_network(train_images, train_annotations, test_images, test_annotations, Drop_Prob = 0.3, Init_Filters = 32, batch_size = 6, loss_function = 'UNet_loss', optim = 'Adam', \n","                  learning_rate = tf.Variable(1e-3, dtype=tf.float32), MAX_EPOCH = 1000, SaveResults = True)\n","\n","# (TrainImages, TrainAnnotations, TestImages, TestAnnotations, \n","#                   Drop_Prob = 0.1, Init_Filters = 64, batch_size = 3, loss_function = 'UNet_loss', optim = 'Adam', \n","#                   learning_rate = tf.Variable(1e-3, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setting up summary writer for tensorboard...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Iterator.get_next_as_optional()` instead.\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","epoch 1 took 22.443397998809814s\n","   train loss: 0.6816828846931458\n","   train auc: 0.48101672530174255\n","   train aupr: 0.01140862237662077\n","Testing\n","   test loss: 0.08668231219053268\n","   test auc: 0.6873382925987244\n","   test aupr: 0.10420048236846924\n","epoch 2 took 9.259814739227295s\n","   train loss: 0.07238847762346268\n","   train auc: 0.5902842283248901\n","   train aupr: 0.020085085183382034\n","Testing\n","   test loss: 0.0796571671962738\n","   test auc: 0.6882332563400269\n","   test aupr: 0.10955250263214111\n","epoch 3 took 9.260239839553833s\n","   train loss: 0.06273059546947479\n","   train auc: 0.6511938571929932\n","   train aupr: 0.032006070017814636\n","Testing\n","   test loss: 0.0757412239909172\n","   test auc: 0.6983540654182434\n","   test aupr: 0.12047142535448074\n","epoch 4 took 9.309028625488281s\n","   train loss: 0.05194590240716934\n","   train auc: 0.6867783069610596\n","   train aupr: 0.043213680386543274\n","Testing\n","   test loss: 0.06259013712406158\n","   test auc: 0.7157254815101624\n","   test aupr: 0.14536631107330322\n","epoch 5 took 9.359385967254639s\n","   train loss: 0.04860261082649231\n","   train auc: 0.7179515361785889\n","   train aupr: 0.059457626193761826\n","Testing\n","   test loss: 0.07323039323091507\n","   test auc: 0.7280136346817017\n","   test aupr: 0.17532096803188324\n","epoch 6 took 9.384145259857178s\n","   train loss: 0.04390854388475418\n","   train auc: 0.7416664958000183\n","   train aupr: 0.07632804661989212\n","Testing\n","   test loss: 0.05893154442310333\n","   test auc: 0.7397604584693909\n","   test aupr: 0.2043769508600235\n","epoch 7 took 9.397990703582764s\n","   train loss: 0.041087064892053604\n","   train auc: 0.7633129358291626\n","   train aupr: 0.09825724363327026\n","Testing\n","   test loss: 0.05360843986272812\n","   test auc: 0.7575244307518005\n","   test aupr: 0.23574945330619812\n","epoch 8 took 9.403146505355835s\n","   train loss: 0.037014394998550415\n","   train auc: 0.7815936803817749\n","   train aupr: 0.12107651680707932\n","Testing\n","   test loss: 0.05405239388346672\n","   test auc: 0.7728426456451416\n","   test aupr: 0.2642633318901062\n","epoch 9 took 9.442443609237671s\n","   train loss: 0.035702042281627655\n","   train auc: 0.7988241910934448\n","   train aupr: 0.1473192721605301\n","Testing\n","   test loss: 0.04232694208621979\n","   test auc: 0.7881390452384949\n","   test aupr: 0.2936278283596039\n","epoch 10 took 9.47470211982727s\n","   train loss: 0.03125373646616936\n","   train auc: 0.813265323638916\n","   train aupr: 0.17141185700893402\n","Testing\n","   test loss: 0.042581286281347275\n","   test auc: 0.8040676116943359\n","   test aupr: 0.3195144236087799\n","epoch 11 took 9.48633623123169s\n","   train loss: 0.027165710926055908\n","   train auc: 0.8262428641319275\n","   train aupr: 0.1944073587656021\n","Testing\n","   test loss: 0.03672018647193909\n","   test auc: 0.8182716369628906\n","   test aupr: 0.3446251451969147\n","epoch 12 took 9.557844638824463s\n","   train loss: 0.03065940923988819\n","   train auc: 0.8389706015586853\n","   train aupr: 0.2196122258901596\n","Testing\n","   test loss: 0.045739393681287766\n","   test auc: 0.8281773924827576\n","   test aupr: 0.36569929122924805\n","epoch 13 took 9.562418937683105s\n","   train loss: 0.025831688195466995\n","   train auc: 0.8488560318946838\n","   train aupr: 0.24014243483543396\n","Testing\n","   test loss: 0.031368423253297806\n","   test auc: 0.8377302885055542\n","   test aupr: 0.3862072229385376\n","epoch 14 took 9.57005500793457s\n","   train loss: 0.024079587310552597\n","   train auc: 0.8583002686500549\n","   train aupr: 0.2621505856513977\n","Testing\n","   test loss: 0.03683272376656532\n","   test auc: 0.847075879573822\n","   test aupr: 0.40844017267227173\n","epoch 15 took 9.555377006530762s\n","   train loss: 0.02599017694592476\n","   train auc: 0.8666592240333557\n","   train aupr: 0.2834557294845581\n","Testing\n","   test loss: 0.048835333436727524\n","   test auc: 0.851813554763794\n","   test aupr: 0.42042258381843567\n","epoch 16 took 9.580126762390137s\n","   train loss: 0.02432483620941639\n","   train auc: 0.8730038404464722\n","   train aupr: 0.30061981081962585\n","Testing\n","   test loss: 0.03201517090201378\n","   test auc: 0.8567984104156494\n","   test aupr: 0.43275657296180725\n","epoch 17 took 9.566540718078613s\n","   train loss: 0.02217590995132923\n","   train auc: 0.8798445463180542\n","   train aupr: 0.32119500637054443\n","Testing\n","   test loss: 0.031080493703484535\n","   test auc: 0.8635776042938232\n","   test aupr: 0.4498404264450073\n","epoch 18 took 9.601831436157227s\n","   train loss: 0.022299861535429955\n","   train auc: 0.8855770230293274\n","   train aupr: 0.3388220965862274\n","Testing\n","   test loss: 0.02783096209168434\n","   test auc: 0.870193362236023\n","   test aupr: 0.46599799394607544\n","epoch 19 took 9.637399673461914s\n","   train loss: 0.02147865481674671\n","   train auc: 0.8907666802406311\n","   train aupr: 0.35566869378089905\n","Testing\n","   test loss: 0.02880050800740719\n","   test auc: 0.8762108683586121\n","   test aupr: 0.4812552332878113\n","epoch 20 took 9.68550992012024s\n","   train loss: 0.019922034814953804\n","   train auc: 0.8957656025886536\n","   train aupr: 0.37327054142951965\n","Testing\n","   test loss: 0.02929018996655941\n","   test auc: 0.8812506198883057\n","   test aupr: 0.4947158992290497\n","epoch 21 took 9.674459218978882s\n","   train loss: 0.019130025058984756\n","   train auc: 0.9003317356109619\n","   train aupr: 0.3899179995059967\n","Testing\n","   test loss: 0.027194667607545853\n","   test auc: 0.8858896493911743\n","   test aupr: 0.5066469311714172\n","epoch 22 took 9.640975952148438s\n","   train loss: 0.019742045551538467\n","   train auc: 0.9042665958404541\n","   train aupr: 0.4046582877635956\n","Testing\n","   test loss: 0.0250041875988245\n","   test auc: 0.8904181718826294\n","   test aupr: 0.5183076858520508\n","epoch 23 took 9.62763500213623s\n","   train loss: 0.01878395304083824\n","   train auc: 0.9079785943031311\n","   train aupr: 0.419277548789978\n","Testing\n","   test loss: 0.025604652240872383\n","   test auc: 0.8946205377578735\n","   test aupr: 0.5296622514724731\n","epoch 24 took 9.651540994644165s\n","   train loss: 0.018283601850271225\n","   train auc: 0.9113990068435669\n","   train aupr: 0.4331173598766327\n","Testing\n","   test loss: 0.02629362791776657\n","   test auc: 0.898357629776001\n","   test aupr: 0.5398623943328857\n","epoch 25 took 9.6725594997406s\n","   train loss: 0.01892383210361004\n","   train auc: 0.9146510362625122\n","   train aupr: 0.4473375380039215\n","Testing\n","   test loss: 0.028169402852654457\n","   test auc: 0.9015511274337769\n","   test aupr: 0.5486283898353577\n","epoch 26 took 9.681899547576904s\n","   train loss: 0.01838948391377926\n","   train auc: 0.9174391627311707\n","   train aupr: 0.45951011776924133\n","Testing\n","   test loss: 0.027358807623386383\n","   test auc: 0.9043014049530029\n","   test aupr: 0.5568711757659912\n","epoch 27 took 9.7116060256958s\n","   train loss: 0.0177925992757082\n","   train auc: 0.9201783537864685\n","   train aupr: 0.4716677963733673\n","Testing\n","   test loss: 0.027713630348443985\n","   test auc: 0.9067235589027405\n","   test aupr: 0.564723551273346\n","epoch 28 took 9.731353759765625s\n","   train loss: 0.018181322142481804\n","   train auc: 0.9226773381233215\n","   train aupr: 0.4831591248512268\n","Testing\n","   test loss: 0.02648553065955639\n","   test auc: 0.9090604186058044\n","   test aupr: 0.5718448162078857\n","epoch 29 took 9.72946810722351s\n","   train loss: 0.021248994395136833\n","   train auc: 0.9246814846992493\n","   train aupr: 0.4917677640914917\n","Testing\n","   test loss: 0.03245650976896286\n","   test auc: 0.9110406637191772\n","   test aupr: 0.5770077109336853\n","epoch 30 took 9.692202806472778s\n","   train loss: 0.01876899227499962\n","   train auc: 0.9266383647918701\n","   train aupr: 0.5005395412445068\n","Testing\n","   test loss: 0.030237777158617973\n","   test auc: 0.9126307964324951\n","   test aupr: 0.5815436840057373\n","epoch 31 took 9.698878526687622s\n","   train loss: 0.018497470766305923\n","   train auc: 0.9285323619842529\n","   train aupr: 0.5090351700782776\n","Testing\n","   test loss: 0.03843666613101959\n","   test auc: 0.9134600758552551\n","   test aupr: 0.5851839780807495\n","epoch 32 took 9.689273118972778s\n","   train loss: 0.02182493358850479\n","   train auc: 0.9302834272384644\n","   train aupr: 0.5171891450881958\n","Testing\n","   test loss: 0.03392144292593002\n","   test auc: 0.9143961668014526\n","   test aupr: 0.5876986980438232\n","epoch 33 took 9.814779281616211s\n","   train loss: 0.02044578455388546\n","   train auc: 0.9316151738166809\n","   train aupr: 0.5229227542877197\n","Testing\n","   test loss: 0.028852177783846855\n","   test auc: 0.9159592986106873\n","   test aupr: 0.5913797616958618\n","epoch 34 took 9.71676778793335s\n","   train loss: 0.017810730263590813\n","   train auc: 0.9332305788993835\n","   train aupr: 0.5303250551223755\n","Testing\n","   test loss: 0.026083795353770256\n","   test auc: 0.9176856279373169\n","   test aupr: 0.5961818695068359\n","epoch 35 took 9.722298383712769s\n","   train loss: 0.01714833825826645\n","   train auc: 0.9348375797271729\n","   train aupr: 0.5381240248680115\n","Testing\n","   test loss: 0.023529749363660812\n","   test auc: 0.9195168614387512\n","   test aupr: 0.601756751537323\n","epoch 36 took 9.714398384094238s\n","   train loss: 0.017025500535964966\n","   train auc: 0.936295747756958\n","   train aupr: 0.5451978445053101\n","Testing\n","   test loss: 0.023432187736034393\n","   test auc: 0.9212824106216431\n","   test aupr: 0.6076169013977051\n","epoch 37 took 9.717053413391113s\n","   train loss: 0.016805140301585197\n","   train auc: 0.9376695156097412\n","   train aupr: 0.5519497394561768\n","Testing\n","   test loss: 0.023278644308447838\n","   test auc: 0.922987699508667\n","   test aupr: 0.6130440831184387\n","epoch 38 took 9.718844413757324s\n","   train loss: 0.016665099188685417\n","   train auc: 0.9391170740127563\n","   train aupr: 0.5596180558204651\n","Testing\n","   test loss: 0.024060923606157303\n","   test auc: 0.9245595335960388\n","   test aupr: 0.6179647445678711\n","epoch 39 took 9.701294898986816s\n","   train loss: 0.01668713241815567\n","   train auc: 0.9404492974281311\n","   train aupr: 0.5666794180870056\n","Testing\n","   test loss: 0.029239220544695854\n","   test auc: 0.9257578253746033\n","   test aupr: 0.6219215393066406\n","epoch 40 took 9.714569330215454s\n","   train loss: 0.017793599516153336\n","   train auc: 0.9415945410728455\n","   train aupr: 0.5725378394126892\n","Testing\n","   test loss: 0.03078632242977619\n","   test auc: 0.9265235662460327\n","   test aupr: 0.6248222589492798\n","epoch 41 took 9.760303020477295s\n","   train loss: 0.016471078619360924\n","   train auc: 0.9427384734153748\n","   train aupr: 0.578424334526062\n","Testing\n","   test loss: 0.03132271394133568\n","   test auc: 0.9271390438079834\n","   test aupr: 0.6275370717048645\n","epoch 42 took 9.705909490585327s\n","   train loss: 0.01642109453678131\n","   train auc: 0.9438403844833374\n","   train aupr: 0.5843690037727356\n","Testing\n","   test loss: 0.03341279178857803\n","   test auc: 0.927554726600647\n","   test aupr: 0.6298837065696716\n","epoch 43 took 9.694289445877075s\n","   train loss: 0.016118783503770828\n","   train auc: 0.9448895454406738\n","   train aupr: 0.5900037884712219\n","Testing\n","   test loss: 0.027637014165520668\n","   test auc: 0.9281326532363892\n","   test aupr: 0.6325134038925171\n","epoch 44 took 9.714994668960571s\n","   train loss: 0.016046302393078804\n","   train auc: 0.945949137210846\n","   train aupr: 0.5959160327911377\n","Testing\n","   test loss: 0.02374722808599472\n","   test auc: 0.9291368722915649\n","   test aupr: 0.6360093355178833\n","epoch 45 took 9.71625018119812s\n","   train loss: 0.01595431938767433\n","   train auc: 0.9469514489173889\n","   train aupr: 0.6015498638153076\n","Testing\n","   test loss: 0.030826615169644356\n","   test auc: 0.9299326539039612\n","   test aupr: 0.6390051245689392\n","epoch 46 took 9.704939603805542s\n","   train loss: 0.016008134931325912\n","   train auc: 0.9479038715362549\n","   train aupr: 0.6068028807640076\n","Testing\n","   test loss: 0.038589958101511\n","   test auc: 0.9300190806388855\n","   test aupr: 0.6404796242713928\n","epoch 47 took 9.703899621963501s\n","   train loss: 0.01605786569416523\n","   train auc: 0.9487491250038147\n","   train aupr: 0.6113558411598206\n","Testing\n","   test loss: 0.028805943205952644\n","   test auc: 0.9302133321762085\n","   test aupr: 0.6421299576759338\n","epoch 48 took 9.72731900215149s\n","   train loss: 0.01574842445552349\n","   train auc: 0.9496297240257263\n","   train aupr: 0.6162320375442505\n","Testing\n","   test loss: 0.02851547673344612\n","   test auc: 0.9307801723480225\n","   test aupr: 0.6445855498313904\n","epoch 49 took 9.71483826637268s\n","   train loss: 0.015492923557758331\n","   train auc: 0.9504663944244385\n","   train aupr: 0.6209285855293274\n","Testing\n","   test loss: 0.029231548309326172\n","   test auc: 0.9313104152679443\n","   test aupr: 0.6469717025756836\n","epoch 50 took 9.707602977752686s\n","   train loss: 0.015512663871049881\n","   train auc: 0.9513123035430908\n","   train aupr: 0.6258630156517029\n","Testing\n","   test loss: 0.029857248067855835\n","   test auc: 0.9317394495010376\n","   test aupr: 0.6492307186126709\n","epoch 51 took 9.739568948745728s\n","   train loss: 0.015312855131924152\n","   train auc: 0.9520888924598694\n","   train aupr: 0.6303017735481262\n","Testing\n","   test loss: 0.035503216087818146\n","   test auc: 0.9319043755531311\n","   test aupr: 0.6509678363800049\n","epoch 52 took 9.740853309631348s\n","   train loss: 0.015178103931248188\n","   train auc: 0.9528335928916931\n","   train aupr: 0.6345423460006714\n","Testing\n","   test loss: 0.03550554811954498\n","   test auc: 0.9318869113922119\n","   test aupr: 0.6521400213241577\n","epoch 53 took 9.730999231338501s\n","   train loss: 0.015528859570622444\n","   train auc: 0.9535496830940247\n","   train aupr: 0.6386178135871887\n","Testing\n","   test loss: 0.0324731320142746\n","   test auc: 0.9319410920143127\n","   test aupr: 0.6534261703491211\n","epoch 54 took 9.736563205718994s\n","   train loss: 0.015434982255101204\n","   train auc: 0.9542044401168823\n","   train aupr: 0.6422681212425232\n","Testing\n","   test loss: 0.035251252353191376\n","   test auc: 0.9320089817047119\n","   test aupr: 0.6547921895980835\n","epoch 55 took 9.728455066680908s\n","   train loss: 0.015087747015058994\n","   train auc: 0.9548910856246948\n","   train aupr: 0.6463012099266052\n","Testing\n","   test loss: 0.030469996854662895\n","   test auc: 0.9321272373199463\n","   test aupr: 0.6561775207519531\n","epoch 56 took 9.745718479156494s\n","   train loss: 0.015056755393743515\n","   train auc: 0.9555145502090454\n","   train aupr: 0.6498700380325317\n","Testing\n","   test loss: 0.03051697462797165\n","   test auc: 0.9323952794075012\n","   test aupr: 0.6577779054641724\n","epoch 57 took 9.738049268722534s\n","   train loss: 0.015259494073688984\n","   train auc: 0.956188976764679\n","   train aupr: 0.6539233922958374\n","Testing\n","   test loss: 0.032226379960775375\n","   test auc: 0.9325954914093018\n","   test aupr: 0.6592690944671631\n","epoch 58 took 9.731545448303223s\n","   train loss: 0.016902461647987366\n","   train auc: 0.9567436575889587\n","   train aupr: 0.6571337580680847\n","Testing\n","   test loss: 0.03216668590903282\n","   test auc: 0.9329090714454651\n","   test aupr: 0.659450113773346\n","epoch 59 took 9.664381980895996s\n","   train loss: 0.05005376785993576\n","   train auc: 0.9560894966125488\n","   train aupr: 0.6554874777793884\n","Testing\n","   test loss: 0.06600680202245712\n","   test auc: 0.931732177734375\n","   test aupr: 0.6560326218605042\n","epoch 60 took 9.675115585327148s\n","   train loss: 0.03489680960774422\n","   train auc: 0.9552684426307678\n","   train aupr: 0.6517638564109802\n","Testing\n","   test loss: 0.056014832109212875\n","   test auc: 0.9301353096961975\n","   test aupr: 0.651023268699646\n","epoch 61 took 9.646369218826294s\n","   train loss: 0.02667163871228695\n","   train auc: 0.9550465941429138\n","   train aupr: 0.6501918435096741\n","Testing\n","   test loss: 0.049078669399023056\n","   test auc: 0.9294602870941162\n","   test aupr: 0.6478315591812134\n","epoch 62 took 9.666684865951538s\n","   train loss: 0.023282654583454132\n","   train auc: 0.9552783370018005\n","   train aupr: 0.6504192352294922\n","Testing\n","   test loss: 0.0427621454000473\n","   test auc: 0.9290082454681396\n","   test aupr: 0.6463437676429749\n","epoch 63 took 9.675507545471191s\n","   train loss: 0.021386055275797844\n","   train auc: 0.9556053876876831\n","   train aupr: 0.6513083577156067\n","Testing\n","   test loss: 0.05153948441147804\n","   test auc: 0.9284631013870239\n","   test aupr: 0.6446850299835205\n","epoch 64 took 9.67318606376648s\n","   train loss: 0.019818615168333054\n","   train auc: 0.9559794068336487\n","   train aupr: 0.6527761816978455\n","Testing\n","   test loss: 0.04710715264081955\n","   test auc: 0.9278908967971802\n","   test aupr: 0.6426464319229126\n","epoch 65 took 9.682840347290039s\n","   train loss: 0.021515870466828346\n","   train auc: 0.9563143849372864\n","   train aupr: 0.6539148688316345\n","Testing\n","   test loss: 0.05678902193903923\n","   test auc: 0.9271405935287476\n","   test aupr: 0.6407867670059204\n","epoch 66 took 9.687280893325806s\n","   train loss: 0.019439080730080605\n","   train auc: 0.9566856026649475\n","   train aupr: 0.655494213104248\n","Testing\n","   test loss: 0.05505644902586937\n","   test auc: 0.9259907007217407\n","   test aupr: 0.6386454105377197\n","epoch 67 took 9.709030866622925s\n","   train loss: 0.019185014069080353\n","   train auc: 0.9570560455322266\n","   train aupr: 0.657106339931488\n","Testing\n","   test loss: 0.03619133681058884\n","   test auc: 0.9256197810173035\n","   test aupr: 0.6377615332603455\n","epoch 68 took 9.703466653823853s\n","   train loss: 0.019171228632330894\n","   train auc: 0.9573981165885925\n","   train aupr: 0.6584997773170471\n","Testing\n","   test loss: 0.04710014536976814\n","   test auc: 0.9255001544952393\n","   test aupr: 0.6373738050460815\n","epoch 69 took 9.683996438980103s\n","   train loss: 0.01846248097717762\n","   train auc: 0.9577881097793579\n","   train aupr: 0.6604405045509338\n","Testing\n","   test loss: 0.027646753937005997\n","   test auc: 0.925621747970581\n","   test aupr: 0.6377239227294922\n","epoch 70 took 9.688624858856201s\n","   train loss: 0.018380019813776016\n","   train auc: 0.9581286311149597\n","   train aupr: 0.6619768738746643\n","Testing\n","   test loss: 0.03697033226490021\n","   test auc: 0.9260436296463013\n","   test aupr: 0.638687252998352\n","epoch 71 took 9.694610834121704s\n","   train loss: 0.019544091075658798\n","   train auc: 0.9584984183311462\n","   train aupr: 0.6638537049293518\n","Testing\n","   test loss: 0.035928331315517426\n","   test auc: 0.9262715578079224\n","   test aupr: 0.6390348672866821\n","epoch 72 took 9.673747062683105s\n","   train loss: 0.019527878612279892\n","   train auc: 0.9587839841842651\n","   train aupr: 0.6650792360305786\n","Testing\n","   test loss: 0.03528199717402458\n","   test auc: 0.9265276789665222\n","   test aupr: 0.6394938826560974\n","epoch 73 took 9.675388097763062s\n","   train loss: 0.018391070887446404\n","   train auc: 0.9591025710105896\n","   train aupr: 0.6666297316551208\n","Testing\n","   test loss: 0.04313119873404503\n","   test auc: 0.9264926314353943\n","   test aupr: 0.63941890001297\n","epoch 74 took 9.673262596130371s\n","   train loss: 0.018418805673718452\n","   train auc: 0.9594248533248901\n","   train aupr: 0.6682772040367126\n","Testing\n","   test loss: 0.0418086014688015\n","   test auc: 0.9262968897819519\n","   test aupr: 0.6389235258102417\n","epoch 75 took 9.693062782287598s\n","   train loss: 0.01720893383026123\n","   train auc: 0.959761917591095\n","   train aupr: 0.6700001358985901\n","Testing\n","   test loss: 0.04473809897899628\n","   test auc: 0.9261003136634827\n","   test aupr: 0.6383543610572815\n","epoch 76 took 9.693013668060303s\n","   train loss: 0.017076127231121063\n","   train auc: 0.9601299166679382\n","   train aupr: 0.672046959400177\n","Testing\n","   test loss: 0.03987834230065346\n","   test auc: 0.9259027242660522\n","   test aupr: 0.6380668878555298\n","epoch 77 took 9.679998874664307s\n","   train loss: 0.016679665073752403\n","   train auc: 0.9604719877243042\n","   train aupr: 0.673931360244751\n","Testing\n","   test loss: 0.03636118397116661\n","   test auc: 0.92591792345047\n","   test aupr: 0.6384559869766235\n","epoch 78 took 9.700163841247559s\n","   train loss: 0.01694670133292675\n","   train auc: 0.9608025550842285\n","   train aupr: 0.6757636070251465\n","Testing\n","   test loss: 0.04755725711584091\n","   test auc: 0.9257475733757019\n","   test aupr: 0.6381517648696899\n","epoch 79 took 9.701256513595581s\n","   train loss: 0.01814742013812065\n","   train auc: 0.961120069026947\n","   train aupr: 0.6774269342422485\n","Testing\n","   test loss: 0.040306977927684784\n","   test auc: 0.9254894256591797\n","   test aupr: 0.6376175284385681\n","epoch 80 took 9.705673694610596s\n","   train loss: 0.016851110383868217\n","   train auc: 0.9614201188087463\n","   train aupr: 0.678996205329895\n","Testing\n","   test loss: 0.04134298861026764\n","   test auc: 0.9253562092781067\n","   test aupr: 0.6376243233680725\n","epoch 81 took 9.696016788482666s\n","   train loss: 0.016496313735842705\n","   train auc: 0.9617118239402771\n","   train aupr: 0.6805508136749268\n","Testing\n","   test loss: 0.04083077609539032\n","   test auc: 0.92518150806427\n","   test aupr: 0.6377658247947693\n","epoch 82 took 9.718822240829468s\n","   train loss: 0.016307201236486435\n","   train auc: 0.9620466828346252\n","   train aupr: 0.6825717091560364\n","Testing\n","   test loss: 0.03389584645628929\n","   test auc: 0.9251850843429565\n","   test aupr: 0.6382633447647095\n","epoch 83 took 9.698037147521973s\n","   train loss: 0.02084680087864399\n","   train auc: 0.9622647762298584\n","   train aupr: 0.68376225233078\n","Testing\n","   test loss: 0.040438637137413025\n","   test auc: 0.9252458810806274\n","   test aupr: 0.6386550068855286\n","epoch 84 took 9.69713544845581s\n","   train loss: 0.016978183761239052\n","   train auc: 0.9624831080436707\n","   train aupr: 0.684866726398468\n","Testing\n","   test loss: 0.04116980358958244\n","   test auc: 0.9251782298088074\n","   test aupr: 0.6387813091278076\n","epoch 85 took 9.698268175125122s\n","   train loss: 0.01611539162695408\n","   train auc: 0.9627938866615295\n","   train aupr: 0.6867077350616455\n","Testing\n","   test loss: 0.028700552880764008\n","   test auc: 0.9253438711166382\n","   test aupr: 0.6396052837371826\n","epoch 86 took 9.705389976501465s\n","   train loss: 0.016117040067911148\n","   train auc: 0.9630757570266724\n","   train aupr: 0.6882917881011963\n","Testing\n","   test loss: 0.0294752586632967\n","   test auc: 0.9257335066795349\n","   test aupr: 0.6410143375396729\n","epoch 87 took 9.687469959259033s\n","   train loss: 0.016087010502815247\n","   train auc: 0.9633663296699524\n","   train aupr: 0.6899901032447815\n","Testing\n","   test loss: 0.0382610559463501\n","   test auc: 0.9259181618690491\n","   test aupr: 0.6419133543968201\n","epoch 88 took 9.705223798751831s\n","   train loss: 0.015751458704471588\n","   train auc: 0.963645875453949\n","   train aupr: 0.6916466951370239\n","Testing\n","   test loss: 0.027800722047686577\n","   test auc: 0.926123321056366\n","   test aupr: 0.6428504586219788\n","epoch 89 took 9.712199687957764s\n","   train loss: 0.015682058408856392\n","   train auc: 0.9639373421669006\n","   train aupr: 0.6934166550636292\n","Testing\n","   test loss: 0.042442385107278824\n","   test auc: 0.9262007474899292\n","   test aupr: 0.64359450340271\n","epoch 90 took 9.708572626113892s\n","   train loss: 0.015548535622656345\n","   train auc: 0.9642273783683777\n","   train aupr: 0.695209264755249\n","Testing\n","   test loss: 0.03517788648605347\n","   test auc: 0.9261256456375122\n","   test aupr: 0.6439952254295349\n","epoch 91 took 9.700983047485352s\n","   train loss: 0.01531449519097805\n","   train auc: 0.9644996523857117\n","   train aupr: 0.6968570351600647\n","Testing\n","   test loss: 0.04090176150202751\n","   test auc: 0.9260900020599365\n","   test aupr: 0.6444777250289917\n","epoch 92 took 9.710700511932373s\n","   train loss: 0.01520967110991478\n","   train auc: 0.9647529721260071\n","   train aupr: 0.6983222961425781\n","Testing\n","   test loss: 0.04352245479822159\n","   test auc: 0.9259092807769775\n","   test aupr: 0.6445457935333252\n","epoch 93 took 9.710766315460205s\n","   train loss: 0.01504694763571024\n","   train auc: 0.965045154094696\n","   train aupr: 0.7002025842666626\n","Testing\n","   test loss: 0.03767899423837662\n","   test auc: 0.9257878065109253\n","   test aupr: 0.6447690725326538\n","epoch 94 took 9.741943120956421s\n","   train loss: 0.016146399080753326\n","   train auc: 0.9652949571609497\n","   train aupr: 0.7016812562942505\n","Testing\n","   test loss: 0.05741222947835922\n","   test auc: 0.9253365397453308\n","   test aupr: 0.6442466974258423\n","epoch 95 took 9.706756830215454s\n","   train loss: 0.02499021589756012\n","   train auc: 0.9654581546783447\n","   train aupr: 0.7025993466377258\n","Testing\n","   test loss: 0.037236280739307404\n","   test auc: 0.9250710010528564\n","   test aupr: 0.643585205078125\n","epoch 96 took 9.684978246688843s\n","   train loss: 0.020898906514048576\n","   train auc: 0.9653853178024292\n","   train aupr: 0.702276349067688\n","Testing\n","   test loss: 0.03907845541834831\n","   test auc: 0.9251940250396729\n","   test aupr: 0.6436505913734436\n","epoch 97 took 9.712329864501953s\n","   train loss: 0.01703067123889923\n","   train auc: 0.9655693173408508\n","   train aupr: 0.7031469941139221\n","Testing\n","   test loss: 0.03261777386069298\n","   test auc: 0.9253504872322083\n","   test aupr: 0.6441279649734497\n","epoch 98 took 9.662117719650269s\n","   train loss: 0.016193479299545288\n","   train auc: 0.9657876491546631\n","   train aupr: 0.7043861746788025\n","Testing\n","   test loss: 0.033595357090234756\n","   test auc: 0.925591230392456\n","   test aupr: 0.644888162612915\n","epoch 99 took 9.688241481781006s\n","   train loss: 0.01569877378642559\n","   train auc: 0.9660044312477112\n","   train aupr: 0.7056294083595276\n","Testing\n","   test loss: 0.033835940062999725\n","   test auc: 0.925767183303833\n","   test aupr: 0.645622193813324\n","epoch 100 took 9.684929370880127s\n","   train loss: 0.015432083047926426\n","   train auc: 0.9662273526191711\n","   train aupr: 0.706957221031189\n","Testing\n","   test loss: 0.030869808048009872\n","   test auc: 0.9259795546531677\n","   test aupr: 0.646524965763092\n","epoch 101 took 9.694051265716553s\n","   train loss: 0.015301467850804329\n","   train auc: 0.9664648771286011\n","   train aupr: 0.708444356918335\n","Testing\n","   test loss: 0.029332147911190987\n","   test auc: 0.926259458065033\n","   test aupr: 0.6475962400436401\n","epoch 102 took 9.682515621185303s\n","   train loss: 0.01746531017124653\n","   train auc: 0.9666731357574463\n","   train aupr: 0.7096184492111206\n","Testing\n","   test loss: 0.02983575314283371\n","   test auc: 0.9265811443328857\n","   test aupr: 0.6484388113021851\n","epoch 103 took 9.661883354187012s\n","   train loss: 0.029961861670017242\n","   train auc: 0.9666629433631897\n","   train aupr: 0.7095208168029785\n","Testing\n","   test loss: 0.0703258216381073\n","   test auc: 0.9261115789413452\n","   test aupr: 0.6476086378097534\n","epoch 104 took 9.655839681625366s\n","   train loss: 0.020811792463064194\n","   train auc: 0.9665989279747009\n","   train aupr: 0.7090041637420654\n","Testing\n","   test loss: 0.034761521965265274\n","   test auc: 0.9256124496459961\n","   test aupr: 0.6468257308006287\n","epoch 105 took 9.644469261169434s\n","   train loss: 0.017597300931811333\n","   train auc: 0.9667519927024841\n","   train aupr: 0.7096081376075745\n","Testing\n","   test loss: 0.05230467766523361\n","   test auc: 0.9254468679428101\n","   test aupr: 0.6464378833770752\n","epoch 106 took 9.668625593185425s\n","   train loss: 0.01661917194724083\n","   train auc: 0.9669396281242371\n","   train aupr: 0.7106024622917175\n","Testing\n","   test loss: 0.052069440484046936\n","   test auc: 0.9249890446662903\n","   test aupr: 0.6456302404403687\n","epoch 107 took 9.668795108795166s\n","   train loss: 0.01629512570798397\n","   train auc: 0.9671372771263123\n","   train aupr: 0.7117061614990234\n","Testing\n","   test loss: 0.03807183355093002\n","   test auc: 0.9247801899909973\n","   test aupr: 0.6455641984939575\n","epoch 108 took 9.659382343292236s\n","   train loss: 0.01599774695932865\n","   train auc: 0.9673295617103577\n","   train aupr: 0.7127803564071655\n","Testing\n","   test loss: 0.037277717143297195\n","   test auc: 0.9247646331787109\n","   test aupr: 0.6459182500839233\n","epoch 109 took 9.68857192993164s\n","   train loss: 0.0175356213003397\n","   train auc: 0.9674929976463318\n","   train aupr: 0.7136684060096741\n","Testing\n","   test loss: 0.03615648299455643\n","   test auc: 0.9248390793800354\n","   test aupr: 0.6463422775268555\n","epoch 110 took 9.674910068511963s\n","   train loss: 0.01607833243906498\n","   train auc: 0.967663586139679\n","   train aupr: 0.7146070003509521\n","Testing\n","   test loss: 0.03483545035123825\n","   test auc: 0.9249544739723206\n","   test aupr: 0.6468044519424438\n","epoch 111 took 9.672617435455322s\n","   train loss: 0.019296608865261078\n","   train auc: 0.9678254127502441\n","   train aupr: 0.7154744267463684\n","Testing\n","   test loss: 0.03924546018242836\n","   test auc: 0.9250116944313049\n","   test aupr: 0.6472299098968506\n","epoch 112 took 9.658615350723267s\n","   train loss: 0.017593666911125183\n","   train auc: 0.9679396152496338\n","   train aupr: 0.716032087802887\n","Testing\n","   test loss: 0.03509270399808884\n","   test auc: 0.9250831604003906\n","   test aupr: 0.6476043462753296\n","epoch 113 took 9.687469959259033s\n","   train loss: 0.016055531799793243\n","   train auc: 0.9681060910224915\n","   train aupr: 0.716978132724762\n","Testing\n","   test loss: 0.038301821798086166\n","   test auc: 0.9251354932785034\n","   test aupr: 0.6479501128196716\n","epoch 114 took 9.676515817642212s\n","   train loss: 0.015565963461995125\n","   train auc: 0.9682872295379639\n","   train aupr: 0.7180626392364502\n","Testing\n","   test loss: 0.030421825125813484\n","   test auc: 0.925227165222168\n","   test aupr: 0.6485792994499207\n","epoch 115 took 9.676819086074829s\n","   train loss: 0.015298440121114254\n","   train auc: 0.9684681296348572\n","   train aupr: 0.7191477417945862\n","Testing\n","   test loss: 0.03452194109559059\n","   test auc: 0.9253629446029663\n","   test aupr: 0.6493746638298035\n","epoch 116 took 9.675379753112793s\n","   train loss: 0.01500660926103592\n","   train auc: 0.9686428308486938\n","   train aupr: 0.7202017307281494\n","Testing\n","   test loss: 0.029185263440012932\n","   test auc: 0.9255247712135315\n","   test aupr: 0.6501871347427368\n","epoch 117 took 9.697674751281738s\n","   train loss: 0.015030529350042343\n","   train auc: 0.9688251614570618\n","   train aupr: 0.7213352918624878\n","Testing\n","   test loss: 0.04335250332951546\n","   test auc: 0.9255290627479553\n","   test aupr: 0.6507031917572021\n","epoch 118 took 9.677048206329346s\n","   train loss: 0.01496813166886568\n","   train auc: 0.9689956903457642\n","   train aupr: 0.7223384380340576\n","Testing\n","   test loss: 0.036650948226451874\n","   test auc: 0.9254189729690552\n","   test aupr: 0.651046633720398\n","epoch 119 took 9.677319288253784s\n","   train loss: 0.014882378280162811\n","   train auc: 0.9691804051399231\n","   train aupr: 0.723523736000061\n","Testing\n","   test loss: 0.04217740148305893\n","   test auc: 0.9253174662590027\n","   test aupr: 0.6514290571212769\n","epoch 120 took 9.661767959594727s\n","   train loss: 0.014842555858194828\n","   train auc: 0.969359278678894\n","   train aupr: 0.7246429920196533\n","Testing\n","   test loss: 0.04664880037307739\n","   test auc: 0.9250791072845459\n","   test aupr: 0.6515035033226013\n","epoch 121 took 9.698768138885498s\n","   train loss: 0.014643601141870022\n","   train auc: 0.969539225101471\n","   train aupr: 0.7258086800575256\n","Testing\n","   test loss: 0.034070879220962524\n","   test auc: 0.924991250038147\n","   test aupr: 0.6517956256866455\n","epoch 122 took 9.676735162734985s\n","   train loss: 0.014429955743253231\n","   train auc: 0.9697082042694092\n","   train aupr: 0.7268545627593994\n","Testing\n","   test loss: 0.03557289019227028\n","   test auc: 0.925047755241394\n","   test aupr: 0.6524104475975037\n","epoch 123 took 9.678784608840942s\n","   train loss: 0.01450023427605629\n","   train auc: 0.9698848128318787\n","   train aupr: 0.7279841899871826\n","Testing\n","   test loss: 0.03922106325626373\n","   test auc: 0.9250280857086182\n","   test aupr: 0.6528249979019165\n","epoch 124 took 9.699046850204468s\n","   train loss: 0.017310991883277893\n","   train auc: 0.9700379371643066\n","   train aupr: 0.72892165184021\n","Testing\n","   test loss: 0.03279893100261688\n","   test auc: 0.9251031875610352\n","   test aupr: 0.6533033847808838\n","epoch 125 took 9.689420223236084s\n","   train loss: 0.01712968945503235\n","   train auc: 0.9701372981071472\n","   train aupr: 0.7294865846633911\n","Testing\n","   test loss: 0.038515347987413406\n","   test auc: 0.9251774549484253\n","   test aupr: 0.653791069984436\n","epoch 126 took 9.671058177947998s\n","   train loss: 0.015531697310507298\n","   train auc: 0.9702683091163635\n","   train aupr: 0.73023521900177\n","Testing\n","   test loss: 0.03332333266735077\n","   test auc: 0.9252296686172485\n","   test aupr: 0.6542116403579712\n","epoch 127 took 9.665727615356445s\n","   train loss: 0.014794332906603813\n","   train auc: 0.9704251885414124\n","   train aupr: 0.7312366962432861\n","Testing\n","   test loss: 0.030787326395511627\n","   test auc: 0.9253798127174377\n","   test aupr: 0.6548521518707275\n","epoch 128 took 9.68972373008728s\n","   train loss: 0.014747573994100094\n","   train auc: 0.9705767035484314\n","   train aupr: 0.7321615815162659\n","Testing\n","   test loss: 0.035533491522073746\n","   test auc: 0.9254777431488037\n","   test aupr: 0.6554749608039856\n","epoch 129 took 9.677394151687622s\n","   train loss: 0.014460867270827293\n","   train auc: 0.9707293510437012\n","   train aupr: 0.7331060767173767\n","Testing\n","   test loss: 0.02960132248699665\n","   test auc: 0.925586998462677\n","   test aupr: 0.6561402082443237\n","epoch 130 took 9.667937278747559s\n","   train loss: 0.01426017191261053\n","   train auc: 0.970884382724762\n","   train aupr: 0.734102725982666\n","Testing\n","   test loss: 0.03242659196257591\n","   test auc: 0.9257380962371826\n","   test aupr: 0.6569271087646484\n","epoch 131 took 9.685385942459106s\n","   train loss: 0.0141147430986166\n","   train auc: 0.9710457921028137\n","   train aupr: 0.7351656556129456\n","Testing\n","   test loss: 0.03257874399423599\n","   test auc: 0.9258435964584351\n","   test aupr: 0.6576359272003174\n","epoch 132 took 9.69424033164978s\n","   train loss: 0.013954444788396358\n","   train auc: 0.9711970090866089\n","   train aupr: 0.7361502051353455\n","Testing\n","   test loss: 0.03373365104198456\n","   test auc: 0.925902247428894\n","   test aupr: 0.6582464575767517\n","epoch 133 took 9.678857564926147s\n","   train loss: 0.01396257895976305\n","   train auc: 0.9713497161865234\n","   train aupr: 0.7371692657470703\n","Testing\n","   test loss: 0.03525012731552124\n","   test auc: 0.9259337186813354\n","   test aupr: 0.6587873101234436\n","epoch 134 took 9.702828407287598s\n","   train loss: 0.01379888691008091\n","   train auc: 0.9715055823326111\n","   train aupr: 0.7382011413574219\n","Testing\n","   test loss: 0.03583493083715439\n","   test auc: 0.9259380102157593\n","   test aupr: 0.6592683792114258\n","epoch 135 took 9.685563325881958s\n","   train loss: 0.01390706468373537\n","   train auc: 0.9716579914093018\n","   train aupr: 0.7392218112945557\n","Testing\n","   test loss: 0.02754046395421028\n","   test auc: 0.9260553121566772\n","   test aupr: 0.6599635481834412\n","epoch 136 took 9.686524391174316s\n","   train loss: 0.013683994300663471\n","   train auc: 0.9718092679977417\n","   train aupr: 0.7402142882347107\n","Testing\n","   test loss: 0.028873832896351814\n","   test auc: 0.9262648820877075\n","   test aupr: 0.660853922367096\n","epoch 137 took 9.713890314102173s\n","   train loss: 0.013519876636564732\n","   train auc: 0.971953809261322\n","   train aupr: 0.7411658763885498\n","Testing\n","   test loss: 0.04155256226658821\n","   test auc: 0.9262746572494507\n","   test aupr: 0.6613540053367615\n","epoch 138 took 9.699957370758057s\n","   train loss: 0.013698958791792393\n","   train auc: 0.9721022248268127\n","   train aupr: 0.7421643733978271\n","Testing\n","   test loss: 0.040147025138139725\n","   test auc: 0.9261434078216553\n","   test aupr: 0.6615750193595886\n","epoch 139 took 9.703619480133057s\n","   train loss: 0.013477723114192486\n","   train auc: 0.9722420573234558\n","   train aupr: 0.7430546283721924\n","Testing\n","   test loss: 0.03947941213846207\n","   test auc: 0.926047682762146\n","   test aupr: 0.6618780493736267\n","epoch 140 took 9.693792343139648s\n","   train loss: 0.013798373751342297\n","   train auc: 0.9723911881446838\n","   train aupr: 0.7440598607063293\n","Testing\n","   test loss: 0.03295683488249779\n","   test auc: 0.9260486960411072\n","   test aupr: 0.6623735427856445\n","epoch 141 took 9.68947696685791s\n","   train loss: 0.014032816514372826\n","   train auc: 0.9725334048271179\n","   train aupr: 0.7450199723243713\n","Testing\n","   test loss: 0.03769823908805847\n","   test auc: 0.9260559678077698\n","   test aupr: 0.6628566980361938\n","epoch 142 took 9.690900802612305s\n","   train loss: 0.013933056965470314\n","   train auc: 0.9726598262786865\n","   train aupr: 0.7458144426345825\n","Testing\n","   test loss: 0.03042312152683735\n","   test auc: 0.9261182546615601\n","   test aupr: 0.6634840369224548\n","epoch 143 took 9.690489530563354s\n","   train loss: 0.013469252735376358\n","   train auc: 0.9727939367294312\n","   train aupr: 0.7466891407966614\n","Testing\n","   test loss: 0.03403107821941376\n","   test auc: 0.9262024760246277\n","   test aupr: 0.6641601324081421\n","epoch 144 took 9.680920124053955s\n","   train loss: 0.013630271889269352\n","   train auc: 0.9729387760162354\n","   train aupr: 0.7476840019226074\n","Testing\n","   test loss: 0.04755276069045067\n","   test auc: 0.9261036515235901\n","   test aupr: 0.6644171476364136\n","epoch 145 took 9.689955472946167s\n","   train loss: 0.013866941444575787\n","   train auc: 0.9730637669563293\n","   train aupr: 0.748489499092102\n","Testing\n","   test loss: 0.03304881602525711\n","   test auc: 0.9260308742523193\n","   test aupr: 0.664715051651001\n","epoch 146 took 9.694775581359863s\n","   train loss: 0.014476917684078217\n","   train auc: 0.9731886982917786\n","   train aupr: 0.7492972612380981\n","Testing\n","   test loss: 0.03828466311097145\n","   test auc: 0.9260210394859314\n","   test aupr: 0.6651659607887268\n","epoch 147 took 9.70811128616333s\n","   train loss: 0.013828246854245663\n","   train auc: 0.9733103513717651\n","   train aupr: 0.7500566840171814\n","Testing\n","   test loss: 0.02329086884856224\n","   test auc: 0.9261525273323059\n","   test aupr: 0.6658239960670471\n","epoch 148 took 9.703261613845825s\n","   train loss: 0.015701057389378548\n","   train auc: 0.9734265804290771\n","   train aupr: 0.7507873773574829\n","Testing\n","   test loss: 0.05303468927741051\n","   test auc: 0.9261044263839722\n","   test aupr: 0.6659078001976013\n","epoch 149 took 9.673329830169678s\n","   train loss: 0.017050620168447495\n","   train auc: 0.973510205745697\n","   train aupr: 0.751250684261322\n","Testing\n","   test loss: 0.03603684529662132\n","   test auc: 0.925940215587616\n","   test aupr: 0.6657708287239075\n","epoch 150 took 9.65030574798584s\n","   train loss: 0.04077348858118057\n","   train auc: 0.9734833240509033\n","   train aupr: 0.7511684894561768\n","Testing\n","   test loss: 0.07564190030097961\n","   test auc: 0.9255920648574829\n","   test aupr: 0.6643305420875549\n","epoch 151 took 9.573518753051758s\n","   train loss: 0.07017943263053894\n","   train auc: 0.972575843334198\n","   train aupr: 0.7476699352264404\n","Testing\n","   test loss: 0.08067925274372101\n","   test auc: 0.9250708818435669\n","   test aupr: 0.6609424352645874\n","epoch 152 took 9.533899307250977s\n","   train loss: 0.06639228761196136\n","   train auc: 0.9717829823493958\n","   train aupr: 0.7435409426689148\n","Testing\n","   test loss: 0.07924694567918777\n","   test auc: 0.9246054887771606\n","   test aupr: 0.6568607091903687\n","epoch 153 took 9.542569875717163s\n","   train loss: 0.06559478491544724\n","   train auc: 0.9711693525314331\n","   train aupr: 0.7393003106117249\n","Testing\n","   test loss: 0.07987309247255325\n","   test auc: 0.9240541458129883\n","   test aupr: 0.652788519859314\n","epoch 154 took 9.487303495407104s\n","   train loss: 0.06628096103668213\n","   train auc: 0.9705789089202881\n","   train aupr: 0.7354903817176819\n","Testing\n","   test loss: 0.08064354956150055\n","   test auc: 0.9235255718231201\n","   test aupr: 0.6490475535392761\n","epoch 155 took 9.495349168777466s\n","   train loss: 0.0656304657459259\n","   train auc: 0.9699524641036987\n","   train aupr: 0.7312807440757751\n","Testing\n","   test loss: 0.08266819268465042\n","   test auc: 0.9228777289390564\n","   test aupr: 0.6451756954193115\n","epoch 156 took 9.485227823257446s\n","   train loss: 0.06529002636671066\n","   train auc: 0.9694065451622009\n","   train aupr: 0.72709721326828\n","Testing\n","   test loss: 0.08182503283023834\n","   test auc: 0.922274649143219\n","   test aupr: 0.6414712071418762\n","epoch 157 took 9.494136095046997s\n","   train loss: 0.06571602821350098\n","   train auc: 0.9688701033592224\n","   train aupr: 0.7231898903846741\n","Testing\n","   test loss: 0.09423931688070297\n","   test auc: 0.9213621616363525\n","   test aupr: 0.6375160217285156\n","epoch 158 took 9.496612787246704s\n","   train loss: 0.06356128305196762\n","   train auc: 0.9682889580726624\n","   train aupr: 0.719350278377533\n","Testing\n","   test loss: 0.086502805352211\n","   test auc: 0.9202114939689636\n","   test aupr: 0.6335034966468811\n","epoch 159 took 9.504698753356934s\n","   train loss: 0.06236300989985466\n","   train auc: 0.9676983952522278\n","   train aupr: 0.715549111366272\n","Testing\n","   test loss: 0.07849523425102234\n","   test auc: 0.9193261861801147\n","   test aupr: 0.629857063293457\n","epoch 160 took 9.533564567565918s\n","   train loss: 0.057188715785741806\n","   train auc: 0.9671053886413574\n","   train aupr: 0.712073802947998\n","Testing\n","   test loss: 0.09299307316541672\n","   test auc: 0.9181833863258362\n","   test aupr: 0.6265918016433716\n","epoch 161 took 9.535723447799683s\n","   train loss: 0.05508490279316902\n","   train auc: 0.9665030837059021\n","   train aupr: 0.7090657353401184\n","Testing\n","   test loss: 0.0674256980419159\n","   test auc: 0.9172333478927612\n","   test aupr: 0.6238594651222229\n","epoch 162 took 9.535408020019531s\n","   train loss: 0.050124410539865494\n","   train auc: 0.9659765958786011\n","   train aupr: 0.7062419056892395\n","Testing\n","   test loss: 0.06771393120288849\n","   test auc: 0.9165970683097839\n","   test aupr: 0.6214539408683777\n","epoch 163 took 9.541471719741821s\n","   train loss: 0.04903891682624817\n","   train auc: 0.9655296206474304\n","   train aupr: 0.7038429975509644\n","Testing\n","   test loss: 0.10708848387002945\n","   test auc: 0.9155898690223694\n","   test aupr: 0.6191208362579346\n","epoch 164 took 9.539647102355957s\n","   train loss: 0.049035005271434784\n","   train auc: 0.9649918079376221\n","   train aupr: 0.7013186812400818\n","Testing\n","   test loss: 0.07622773200273514\n","   test auc: 0.9145510196685791\n","   test aupr: 0.6169938445091248\n","epoch 165 took 9.550356388092041s\n","   train loss: 0.04168238863348961\n","   train auc: 0.9645892977714539\n","   train aupr: 0.6993388533592224\n","Testing\n","   test loss: 0.06372940540313721\n","   test auc: 0.9138733744621277\n","   test aupr: 0.6153485774993896\n","epoch 166 took 9.533524513244629s\n","   train loss: 0.040603432804346085\n","   train auc: 0.9642811417579651\n","   train aupr: 0.6977652311325073\n","Testing\n","   test loss: 0.05997959151864052\n","   test auc: 0.9134369492530823\n","   test aupr: 0.6139689087867737\n","epoch 167 took 9.542571306228638s\n","   train loss: 0.040161848068237305\n","   train auc: 0.9639816284179688\n","   train aupr: 0.6961876153945923\n","Testing\n","   test loss: 0.06097697839140892\n","   test auc: 0.9130599498748779\n","   test aupr: 0.6126226782798767\n","epoch 168 took 9.542381286621094s\n","   train loss: 0.03734287619590759\n","   train auc: 0.963729202747345\n","   train aupr: 0.6948012113571167\n","Testing\n","   test loss: 0.06444705277681351\n","   test auc: 0.9125989079475403\n","   test aupr: 0.6112186908721924\n","epoch 169 took 9.544144868850708s\n","   train loss: 0.03811015188694\n","   train auc: 0.9634818434715271\n","   train aupr: 0.6934932470321655\n","Testing\n","   test loss: 0.06748384982347488\n","   test auc: 0.9120074510574341\n","   test aupr: 0.609796404838562\n","epoch 170 took 9.555582761764526s\n","   train loss: 0.035539448261260986\n","   train auc: 0.9632664918899536\n","   train aupr: 0.6922983527183533\n","Testing\n","   test loss: 0.07642653584480286\n","   test auc: 0.9111951589584351\n","   test aupr: 0.6081826686859131\n","epoch 171 took 9.559269189834595s\n","   train loss: 0.0349966362118721\n","   train auc: 0.9630906581878662\n","   train aupr: 0.6912787556648254\n","Testing\n","   test loss: 0.059673648327589035\n","   test auc: 0.9105640649795532\n","   test aupr: 0.606681227684021\n","epoch 172 took 9.563830137252808s\n","   train loss: 0.03460429981350899\n","   train auc: 0.9629061818122864\n","   train aupr: 0.6902259588241577\n","Testing\n","   test loss: 0.07806657999753952\n","   test auc: 0.9099741578102112\n","   test aupr: 0.6052497625350952\n","epoch 173 took 9.5654878616333s\n","   train loss: 0.03386669605970383\n","   train auc: 0.962748646736145\n","   train aupr: 0.6892677545547485\n","Testing\n","   test loss: 0.0677439495921135\n","   test auc: 0.9092432260513306\n","   test aupr: 0.6037963032722473\n","epoch 174 took 9.580030918121338s\n","   train loss: 0.03305935114622116\n","   train auc: 0.9626186490058899\n","   train aupr: 0.6884556412696838\n","Testing\n","   test loss: 0.05598596855998039\n","   test auc: 0.9088371992111206\n","   test aupr: 0.6026906371116638\n","epoch 175 took 9.574793338775635s\n","   train loss: 0.03415709361433983\n","   train auc: 0.9624602198600769\n","   train aupr: 0.6875362396240234\n","Testing\n","   test loss: 0.06320000439882278\n","   test auc: 0.9084999561309814\n","   test aupr: 0.6017104983329773\n","epoch 176 took 9.554144859313965s\n","   train loss: 0.031242651864886284\n","   train auc: 0.9623323082923889\n","   train aupr: 0.6867450475692749\n","Testing\n","   test loss: 0.06863971054553986\n","   test auc: 0.9079484939575195\n","   test aupr: 0.6005446910858154\n","epoch 177 took 9.565632343292236s\n","   train loss: 0.032856956124305725\n","   train auc: 0.9622365236282349\n","   train aupr: 0.6860968470573425\n","Testing\n","   test loss: 0.0563785582780838\n","   test auc: 0.9075251817703247\n","   test aupr: 0.5995200872421265\n","epoch 178 took 9.57565689086914s\n","   train loss: 0.03145802393555641\n","   train auc: 0.9621086120605469\n","   train aupr: 0.6853575706481934\n","Testing\n","   test loss: 0.07069598138332367\n","   test auc: 0.9070645570755005\n","   test aupr: 0.5984198451042175\n","epoch 179 took 9.568109035491943s\n","   train loss: 0.03269222006201744\n","   train auc: 0.9620087146759033\n","   train aupr: 0.684723436832428\n","Testing\n","   test loss: 0.06261201947927475\n","   test auc: 0.9065409302711487\n","   test aupr: 0.597377359867096\n","epoch 180 took 9.545655012130737s\n","   train loss: 0.03261348977684975\n","   train auc: 0.9618639349937439\n","   train aupr: 0.6839495301246643\n","Testing\n","   test loss: 0.05855438858270645\n","   test auc: 0.9062173366546631\n","   test aupr: 0.5965340733528137\n","epoch 181 took 9.562202215194702s\n","   train loss: 0.03016473911702633\n","   train auc: 0.9617595076560974\n","   train aupr: 0.6832778453826904\n","Testing\n","   test loss: 0.05736968293786049\n","   test auc: 0.9059596657752991\n","   test aupr: 0.5956644415855408\n","epoch 182 took 9.549458980560303s\n","   train loss: 0.028833063319325447\n","   train auc: 0.9617019295692444\n","   train aupr: 0.6828085780143738\n","Testing\n","   test loss: 0.058991868048906326\n","   test auc: 0.9056831002235413\n","   test aupr: 0.5948598384857178\n","epoch 183 took 9.564806699752808s\n","   train loss: 0.028117820620536804\n","   train auc: 0.9616630673408508\n","   train aupr: 0.6824765801429749\n","Testing\n","   test loss: 0.05975046381354332\n","   test auc: 0.9053800106048584\n","   test aupr: 0.5940276384353638\n","epoch 184 took 9.541495323181152s\n","   train loss: 0.028334803879261017\n","   train auc: 0.961627721786499\n","   train aupr: 0.6821661591529846\n","Testing\n","   test loss: 0.05808739364147186\n","   test auc: 0.9050877690315247\n","   test aupr: 0.593248724937439\n","epoch 185 took 9.570109844207764s\n","   train loss: 0.02902781032025814\n","   train auc: 0.9615761041641235\n","   train aupr: 0.6817622780799866\n","Testing\n","   test loss: 0.07442586123943329\n","   test auc: 0.9045745134353638\n","   test aupr: 0.5921533703804016\n","epoch 186 took 9.584728717803955s\n","   train loss: 0.028309965506196022\n","   train auc: 0.9615246057510376\n","   train aupr: 0.6813710331916809\n","Testing\n","   test loss: 0.0737229660153389\n","   test auc: 0.9038568735122681\n","   test aupr: 0.5907526612281799\n","epoch 187 took 9.569029808044434s\n","   train loss: 0.02882491424679756\n","   train auc: 0.9614830613136292\n","   train aupr: 0.6810587644577026\n","Testing\n","   test loss: 0.04479852691292763\n","   test auc: 0.9036032557487488\n","   test aupr: 0.5900048613548279\n","epoch 188 took 9.557194471359253s\n","   train loss: 0.0284907016903162\n","   train auc: 0.9614218473434448\n","   train aupr: 0.6806282997131348\n","Testing\n","   test loss: 0.05399855226278305\n","   test auc: 0.903593122959137\n","   test aupr: 0.5896453261375427\n","epoch 189 took 9.56348967552185s\n","   train loss: 0.027106883004307747\n","   train auc: 0.9614017009735107\n","   train aupr: 0.6804186701774597\n","Testing\n","   test loss: 0.0620739609003067\n","   test auc: 0.9033230543136597\n","   test aupr: 0.58890700340271\n","epoch 190 took 9.565119981765747s\n","   train loss: 0.02717684954404831\n","   train auc: 0.9613833427429199\n","   train aupr: 0.6802246570587158\n","Testing\n","   test loss: 0.055265069007873535\n","   test auc: 0.9030649662017822\n","   test aupr: 0.5881423950195312\n","epoch 191 took 9.560771226882935s\n","   train loss: 0.02672223187983036\n","   train auc: 0.9613624811172485\n","   train aupr: 0.6800024509429932\n","Testing\n","   test loss: 0.05848967283964157\n","   test auc: 0.9028447866439819\n","   test aupr: 0.5875120759010315\n","epoch 192 took 9.561127662658691s\n","   train loss: 0.02577570639550686\n","   train auc: 0.9613582491874695\n","   train aupr: 0.6798518896102905\n","Testing\n","   test loss: 0.06177038326859474\n","   test auc: 0.9025331139564514\n","   test aupr: 0.5867496132850647\n","epoch 193 took 9.565929889678955s\n","   train loss: 0.02600148692727089\n","   train auc: 0.9613549709320068\n","   train aupr: 0.6797348856925964\n","Testing\n","   test loss: 0.05146555230021477\n","   test auc: 0.902315080165863\n","   test aupr: 0.5861599445343018\n","epoch 194 took 9.548428297042847s\n","   train loss: 0.026070762425661087\n","   train auc: 0.961351752281189\n","   train aupr: 0.6796094179153442\n","Testing\n","   test loss: 0.044437944889068604\n","   test auc: 0.9023281335830688\n","   test aupr: 0.5860057473182678\n","epoch 195 took 9.557034969329834s\n","   train loss: 0.02532731182873249\n","   train auc: 0.9613651633262634\n","   train aupr: 0.6795693039894104\n","Testing\n","   test loss: 0.05781342461705208\n","   test auc: 0.9022523164749146\n","   test aupr: 0.5856958627700806\n","epoch 196 took 9.557940483093262s\n","   train loss: 0.025709887966513634\n","   train auc: 0.9613707065582275\n","   train aupr: 0.6794888973236084\n","Testing\n","   test loss: 0.0553307868540287\n","   test auc: 0.9020460844039917\n","   test aupr: 0.5851138830184937\n","epoch 197 took 9.557589769363403s\n","   train loss: 0.027229217812418938\n","   train auc: 0.9613575339317322\n","   train aupr: 0.6792963743209839\n","Testing\n","   test loss: 0.04972323030233383\n","   test auc: 0.9019591212272644\n","   test aupr: 0.5847209692001343\n","epoch 198 took 9.560333490371704s\n","   train loss: 0.02474709413945675\n","   train auc: 0.9613670110702515\n","   train aupr: 0.6792322993278503\n","Testing\n","   test loss: 0.05519598349928856\n","   test auc: 0.9018582701683044\n","   test aupr: 0.584370493888855\n","epoch 199 took 9.557182550430298s\n","   train loss: 0.024048510938882828\n","   train auc: 0.961396336555481\n","   train aupr: 0.6792839169502258\n","Testing\n","   test loss: 0.05535406619310379\n","   test auc: 0.901688277721405\n","   test aupr: 0.5839165449142456\n","epoch 200 took 9.57433533668518s\n","   train loss: 0.024802232161164284\n","   train auc: 0.9614226818084717\n","   train aupr: 0.6793025732040405\n","Testing\n","   test loss: 0.05413501709699631\n","   test auc: 0.901538610458374\n","   test aupr: 0.583440899848938\n","epoch 201 took 9.577121019363403s\n","   train loss: 0.024050263687968254\n","   train auc: 0.9614382982254028\n","   train aupr: 0.6792762875556946\n","Testing\n","   test loss: 0.04725758731365204\n","   test auc: 0.9014915823936462\n","   test aupr: 0.5831570029258728\n","epoch 202 took 9.578980207443237s\n","   train loss: 0.02402259223163128\n","   train auc: 0.9614753723144531\n","   train aupr: 0.6793510317802429\n","Testing\n","   test loss: 0.04757744073867798\n","   test auc: 0.901519775390625\n","   test aupr: 0.5830440521240234\n","epoch 203 took 9.580780506134033s\n","   train loss: 0.023674054071307182\n","   train auc: 0.9615089297294617\n","   train aupr: 0.6794121861457825\n","Testing\n","   test loss: 0.0467861071228981\n","   test auc: 0.9015637636184692\n","   test aupr: 0.5829471349716187\n","epoch 204 took 9.5781991481781s\n","   train loss: 0.023788170889019966\n","   train auc: 0.9615458250045776\n","   train aupr: 0.6794759035110474\n","Testing\n","   test loss: 0.05500490218400955\n","   test auc: 0.9015149474143982\n","   test aupr: 0.5826998353004456\n","epoch 205 took 9.578609704971313s\n","   train loss: 0.02325623854994774\n","   train auc: 0.9615872502326965\n","   train aupr: 0.679562509059906\n","Testing\n","   test loss: 0.04693163186311722\n","   test auc: 0.9014715552330017\n","   test aupr: 0.5824701189994812\n","epoch 206 took 9.571574926376343s\n","   train loss: 0.02414332889020443\n","   train auc: 0.9616184234619141\n","   train aupr: 0.6796214580535889\n","Testing\n","   test loss: 0.04637795314192772\n","   test auc: 0.9015260934829712\n","   test aupr: 0.5824177861213684\n","epoch 207 took 9.578349351882935s\n","   train loss: 0.023616420105099678\n","   train auc: 0.9616595506668091\n","   train aupr: 0.6797082424163818\n","Testing\n","   test loss: 0.06138060614466667\n","   test auc: 0.9013945460319519\n","   test aupr: 0.58211749792099\n","epoch 208 took 9.581376075744629s\n","   train loss: 0.02291383594274521\n","   train auc: 0.9617066383361816\n","   train aupr: 0.6798328161239624\n","Testing\n","   test loss: 0.04857144132256508\n","   test auc: 0.9012447595596313\n","   test aupr: 0.5818029642105103\n","epoch 209 took 9.595494747161865s\n","   train loss: 0.0231986865401268\n","   train auc: 0.9617475867271423\n","   train aupr: 0.679906964302063\n","Testing\n","   test loss: 0.05067775398492813\n","   test auc: 0.9012212753295898\n","   test aupr: 0.5816024541854858\n","epoch 210 took 9.567204236984253s\n","   train loss: 0.022649148479104042\n","   train auc: 0.9617931246757507\n","   train aupr: 0.6800564527511597\n","Testing\n","   test loss: 0.041945867240428925\n","   test auc: 0.9012726545333862\n","   test aupr: 0.5815422534942627\n","epoch 211 took 9.566941976547241s\n","   train loss: 0.023404011502861977\n","   train auc: 0.9618403911590576\n","   train aupr: 0.6801897883415222\n","Testing\n","   test loss: 0.04172647371888161\n","   test auc: 0.9014151692390442\n","   test aupr: 0.5816881060600281\n","epoch 212 took 9.558199644088745s\n","   train loss: 0.022768141701817513\n","   train auc: 0.9618880748748779\n","   train aupr: 0.680334746837616\n","Testing\n","   test loss: 0.04875754937529564\n","   test auc: 0.9014752507209778\n","   test aupr: 0.5817024111747742\n","epoch 213 took 9.567181825637817s\n","   train loss: 0.022310204803943634\n","   train auc: 0.9619358777999878\n","   train aupr: 0.6804832816123962\n","Testing\n","   test loss: 0.056766387075185776\n","   test auc: 0.9013715386390686\n","   test aupr: 0.5814493298530579\n","epoch 214 took 9.567312479019165s\n","   train loss: 0.022932186722755432\n","   train auc: 0.9619817137718201\n","   train aupr: 0.680614709854126\n","Testing\n","   test loss: 0.05557476729154587\n","   test auc: 0.9012043476104736\n","   test aupr: 0.5810945630073547\n","epoch 215 took 9.584225177764893s\n","   train loss: 0.021676894277334213\n","   train auc: 0.9620347619056702\n","   train aupr: 0.6808111667633057\n","Testing\n","   test loss: 0.0551275797188282\n","   test auc: 0.901051938533783\n","   test aupr: 0.5807701945304871\n","epoch 216 took 9.57319951057434s\n","   train loss: 0.021779349073767662\n","   train auc: 0.962095320224762\n","   train aupr: 0.6810349225997925\n","Testing\n","   test loss: 0.04152446985244751\n","   test auc: 0.9010417461395264\n","   test aupr: 0.5807163715362549\n","epoch 217 took 9.584930896759033s\n","   train loss: 0.020680859684944153\n","   train auc: 0.9621626734733582\n","   train aupr: 0.6813330054283142\n","Testing\n","   test loss: 0.05363456904888153\n","   test auc: 0.9010350704193115\n","   test aupr: 0.580696165561676\n","epoch 218 took 9.585187435150146s\n","   train loss: 0.021635496988892555\n","   train auc: 0.9622248411178589\n","   train aupr: 0.6815857291221619\n","Testing\n","   test loss: 0.04894208163022995\n","   test auc: 0.9009730219841003\n","   test aupr: 0.5805426239967346\n","epoch 219 took 9.579897403717041s\n","   train loss: 0.022150354459881783\n","   train auc: 0.9622787833213806\n","   train aupr: 0.6817823052406311\n","Testing\n","   test loss: 0.042639896273612976\n","   test auc: 0.9010072946548462\n","   test aupr: 0.5805875062942505\n","epoch 220 took 9.569322109222412s\n","   train loss: 0.02081991359591484\n","   train auc: 0.9623423218727112\n","   train aupr: 0.6820651292800903\n","Testing\n","   test loss: 0.03666972741484642\n","   test auc: 0.9011519551277161\n","   test aupr: 0.5808519721031189\n","epoch 221 took 9.575414657592773s\n","   train loss: 0.021094314754009247\n","   train auc: 0.9623996019363403\n","   train aupr: 0.6823028922080994\n","Testing\n","   test loss: 0.03871157020330429\n","   test auc: 0.9013262987136841\n","   test aupr: 0.581183671951294\n","epoch 222 took 9.580777883529663s\n","   train loss: 0.020559310913085938\n","   train auc: 0.9624717235565186\n","   train aupr: 0.6826417446136475\n","Testing\n","   test loss: 0.044834114611148834\n","   test auc: 0.9014180898666382\n","   test aupr: 0.581405520439148\n","epoch 223 took 9.571816444396973s\n","   train loss: 0.020518306642770767\n","   train auc: 0.9625381827354431\n","   train aupr: 0.68294358253479\n","Testing\n","   test loss: 0.045673731714487076\n","   test auc: 0.9014496207237244\n","   test aupr: 0.5815062522888184\n","epoch 224 took 9.585206031799316s\n","   train loss: 0.028035087510943413\n","   train auc: 0.962601363658905\n","   train aupr: 0.6832410097122192\n","Testing\n","   test loss: 0.080551378428936\n","   test auc: 0.9011062383651733\n","   test aupr: 0.5785483121871948\n","epoch 225 took 9.518039464950562s\n","   train loss: 0.04355941712856293\n","   train auc: 0.9623405337333679\n","   train aupr: 0.6821057200431824\n","Testing\n","   test loss: 0.0489799939095974\n","   test auc: 0.9007772207260132\n","   test aupr: 0.5757244229316711\n","epoch 226 took 9.533626794815063s\n","   train loss: 0.027531979605555534\n","   train auc: 0.9622260928153992\n","   train aupr: 0.681458592414856\n","Testing\n","   test loss: 0.043247099965810776\n","   test auc: 0.9008301496505737\n","   test aupr: 0.5756588578224182\n","epoch 227 took 9.541533946990967s\n","   train loss: 0.02477036416530609\n","   train auc: 0.9622392058372498\n","   train aupr: 0.6813563704490662\n","Testing\n","   test loss: 0.050439901649951935\n","   test auc: 0.9008513689041138\n","   test aupr: 0.5756380558013916\n","epoch 228 took 9.546177387237549s\n","   train loss: 0.023847557604312897\n","   train auc: 0.9622645974159241\n","   train aupr: 0.6813387274742126\n","Testing\n","   test loss: 0.04424215853214264\n","   test auc: 0.9008657932281494\n","   test aupr: 0.5755943655967712\n","epoch 229 took 9.583741188049316s\n","   train loss: 0.022954560816287994\n","   train auc: 0.9622981548309326\n","   train aupr: 0.6813881993293762\n","Testing\n","   test loss: 0.044534437358379364\n","   test auc: 0.9009393453598022\n","   test aupr: 0.5756751298904419\n","epoch 230 took 9.550853967666626s\n","   train loss: 0.023080667480826378\n","   train auc: 0.9623494148254395\n","   train aupr: 0.6815512776374817\n","Testing\n","   test loss: 0.0469580739736557\n","   test auc: 0.9009866118431091\n","   test aupr: 0.5757230520248413\n","epoch 231 took 9.566718816757202s\n","   train loss: 0.022451946511864662\n","   train auc: 0.96238112449646\n","   train aupr: 0.6816189885139465\n","Testing\n","   test loss: 0.043889448046684265\n","   test auc: 0.9010297060012817\n","   test aupr: 0.575793981552124\n","epoch 232 took 9.569337606430054s\n","   train loss: 0.02194286696612835\n","   train auc: 0.9624356031417847\n","   train aupr: 0.6818289160728455\n","Testing\n","   test loss: 0.05000009015202522\n","   test auc: 0.9010297060012817\n","   test aupr: 0.5758368372917175\n","epoch 233 took 9.560795783996582s\n","   train loss: 0.021982943639159203\n","   train auc: 0.9624866843223572\n","   train aupr: 0.6820027232170105\n","Testing\n","   test loss: 0.044780828058719635\n","   test auc: 0.901027262210846\n","   test aupr: 0.5758753418922424\n","epoch 234 took 9.566944360733032s\n","   train loss: 0.02087860368192196\n","   train auc: 0.9625434875488281\n","   train aupr: 0.6822227239608765\n","Testing\n","   test loss: 0.04148097336292267\n","   test auc: 0.901098370552063\n","   test aupr: 0.5760505199432373\n","epoch 235 took 9.571529388427734s\n","   train loss: 0.02058054506778717\n","   train auc: 0.9626064896583557\n","   train aupr: 0.6824876070022583\n","Testing\n","   test loss: 0.056142907589673996\n","   test auc: 0.9010527729988098\n","   test aupr: 0.576041579246521\n","epoch 236 took 9.560070991516113s\n","   train loss: 0.020389577373862267\n","   train auc: 0.9626758098602295\n","   train aupr: 0.682805597782135\n","Testing\n","   test loss: 0.05094842612743378\n","   test auc: 0.9009252786636353\n","   test aupr: 0.5759104490280151\n","epoch 237 took 9.55408000946045s\n","   train loss: 0.019966531544923782\n","   train auc: 0.9627506732940674\n","   train aupr: 0.6831599473953247\n","Testing\n","   test loss: 0.046244267374277115\n","   test auc: 0.900891900062561\n","   test aupr: 0.5759561657905579\n","epoch 238 took 9.572191953659058s\n","   train loss: 0.0199031513184309\n","   train auc: 0.9628220200538635\n","   train aupr: 0.6834916472434998\n","Testing\n","   test loss: 0.04099677503108978\n","   test auc: 0.9009519219398499\n","   test aupr: 0.5761449337005615\n","epoch 239 took 9.573862791061401s\n","   train loss: 0.019693246111273766\n","   train auc: 0.9628906846046448\n","   train aupr: 0.6838080286979675\n","Testing\n","   test loss: 0.04857463017106056\n","   test auc: 0.900989294052124\n","   test aupr: 0.5762689113616943\n","epoch 240 took 9.578521013259888s\n","   train loss: 0.019836368039250374\n","   train auc: 0.9629676342010498\n","   train aupr: 0.6841796040534973\n","Testing\n","   test loss: 0.05569928511977196\n","   test auc: 0.9008885622024536\n","   test aupr: 0.5761610269546509\n","epoch 241 took 9.576918840408325s\n","   train loss: 0.019673652946949005\n","   train auc: 0.9630336761474609\n","   train aupr: 0.684478223323822\n","Testing\n","   test loss: 0.0473279170691967\n","   test auc: 0.900799036026001\n","   test aupr: 0.576115071773529\n","epoch 242 took 9.58132266998291s\n","   train loss: 0.020091628655791283\n","   train auc: 0.9631134271621704\n","   train aupr: 0.6848601698875427\n","Testing\n","   test loss: 0.04232990741729736\n","   test auc: 0.9008356928825378\n","   test aupr: 0.5763199925422668\n","epoch 243 took 9.578114748001099s\n","   train loss: 0.019435105845332146\n","   train auc: 0.9631775617599487\n","   train aupr: 0.685118556022644\n","Testing\n","   test loss: 0.04157666862010956\n","   test auc: 0.9009093046188354\n","   test aupr: 0.576590895652771\n","epoch 244 took 9.558310508728027s\n","   train loss: 0.019082751125097275\n","   train auc: 0.9632559418678284\n","   train aupr: 0.6855403184890747\n","Testing\n","   test loss: 0.03554309159517288\n","   test auc: 0.9010428786277771\n","   test aupr: 0.576948344707489\n","epoch 245 took 9.567171573638916s\n","   train loss: 0.019526494666934013\n","   train auc: 0.9633303880691528\n","   train aupr: 0.6859091520309448\n","Testing\n","   test loss: 0.03652326017618179\n","   test auc: 0.9012143611907959\n","   test aupr: 0.5773884654045105\n","epoch 246 took 9.561133861541748s\n","   train loss: 0.022754814475774765\n","   train auc: 0.9633753895759583\n","   train aupr: 0.6860644817352295\n","Testing\n","   test loss: 0.043214913457632065\n","   test auc: 0.9013218283653259\n","   test aupr: 0.5777102708816528\n","epoch 247 took 9.557795763015747s\n","   train loss: 0.020023295655846596\n","   train auc: 0.9634301662445068\n","   train aupr: 0.6863337755203247\n","Testing\n","   test loss: 0.042048726230859756\n","   test auc: 0.9013957381248474\n","   test aupr: 0.5779203772544861\n","epoch 248 took 9.541780233383179s\n","   train loss: 0.019996771588921547\n","   train auc: 0.963498055934906\n","   train aupr: 0.6866474151611328\n","Testing\n","   test loss: 0.05166931822896004\n","   test auc: 0.9013949036598206\n","   test aupr: 0.57798832654953\n","epoch 249 took 9.545833826065063s\n","   train loss: 0.01966402307152748\n","   train auc: 0.9635596871376038\n","   train aupr: 0.6869350075721741\n","Testing\n","   test loss: 0.052264969795942307\n","   test auc: 0.9013046622276306\n","   test aupr: 0.5779261589050293\n","epoch 250 took 9.60117506980896s\n","   train loss: 0.01917780190706253\n","   train auc: 0.9636340737342834\n","   train aupr: 0.6873122453689575\n","Testing\n","   test loss: 0.045715589076280594\n","   test auc: 0.901274561882019\n","   test aupr: 0.5779613852500916\n","epoch 251 took 9.55939507484436s\n","   train loss: 0.01852189004421234\n","   train auc: 0.9637088775634766\n","   train aupr: 0.6876856684684753\n","Testing\n","   test loss: 0.04295298084616661\n","   test auc: 0.9013181328773499\n","   test aupr: 0.578133225440979\n","epoch 252 took 9.547083854675293s\n","   train loss: 0.018424753099679947\n","   train auc: 0.9637898206710815\n","   train aupr: 0.688130259513855\n","Testing\n","   test loss: 0.0453316792845726\n","   test auc: 0.9013598561286926\n","   test aupr: 0.5783002972602844\n","epoch 253 took 9.551788330078125s\n","   train loss: 0.01832514815032482\n","   train auc: 0.9638679027557373\n","   train aupr: 0.6885419487953186\n","Testing\n","   test loss: 0.044549744576215744\n","   test auc: 0.9013891816139221\n","   test aupr: 0.5784514546394348\n","epoch 254 took 9.563114166259766s\n","   train loss: 0.018216794356703758\n","   train auc: 0.96394944190979\n","   train aupr: 0.6889938712120056\n","Testing\n","   test loss: 0.047271728515625\n","   test auc: 0.9013997316360474\n","   test aupr: 0.5785737633705139\n","epoch 255 took 9.56356143951416s\n","   train loss: 0.01821991056203842\n","   train auc: 0.9640175700187683\n","   train aupr: 0.6893336176872253\n","Testing\n","   test loss: 0.0476340726017952\n","   test auc: 0.9013850688934326\n","   test aupr: 0.5786471962928772\n","epoch 256 took 9.571667194366455s\n","   train loss: 0.018268533051013947\n","   train auc: 0.9640960693359375\n","   train aupr: 0.6897667050361633\n","Testing\n","   test loss: 0.050841156393289566\n","   test auc: 0.9013432264328003\n","   test aupr: 0.5786710381507874\n","epoch 257 took 9.560638189315796s\n","   train loss: 0.018108196556568146\n","   train auc: 0.9641732573509216\n","   train aupr: 0.6901831030845642\n","Testing\n","   test loss: 0.04518015682697296\n","   test auc: 0.9013249278068542\n","   test aupr: 0.5787310600280762\n","epoch 258 took 9.564175605773926s\n","   train loss: 0.01805184595286846\n","   train auc: 0.964248538017273\n","   train aupr: 0.6905903816223145\n","Testing\n","   test loss: 0.046590205281972885\n","   test auc: 0.9013389945030212\n","   test aupr: 0.578856885433197\n","epoch 259 took 9.571356058120728s\n","   train loss: 0.018155904486775398\n","   train auc: 0.9643298387527466\n","   train aupr: 0.691059947013855\n","Testing\n","   test loss: 0.04885030537843704\n","   test auc: 0.9013246297836304\n","   test aupr: 0.5789278149604797\n","epoch 260 took 9.566588401794434s\n","   train loss: 0.018036242574453354\n","   train auc: 0.9644065499305725\n","   train aupr: 0.6914740204811096\n","Testing\n","   test loss: 0.046263858675956726\n","   test auc: 0.9013115167617798\n","   test aupr: 0.5790119767189026\n","epoch 261 took 9.572527170181274s\n","   train loss: 0.017960814759135246\n","   train auc: 0.964482843875885\n","   train aupr: 0.6918929815292358\n","Testing\n","   test loss: 0.04825570061802864\n","   test auc: 0.9013020992279053\n","   test aupr: 0.5791135430335999\n","epoch 262 took 9.578490972518921s\n","   train loss: 0.018084673210978508\n","   train auc: 0.9645577669143677\n","   train aupr: 0.692305326461792\n","Testing\n","   test loss: 0.04390901327133179\n","   test auc: 0.9013132452964783\n","   test aupr: 0.5792470574378967\n","epoch 263 took 9.585193872451782s\n","   train loss: 0.017985602840781212\n","   train auc: 0.9646317958831787\n","   train aupr: 0.6927025318145752\n","Testing\n","   test loss: 0.045227885246276855\n","   test auc: 0.9013459086418152\n","   test aupr: 0.5794183015823364\n","epoch 264 took 9.549679279327393s\n","   train loss: 0.01791187934577465\n","   train auc: 0.9647051692008972\n","   train aupr: 0.6931079030036926\n","Testing\n","   test loss: 0.04696232080459595\n","   test auc: 0.9013550877571106\n","   test aupr: 0.5795506238937378\n","epoch 265 took 9.579914331436157s\n","   train loss: 0.017855381593108177\n","   train auc: 0.9647827744483948\n","   train aupr: 0.6935550570487976\n","Testing\n","   test loss: 0.04752422869205475\n","   test auc: 0.9013474583625793\n","   test aupr: 0.5796473622322083\n","epoch 266 took 9.583353757858276s\n","   train loss: 0.017970314249396324\n","   train auc: 0.964857816696167\n","   train aupr: 0.6939688324928284\n","Testing\n","   test loss: 0.04633995145559311\n","   test auc: 0.9013485312461853\n","   test aupr: 0.5797485709190369\n","epoch 267 took 9.57962441444397s\n","   train loss: 0.01782742328941822\n","   train auc: 0.9649320244789124\n","   train aupr: 0.6943771839141846\n","Testing\n","   test loss: 0.04780438169836998\n","   test auc: 0.9013457894325256\n","   test aupr: 0.5798570513725281\n","epoch 268 took 9.566478967666626s\n","   train loss: 0.017915068194270134\n","   train auc: 0.9650044441223145\n","   train aupr: 0.6947783827781677\n","Testing\n","   test loss: 0.04762853682041168\n","   test auc: 0.9013333916664124\n","   test aupr: 0.5799437165260315\n","epoch 269 took 9.587320804595947s\n","   train loss: 0.017877427861094475\n","   train auc: 0.9650812745094299\n","   train aupr: 0.6952087879180908\n","Testing\n","   test loss: 0.043290793895721436\n","   test auc: 0.9013558030128479\n","   test aupr: 0.5800856947898865\n","epoch 270 took 9.574541330337524s\n","   train loss: 0.017861977219581604\n","   train auc: 0.965149998664856\n","   train aupr: 0.6955952644348145\n","Testing\n","   test loss: 0.0454435870051384\n","   test auc: 0.9013919830322266\n","   test aupr: 0.5802507996559143\n","epoch 271 took 9.579702138900757s\n","   train loss: 0.017728395760059357\n","   train auc: 0.9652218818664551\n","   train aupr: 0.6960083246231079\n","Testing\n","   test loss: 0.044773124158382416\n","   test auc: 0.9014186859130859\n","   test aupr: 0.5803949236869812\n","epoch 272 took 9.593995332717896s\n","   train loss: 0.01767893321812153\n","   train auc: 0.9652945399284363\n","   train aupr: 0.6964123845100403\n","Testing\n","   test loss: 0.046738941222429276\n","   test auc: 0.901435911655426\n","   test aupr: 0.5805374383926392\n","epoch 273 took 9.59072232246399s\n","   train loss: 0.017613733187317848\n","   train auc: 0.9653640389442444\n","   train aupr: 0.6967822313308716\n","Testing\n","   test loss: 0.04484529793262482\n","   test auc: 0.9014519453048706\n","   test aupr: 0.5806878805160522\n","epoch 274 took 9.58167314529419s\n","   train loss: 0.017680563032627106\n","   train auc: 0.9654362797737122\n","   train aupr: 0.6971843838691711\n","Testing\n","   test loss: 0.04682159051299095\n","   test auc: 0.9014635682106018\n","   test aupr: 0.5808408856391907\n","epoch 275 took 9.574078798294067s\n","   train loss: 0.017681535333395004\n","   train auc: 0.9655073881149292\n","   train aupr: 0.6975821852684021\n","Testing\n","   test loss: 0.045770447701215744\n","   test auc: 0.9014701843261719\n","   test aupr: 0.5809756517410278\n","epoch 276 took 9.585414171218872s\n","   train loss: 0.017634181305766106\n","   train auc: 0.9655762910842896\n","   train aupr: 0.6979730129241943\n","Testing\n","   test loss: 0.044218700379133224\n","   test auc: 0.901497483253479\n","   test aupr: 0.5811337232589722\n","epoch 277 took 9.561426639556885s\n","   train loss: 0.017653368413448334\n","   train auc: 0.9656472206115723\n","   train aupr: 0.6983795762062073\n","Testing\n","   test loss: 0.048607587814331055\n","   test auc: 0.9015036225318909\n","   test aupr: 0.5812502503395081\n","epoch 278 took 9.56270718574524s\n","   train loss: 0.01758185587823391\n","   train auc: 0.9657149910926819\n","   train aupr: 0.6987606883049011\n","Testing\n","   test loss: 0.04851794242858887\n","   test auc: 0.9014793038368225\n","   test aupr: 0.5813156962394714\n","epoch 279 took 9.575084924697876s\n","   train loss: 0.017497191205620766\n","   train auc: 0.9657835364341736\n","   train aupr: 0.6991389989852905\n","Testing\n","   test loss: 0.04769347235560417\n","   test auc: 0.9014575481414795\n","   test aupr: 0.5813909769058228\n","epoch 280 took 9.584778547286987s\n","   train loss: 0.017483225092291832\n","   train auc: 0.965858519077301\n","   train aupr: 0.6995822191238403\n","Testing\n","   test loss: 0.044169679284095764\n","   test auc: 0.901462972164154\n","   test aupr: 0.5815297961235046\n","epoch 281 took 9.57106900215149s\n","   train loss: 0.017431341111660004\n","   train auc: 0.9659233093261719\n","   train aupr: 0.6999402046203613\n","Testing\n","   test loss: 0.04732871428132057\n","   test auc: 0.9014723896980286\n","   test aupr: 0.5816869139671326\n","epoch 282 took 9.585733413696289s\n","   train loss: 0.01748533733189106\n","   train auc: 0.9659976959228516\n","   train aupr: 0.7003768086433411\n","Testing\n","   test loss: 0.04642276093363762\n","   test auc: 0.9014707207679749\n","   test aupr: 0.5818146467208862\n","epoch 283 took 9.599651336669922s\n","   train loss: 0.017360879108309746\n","   train auc: 0.9660652875900269\n","   train aupr: 0.7007620334625244\n","Testing\n","   test loss: 0.04826180636882782\n","   test auc: 0.9014613032341003\n","   test aupr: 0.5819268822669983\n","epoch 284 took 9.58046579360962s\n","   train loss: 0.017438091337680817\n","   train auc: 0.9661351442337036\n","   train aupr: 0.7011798620223999\n","Testing\n","   test loss: 0.04667763411998749\n","   test auc: 0.9014536738395691\n","   test aupr: 0.5820379853248596\n","epoch 285 took 9.576905965805054s\n","   train loss: 0.017329050227999687\n","   train auc: 0.9662009477615356\n","   train aupr: 0.7015419006347656\n","Testing\n","   test loss: 0.045407649129629135\n","   test auc: 0.9014638662338257\n","   test aupr: 0.5821788311004639\n","epoch 286 took 9.582152843475342s\n","   train loss: 0.017435844987630844\n","   train auc: 0.96626877784729\n","   train aupr: 0.7019379138946533\n","Testing\n","   test loss: 0.05088454857468605\n","   test auc: 0.9014451503753662\n","   test aupr: 0.5822558999061584\n","epoch 287 took 9.58139944076538s\n","   train loss: 0.017368335276842117\n","   train auc: 0.9663322567939758\n","   train aupr: 0.7022884488105774\n","Testing\n","   test loss: 0.045479290187358856\n","   test auc: 0.9014306664466858\n","   test aupr: 0.5823394060134888\n","epoch 288 took 9.580062866210938s\n","   train loss: 0.01729951612651348\n","   train auc: 0.9664000868797302\n","   train aupr: 0.7026822566986084\n","Testing\n","   test loss: 0.04674733430147171\n","   test auc: 0.9014387726783752\n","   test aupr: 0.582474946975708\n","epoch 289 took 9.572247505187988s\n","   train loss: 0.017228275537490845\n","   train auc: 0.9664679765701294\n","   train aupr: 0.7030931115150452\n","Testing\n","   test loss: 0.0433015450835228\n","   test auc: 0.9014651775360107\n","   test aupr: 0.5826504230499268\n","epoch 290 took 9.588606834411621s\n","   train loss: 0.017241213470697403\n","   train auc: 0.9665375351905823\n","   train aupr: 0.7034876942634583\n","Testing\n","   test loss: 0.044906556606292725\n","   test auc: 0.9014995098114014\n","   test aupr: 0.5828574299812317\n"],"name":"stdout"}]}]}