{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GPU-UNet_sigmoid_GridSearch.ipynb","provenance":[{"file_id":"1M93XD7BHKAeiPpN2i-FPRZctRlSdkQSJ","timestamp":1596474554095},{"file_id":"18MwkwVWCIj9INM6Kp7FFtHoVoSjUMLAG","timestamp":1596456788249},{"file_id":"1BY1DlYB6TQ4qjmpIVjZOuBaJJFOar3Ia","timestamp":1596374478474},{"file_id":"1QMAMVVMbSl6bTe6A09aprdmgYrNBEcrI","timestamp":1596123645414},{"file_id":"1cPo0a7rxvx8xVA1rLbdlTcdllub8HVn5","timestamp":1596020580331},{"file_id":"1N09tXe6iDGW9tt3_cWA_K-sYrT1Q98qq","timestamp":1596016036593},{"file_id":"1pvSj2cOo177dqWp1y6lbiwohGTGeGVm1","timestamp":1596012829809},{"file_id":"1ko8HCMMTNlaR1MCC-1f6QL1uAaqzHXwO","timestamp":1595928299309},{"file_id":"1cjKN36peUbhC36sgbu1cp5rwTPmzAtcg","timestamp":1595852695030}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ga1Mx266ltfH"},"source":["This script is used to perform lesion segmentation in fundus photographs. The lesions that can be segmented are hard exudates, soft exudates, microaneurysms and hemorrhages. The segmentation is based on a UNet, a CNN that takes an image as an input and that outputs a probability map indicating for every pixel the probability of belonging to a certain type of lesion or not."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9eyAksxJOFsi","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597685509215,"user_tz":-120,"elapsed":2228,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"a6a63659-18b7-4cf9-85b4-cbabcfbc6dd3"},"source":["# import necessary libraries\n","\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.model_selection import train_test_split\n","\n","import time\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F2MDyJaoV9ka","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597685509217,"user_tz":-120,"elapsed":2216,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"30e18738-3af3-4a52-b835-b4636206fabf"},"source":["# define how many gpus are available and set a memmory limit\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","print(\"Number of GPUs Available: \", len(gpus))\n","for i in range(len(gpus)):\n","    tf.config.experimental.set_virtual_device_configuration(gpus[i], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7900)]) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of GPUs Available:  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GEVQxEA5tlKa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597685510300,"user_tz":-120,"elapsed":3292,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"9ae9f1f4-400d-432d-965e-09cb7065f7f1"},"source":["strategy = tf.distribute.MirroredStrategy()\n","# the number of replicas that is created by the strategy should be equal to the number of GPU's available\n","print ('Number of synchronized replicas created: {}'.format(strategy.num_replicas_in_sync))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","Number of synchronized replicas created: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ce8RTd_Vti_s","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597685510301,"user_tz":-120,"elapsed":3283,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"b78903a4-be7f-4a17-e4e2-55d31f1671ec"},"source":["# # read in train and test data in case Google DRIVE is used\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x4SbTTqBltfN","colab":{}},"source":["# read in the train and test data for a certain lesion type\n","\n","# Basepath depends on the lesion\n","# LesionType = 'SoftExudates'\n","LesionType = 'HardExudates'\n","# LesionType = 'Microaneurysms'\n","# LesionType = 'Hemorrhages'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Na905WmrltfS","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597685510302,"user_tz":-120,"elapsed":3263,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"f0f8df97-0f14-404c-8d97-559dfd9b9f63"},"source":["# Basepath for Google DRIVE:\n","Basepath = '/content/drive/My Drive/Stage_ENT_Studios/Data/IDRiD/' + LesionType + '/Arrays/'\n","\n","# Basepath for Jupyter notebooks:\n","# Basepath = 'C:/Users/lunam/Documents/1steMaster/Stage/Data_FinalArrays/IDRiD/'+ LesionType+'/Arrays/'\n","\n","# Basepath for KILI\n","# Basepath = '/home/kili/Desktop/Data_FinalArrays/IDRiD/'+ LesionType+'/Arrays/'\n","\n","# train data\n","train_images = np.load(Basepath + 'train_images_Final.npy')\n","print('Shape train images: {}'.format(train_images.shape))\n","\n","train_annotations =  np.load(Basepath + 'train_annotations_Final.npy')\n","# train_annotations = np.expand_dims(train_annotations, axis = 3)\n","print('Shape train annotations: {}'.format(train_annotations.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape train images: (324, 256, 256, 3)\n","Shape train annotations: (324, 256, 256)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WWjV59oPltfZ","colab":{}},"source":["# direction where the results of the GridSearch will be stored\n","\n","# direction for Google DRIVE\n","dir_gridsearch = '/content/drive/My Drive/Stage_ENT_Studios/Unet/GridSearchResults/GridSearch_UNet_Sigmoid_'+ LesionType +'.txt'\n","\n","# direction for jupyter notebooks\n","# dir_gridsearch = 'C:/Users/lunam/Documents/1steMaster/Stage/Code_Final/DR_classification/FeatureBasedClassification/Lesion_Segmentation/UNet_Sigmoid_GPU/GridSearchResults/GridSearch/' + LesionType + '/Gridsearch_UNetSigmoid.txt'\n","\n","# direction for Kili\n","# dir_gridsearch = '/home/kili/Desktop/FeatureBasedClassification/Lesion_Segmentation/UNet_Sigmoid_GPU/GridSearchResults/GridSearch/' + LesionType + '/Gridsearch_UNetSigmoid.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FICWJHVxltfh","colab":{}},"source":["# The UNet network\n","def UNet(drop_prob = 0.1, init_filters = 64):\n","    '''This function defines the original UNet network'''\n","    # initialization of the weights\n","    W_init = tf.initializers.GlorotUniform()\n","    \n","    # Unet network\n","            \n","    # LEFT part\n","    # input layer\n","    # input_image = tf.keras.layers.InputLayer(input_shape = (512,512,3))\n","    input_image = tf.keras.layers.Input(shape = (256,256,3))\n","        \n","    # Convolutional block 1\n","    conv2d_1 = tf.keras.layers.Conv2D(init_filters, (3, 3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(input_image)\n","    conv2d_2 = tf.keras.layers.Conv2D(init_filters, (3, 3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_1)\n","    pool_1 = tf.keras.layers.MaxPool2D((2, 2), (2, 2))(conv2d_2)\n","    dropout_1 = tf.keras.layers.Dropout(rate = drop_prob)(pool_1)\n","            \n","    # Convolutional block 2\n","    conv2d_3 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_1)\n","    conv2d_4 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_3)\n","    pool_2 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_4)\n","    dropout_2 = tf.keras.layers.Dropout(rate = drop_prob)(pool_2 )\n","            \n","    # Convolutional block 3\n","    conv2d_5 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_2)\n","    conv2d_6 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_5)\n","    pool_3 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_6)\n","    dropout_3 = tf.keras.layers.Dropout(rate = drop_prob)(pool_3)\n","            \n","    # Convolutional block 4\n","    conv2d_7 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_3)\n","    conv2d_8 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_7)\n","    pool_4 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_8)\n","    dropout_4 = tf.keras.layers.Dropout(rate = drop_prob)(pool_4)\n","            \n","    # MIDDLE part\n","    conv2d_9 = tf.keras.layers.Conv2D(16*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_4)\n","    conv2d_10 = tf.keras.layers.Conv2D(16*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_9)\n","            \n","            \n","    # RIGHT part\n","    # Convolutional block 1\n","    upsampling_1 = tf.keras.layers.UpSampling2D((2,2))(conv2d_10)\n","    concat_1 = tf.keras.layers.Concatenate(3)([upsampling_1, conv2d_8])\n","    dropout_5 = tf.keras.layers.Dropout(rate = drop_prob)(concat_1)\n","    conv2d_11 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_5)\n","    conv2d_12 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_11)\n","            \n","    # Convolutional block 2\n","    upsampling_2 = tf.keras.layers.UpSampling2D((2,2))(conv2d_12)\n","    concat_2 = tf.keras.layers.Concatenate(3)([upsampling_2, conv2d_6])\n","    dropout_6 = tf.keras.layers.Dropout(rate = drop_prob)(concat_2)\n","    conv2d_13 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_6)\n","    conv2d_14 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_13)\n","        \n","            \n","    # Convolutional block 3\n","    upsampling_3 = tf.keras.layers.UpSampling2D((2,2))(conv2d_14)\n","    concat_3 = tf.keras.layers.Concatenate(3)([upsampling_3,conv2d_4])\n","    dropout_7 = tf.keras.layers.Dropout(rate = drop_prob)(concat_3)\n","    conv2d_15 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_7)\n","    conv2d_16 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_15)\n","            \n","    # Convolutional block 4\n","    upsampling_4 = tf.keras.layers.UpSampling2D((2,2))(conv2d_16)\n","    concat_4 = tf.keras.layers.Concatenate(3)([upsampling_4,conv2d_2])\n","    dropout_8 = tf.keras.layers.Dropout(rate = drop_prob)(concat_4)\n","    conv2d_17 = tf.keras.layers.Conv2D(init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_8)\n","    conv2d_18 = tf.keras.layers.Conv2D(init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_17)\n","            \n","    # ouput layer\n","    output_image = tf.keras.layers.Conv2D(1, (1,1), kernel_initializer= W_init, padding = 'same')(conv2d_18)\n","\n","    # define the model\n","    model = tf.keras.Model(inputs=input_image, outputs=output_image)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uXe-N1A0JgM5","colab":{}},"source":["# define some different losses\n","# loss defined as in the original UNet paper\n","# The energy function is computed by a pixel-wise soft-max over the final feature map combined with the cross-entropy loss function\n","def loss_UNet(predicted_logits, real_annotations, GlobalBatchSize):\n","    real_annotations = tf.expand_dims(real_annotations, axis = 3)\n","    # real_annotations = tf.convert_to_tensor(real_annotations)\n","    real_annotations = tf.cast(real_annotations, tf.float32)\n","    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels= real_annotations, logits= predicted_logits)\n","\n","    # take the mean over all pixels \n","    loss = tf.reduce_mean(loss, axis = (1,2))\n","    return tf.reduce_sum(loss) * (1. / GlobalBatchSize)\n","\n","# define the focal cross entropy loss function\n","# loss function has to be defined in a way to avoid data inbalance\n","def loss_sfce(predicted_logits, real_annotations, GlobalBatchSize, alpha = 0.25, gamma = 2.0):\n","    '''\n","    This type of loss function tries to avoid data imbalance in image segmentation\n","    There are two parameters alpha and gamma, the default values are indicated\n","    gamma should always be greater than or equal to 0\n","    '''\n","    real_annotations = tf.expand_dims(real_annotations, axis = 3)\n","    real_annotations = tf.cast(real_annotations, tf.float32)\n","  \n","    # classic binary cross_entropy is calculated\n","    ce = K.binary_crossentropy(real_annotations, predicted_logits, from_logits= True)\n","  \n","    # binary cross-entropy is multiplied with two factors: alpha and modulating factor\n","    # convert the logits predictions into probabilities\n","    alpha_factor = 1.0\n","    modulating_factor = 1.0\n","    pred_prob = tf.sigmoid(predicted_logits)\n","  \n","    if alpha:\n","        alpha = tf.convert_to_tensor(alpha, dtype=K.floatx())\n","        alpha_factor = real_annotations * alpha + (1 - real_annotations) * (1 - alpha)\n","\n","    p_t = (real_annotations * pred_prob) + ((1 - real_annotations) * (1 - pred_prob))\n","    if gamma:\n","        gamma = tf.convert_to_tensor(gamma, dtype=K.floatx())\n","        modulating_factor = tf.pow((1.0 - p_t), gamma)\n","  \n","    # compute the final loss and return\n","    loss = alpha_factor * modulating_factor * ce\n","    loss = tf.reduce_mean(loss, axis =(1,2))\n","    return tf.reduce_sum(loss) * (1. / GlobalBatchSize)\n","\n","\n","# Asymmetric similarity loss function, to balance recall and precision\n","# the larger beta, the more important the recall becomes relative to the precision\n","def loss_asl(predicted_logits, real_annotations, GlobalBatchSize, beta = 2):\n","    real_annotations = tf.cast(real_annotations, tf.float32)\n","    pred_prob = tf.nn.sigmoid(predicted_logits)[:,:,:,0]\n","  \n","    prod_pos = pred_prob * real_annotations\n","    sum_prod_pos = tf.reduce_sum(tf.reduce_sum(prod_pos, axis = 2), axis = 1)\n","    prod_neg_pred = (1-pred_prob) * real_annotations\n","    sum_prod_neg_pred = tf.reduce_sum(tf.reduce_sum(prod_neg_pred, axis = 2), axis = 1)\n","    prod_neg_real = (pred_prob) * (1-real_annotations)\n","    sum_prod_neg_real = tf.reduce_sum(tf.reduce_sum(prod_neg_real, axis = 2), axis = 1)\n","\n","    beta = tf.convert_to_tensor(beta, dtype=K.floatx())\n","\n","    num = (1+beta**2) * sum_prod_pos\n","    denom = (1+beta**2) *sum_prod_pos + beta**2 * sum_prod_neg_pred + sum_prod_neg_real\n","\n","    loss = num/denom\n","    return tf.reduce_sum(loss) * (1. / GlobalBatchSize)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VQg7qWs9Wo9r","colab":{}},"source":["def train_network(TrainImages, TrainAnnotations, TestImages, TestAnnotations, \n","                  Drop_Prob = 0.1, Init_Filters = 64, batch_size = 3, loss_function = 'UNet_loss', optim = 'Adam', \n","                  learning_rate = tf.Variable(1e-5, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True):\n","    '''\n","    This function trains the UNet on the indicated train data with corresponding annotations\n","    At the end the trained model is being saved\n","    '''\n","\n","    # define the train and test batches that can be fed into the network\n","    # global batch size defines the batch size over all availabel GPU's\n","    print('Creating distributed data')\n","    Global_batch_size = batch_size * strategy.num_replicas_in_sync\n","    train_batch_data  = tf.data.Dataset.from_tensor_slices((TrainImages, TrainAnnotations)).shuffle(TrainImages.shape[0]).batch(Global_batch_size) \n","    test_batch_data = tf.data.Dataset.from_tensor_slices((TestImages, TestAnnotations)).batch(Global_batch_size) \n","\n","    # distribute the data over the different GPU's\n","    train_dist_data =  strategy.experimental_distribute_dataset(train_batch_data)\n","    test_dist_data =  strategy.experimental_distribute_dataset(test_batch_data)\n","\n","    # define the model that will be used for training and for testing\n","    # the model, optimisation and loss have to be distributed among GPU's\n","    tf.compat.v1.reset_default_graph()\n","    with strategy.scope():\n","        # model\n","        print('Defining the model')\n","        model = UNet(drop_prob = Drop_Prob, init_filters = Init_Filters)\n","    \n","        # loss\n","        print('Defining loss')\n","        def compute_loss(logits, annotations):\n","            if loss_function == 'UNet_loss':\n","                loss = loss_UNet(logits, annotations, Global_batch_size)\n","            elif loss_function == 'Sfce_loss':\n","                loss = loss_sfce(logits, annotations, Global_batch_size)\n","            elif loss_function == 'Asl_loss':\n","              loss = loss_asl(logits, annotations, Global_batch_size)\n","            return loss\n","        \n","        # optimization\n","        steps_per_epoch = int(TrainImages.shape[0]/Global_batch_size)\n","        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate= learning_rate, decay_steps= MAX_EPOCH*steps_per_epoch*0.25, decay_rate=0.2, staircase = True)\n","        print('Defining optimization')\n","        if optim == 'Adam':\n","              train_op = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n","        elif optim == 'sgd':\n","              train_op = tf.keras.optimizers.SGD(learning_rate = lr_schedule)\n","\n","        # defining the metrics\n","        print('Defining the metrics')\n","        test_roc_auc = tf.keras.metrics.AUC(curve = 'ROC')\n","        test_pr_auc = tf.keras.metrics.AUC(curve = 'PR')\n","                \n","    # one train step is a step in which one batch of data is fed to every GPU\n","    def Train_Step(input):\n","        train_images_batch, train_annotations_batch = input\n","\n","        with tf.GradientTape() as tape:\n","            # make prediction with model\n","            pred_logits = model(train_images_batch, training = True)\n","            # compute loss\n","            train_err = compute_loss(pred_logits, train_annotations_batch)\n","            \n","        # update model\n","        train_weights = model.trainable_variables\n","        gradients = tape.gradient(train_err, train_weights)\n","        train_op.apply_gradients(zip(gradients, train_weights))\n","            \n","    # for the last epoch some testing has to be done, in a test step one batch of test data is fed to every GPU\n","    def Test_Step(input):\n","        test_images_batch, test_annotations_batch = input\n","        \n","        # make prediction with model\n","        pred_logits = model(test_images_batch, training = False)\n","        # turn into probability map\n","        test_pos_prob = tf.nn.sigmoid(pred_logits)\n","        # compute loss\n","        test_err = compute_loss(pred_logits, test_annotations_batch)\n","        # compute auc and aupr score\n","        temp_test_annotations = tf.reshape(test_annotations_batch,[-1])\n","        temp_test_positive_prob = tf.reshape(test_pos_prob,[-1])\n","        test_roc_auc.update_state(temp_test_annotations, temp_test_positive_prob)\n","        test_pr_auc.update_state(temp_test_annotations, temp_test_positive_prob)\n","    \n","        # the error, auc and aupr score per replica are returned\n","        return test_err, test_roc_auc.result(), test_pr_auc.result()\n","\n","    @tf.function\n","    def distributed_train_step(dataset_inputs):\n","        strategy.run(Train_Step, args=(dataset_inputs,))\n","\n","    @tf.function\n","    def distributed_test_step(dataset_inputs):\n","        per_replica_losses, per_replica_roc_auc, per_replica_pr_auc = strategy.run(Test_Step, args=(dataset_inputs,))\n","        return per_replica_losses, per_replica_roc_auc, per_replica_pr_auc\n","    \n","    \n","    # the train and test steps now have to be performed with the distributed strategy\n","    print('Training')\n","    for epo in range(1,MAX_EPOCH+1):\n","        \n","        # go over all global batches\n","        for train_input_data in train_dist_data:\n","            distributed_train_step(train_input_data)\n","\n","        # some testing has to be done at the last epoch\n","        if epo == MAX_EPOCH:\n","            print('Testing')\n","            n_test_steps = 0\n","            total_test_loss = 0\n","            total_test_auc = 0\n","            total_test_aupr = 0\n","            \n","            for test_input_data in test_dist_data:\n","                n_test_steps+=1\n","                per_replica_test_losses, per_replica_test_roc_auc, per_replica_test_pr_auc = distributed_test_step(test_input_data)\n","                total_test_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_test_losses, axis=None)\n","                total_test_auc += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_test_roc_auc, axis=None)\n","                total_test_aupr += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_test_pr_auc, axis=None)\n","                \n","            total_test_loss = total_test_loss/n_test_steps\n","            total_test_auc =  total_test_auc/n_test_steps\n","            total_test_aupr =  total_test_aupr/n_test_steps      \n","\n","    return total_test_loss, total_test_auc, total_test_aupr   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eog-QtTMmSu4"},"source":["The functions beneith can be used to perform a gridsearch over different combinations of parameters to define the parametercombination that gives the best results in terms of performance and training time."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3-Cb8VYnltf4","colab":{}},"source":["# gridsearch: look for the most optimal hyperparameters, making use of cross-validation\n","\n","# for this a separate validation set has to be defined\n","# three fold cross-validation is performed in this case\n","def ThreeFoldSplit(n_split):\n","    '''\n","    This function calculates a three-fold split of the train_images and train_annotations\n","    Depending on n_split another train and validation set is defined\n","    '''\n","    \n","    a = train_images.shape[0]\n","    b = int(a/3)\n","    \n","    val_images = train_images[n_split*b:(n_split+1)*b]\n","    val_annotations = train_annotations[n_split*b:(n_split+1)*b]\n","    \n","    # split the train set in to three parts\n","    if n_split == 0:\n","        tr_images = train_images[b:]\n","        tr_annotations = train_annotations[b:]\n","        \n","    if n_split == 1:\n","        tr_images = np.vstack((train_images[0:b], train_images[2*b:]))\n","        tr_annotations = np.vstack((train_annotations[0:b], train_annotations[2*b:]))\n","        \n","    if n_split == 2:\n","        tr_images = train_images[0:2*b]\n","        tr_annotations = train_annotations[0:2*b]\n","    \n","    return tr_images, tr_annotations, val_images, val_annotations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sQbJLfkFltf7","colab":{}},"source":["# function to choose the best hyperparameter combination based on model performance in terms of the ROC AUC score\n","def Hyperparam_Optimization(lr_list, Batch_Size_list, dropout_list, loss_func_list, optim_list, \n","                            init_filters_list, epochs_list, savefile):\n","    n = 0\n","    file = open(savefile,'w') \n","    start = time.time()\n","    for LearnRate in lr_list:\n","        for BatchSize in Batch_Size_list:\n","            for Dropout in dropout_list:\n","                for LossFunc in loss_func_list:\n","                    for Optim in optim_list:\n","                        for InitFilters in init_filters_list:\n","                            for Epoch in epochs_list:\n","                                \n","                                n +=1\n","                                print('Evaluating parameter combination {} ...'.format(n))\n","                                    \n","                                AUC_list = []\n","                                AUPR_list = []\n","                                Loss_list = []\n","                                time_list = []\n","                                \n","                                # 3-fold cross-validation is used to evaluate the model \n","                                # for a certain param combination \n","                                for fold in range(3):\n","                                        \n","                                    tr_images, tr_annotations, val_images, val_annotations = ThreeFoldSplit(fold)\n","                                    \n","                                    # fitting the model to the data for a certain fold and defining the auc, aupr and loss\n","                                    # these values can later on be compared for different param combinations\n","                                    start_time = time.time()\n","                                    \n","                                    val_loss, val_auc, val_aupr = train_network(tr_images, tr_annotations, val_images, val_annotations, Dropout, InitFilters, BatchSize, LossFunc, Optim, \n","                                                  tf.Variable(LearnRate, dtype=tf.float32), Epoch, SaveResults = False)\n","                                    \n","                                    end_time = time.time()\n","                                        \n","                                    # creat a 1D numpy array with the predicted and true outputs   \n","                                    AUC_list.append(val_auc)\n","                                    AUPR_list.append(val_aupr)\n","                                    Loss_list.append(val_loss) \n","                                    time_list.append(end_time-start_time) \n","                                    \n","                                # calculate the mean and standard-dev of the AUC, AUPR, loss and time for all three folds\n","                                mean_AUC = np.mean(np.array(AUC_list), axis = 0)\n","                                std_AUC = np.std(np.array(AUC_list), axis = 0)\n","                                mean_AUPR = np.mean(np.array(AUPR_list), axis = 0)\n","                                std_AUPR = np.std(np.array(AUPR_list), axis = 0)\n","                                mean_Loss = np.mean(np.array(Loss_list), axis = 0)\n","                                std_Loss = np.std(np.array(Loss_list), axis = 0)\n","                                mean_time = np.mean(time_list, axis = 0)\n","                                    \n","                                # print out these values together with the training time for this parameter set\n","                                print('Hyperparameter combination:')\n","                                print('learn rate {}, batch size {}, dropout {}, loss function {}, optimization {}, initial filters {} and epochs {}'.format(LearnRate, BatchSize, Dropout, LossFunc, Optim, InitFilters, Epoch))\n","                                print('Results:')\n","                                print('AUC: mean: {}, standard deviation: {}'.format(mean_AUC, std_AUC))\n","                                print('AUPR: mean: {}, standard deviation: {}'.format(mean_AUPR, std_AUPR)) \n","                                print('Loss: mean: {}, standard deviation: {}'.format(mean_Loss, std_Loss)) \n","                                print('training time per epoch: mean: {}'.format(mean_time))\n","\n","                                # save all values in a txt file\n","                                file.write('Hyperparameter combination: \\n')\n","                                file.write('learn rate {}, batch size {}, dropout {}, loss function {}, optimization {}, initial filters {} and epochs {} \\n'.format(LearnRate, BatchSize, Dropout, LossFunc, Optim, InitFilters, Epoch))\n","                                file.write('AUC: mean: {}, standard deviation: {} \\n'.format(mean_AUC, std_AUC))\n","                                file.write('AUPR: mean: {}, standard deviation: {} \\n'.format(mean_AUPR, std_AUPR))\n","                                file.write('Loss: mean: {}, standard deviation: {} \\n'.format(mean_Loss, std_Loss))\n","                                file.write('training time per epoch: mean: {} \\n'.format(mean_time))\n","                                file.write(' \\n')\n","    print('The gridsearch took {}s'.format(time.time()- start)) \n","    file.close()                                                      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kL85YGBWltf-","colab":{}},"source":["# # different options that have to be tried\n","# # learn rat\n","# LR_list = [1e-3, 1e-5]\n","# # batch size\n","# BS_list = [3,6]\n","# # drop probab\n","# DP_list = [0.3,0.5]\n","# # loss function\n","# LF_list = ['UNet_loss', 'Sfce_loss', 'Asl_loss']\n","# # optimisation function\n","# OF_list = ['Adam', 'sgd']\n","# # initial amount of filters\n","# IF_list = [64, 32]\n","# # amount of epochs\n","# EP_list = [200]\n","# # alpha and gamma, parameters of the focal loss function\n","\n","# Hyperparam_Optimization(lr_list = LR_list, Batch_Size_list = BS_list, dropout_list = DP_list, \n","#                         loss_func_list = LF_list, optim_list = OF_list, init_filters_list = IF_list, \n","#                         epochs_list = EP_list, savefile = dir_gridsearch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_KiGkWgIyzBp","colab_type":"code","colab":{}},"source":["# # different options that have to be tried\n","# # learn rat\n","# LR_list = [1e-3, 1e-5]\n","# # batch size\n","# BS_list = [3,6]\n","# # drop probab\n","# DP_list = [0.3,0.5]\n","# # loss function\n","# LF_list = ['UNet_loss', 'Sfce_loss', 'Asl_loss']\n","# # optimisation function\n","# OF_list = ['Adam', 'sgd']\n","# # initial amount of filters\n","# IF_list = [64, 32]\n","# # amount of epochs\n","# EP_list = [1]\n","# # alpha and gamma, parameters of the focal loss function\n","\n","# Hyperparam_Optimization(lr_list = LR_list, Batch_Size_list = BS_list, dropout_list = DP_list, \n","#                         loss_func_list = LF_list, optim_list = OF_list, init_filters_list = IF_list, \n","#                         epochs_list = EP_list, savefile = dir_gridsearch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cACR_k1sZrkD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597686398661,"user_tz":-120,"elapsed":891556,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"c9c347b8-2671-43c7-8240-12c1b6811693"},"source":["# different options that have to be tried\n","# learn rat\n","LR_list = [1e-3, 1e-5]\n","# batch size\n","BS_list = [3,6]\n","# drop probab\n","DP_list = [0.3,0.5]\n","# loss function\n","LF_list = ['UNet_loss']\n","# optimisation function\n","OF_list = ['Adam']\n","# initial amount of filters\n","IF_list = [64, 32]\n","# amount of epochs\n","EP_list = [1]\n","# alpha and gamma, parameters of the focal loss function\n","\n","Hyperparam_Optimization(lr_list = LR_list, Batch_Size_list = BS_list, dropout_list = DP_list, \n","                        loss_func_list = LF_list, optim_list = OF_list, init_filters_list = IF_list, \n","                        epochs_list = EP_list, savefile = dir_gridsearch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Evaluating parameter combination 1 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Iterator.get_next_as_optional()` instead.\n","Testing\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 0.001, batch size 3, dropout 0.3, loss function UNet_loss, optimization Adam, initial filters 64 and epochs 1\n","Results:\n","AUC: mean: 0.5231063961982727, standard deviation: 0.12564042210578918\n","AUPR: mean: 0.017196757718920708, standard deviation: 0.007429063320159912\n","Loss: mean: 0.09535401314496994, standard deviation: 0.013333136215806007\n","training time per epoch: mean: 25.42395480473836\n","Evaluating parameter combination 2 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 0.001, batch size 3, dropout 0.3, loss function UNet_loss, optimization Adam, initial filters 32 and epochs 1\n","Results:\n","AUC: mean: 0.4266221225261688, standard deviation: 0.08403222262859344\n","AUPR: mean: 0.011993836611509323, standard deviation: 0.0018385275034233928\n","Loss: mean: 0.08806037902832031, standard deviation: 0.009799755178391933\n","training time per epoch: mean: 12.810268719991049\n","Evaluating parameter combination 3 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 0.001, batch size 3, dropout 0.5, loss function UNet_loss, optimization Adam, initial filters 64 and epochs 1\n","Results:\n","AUC: mean: 0.4832812249660492, standard deviation: 0.09877199679613113\n","AUPR: mean: 0.013981446623802185, standard deviation: 0.0035626443568617105\n","Loss: mean: 0.09339386224746704, standard deviation: 0.005374278873205185\n","training time per epoch: mean: 23.722957054773968\n","Evaluating parameter combination 4 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 0.001, batch size 3, dropout 0.5, loss function UNet_loss, optimization Adam, initial filters 32 and epochs 1\n","Results:\n","AUC: mean: 0.5313490629196167, standard deviation: 0.12493755668401718\n","AUPR: mean: 0.017311083152890205, standard deviation: 0.006815959699451923\n","Loss: mean: 0.08994890004396439, standard deviation: 0.007391924969851971\n","training time per epoch: mean: 12.210912227630615\n","Evaluating parameter combination 5 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 0.001, batch size 6, dropout 0.3, loss function UNet_loss, optimization Adam, initial filters 64 and epochs 1\n","Results:\n","AUC: mean: 0.40910568833351135, standard deviation: 0.00831160880625248\n","AUPR: mean: 0.011130422353744507, standard deviation: 0.00015542798792012036\n","Loss: mean: 0.11944092065095901, standard deviation: 0.0024317840579897165\n","training time per epoch: mean: 24.383888641993206\n","Evaluating parameter combination 6 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 0.001, batch size 6, dropout 0.3, loss function UNet_loss, optimization Adam, initial filters 32 and epochs 1\n","Results:\n","AUC: mean: 0.4113675355911255, standard deviation: 0.02233206480741501\n","AUPR: mean: 0.011204089038074017, standard deviation: 0.0004652286588679999\n","Loss: mean: 0.11556345224380493, standard deviation: 0.017038961872458458\n","training time per epoch: mean: 12.97755495707194\n","Evaluating parameter combination 7 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 0.001, batch size 6, dropout 0.5, loss function UNet_loss, optimization Adam, initial filters 64 and epochs 1\n","Results:\n","AUC: mean: 0.3922136723995209, standard deviation: 0.02408040501177311\n","AUPR: mean: 0.010831150226294994, standard deviation: 0.00045949488412588835\n","Loss: mean: 0.11719438433647156, standard deviation: 0.00335479062050581\n","training time per epoch: mean: 22.991861979166668\n","Evaluating parameter combination 8 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 0.001, batch size 6, dropout 0.5, loss function UNet_loss, optimization Adam, initial filters 32 and epochs 1\n","Results:\n","AUC: mean: 0.4525927007198334, standard deviation: 0.009832126088440418\n","AUPR: mean: 0.012220391072332859, standard deviation: 0.0003366447926964611\n","Loss: mean: 0.13944844901561737, standard deviation: 0.03535677492618561\n","training time per epoch: mean: 12.519526799519857\n","Evaluating parameter combination 9 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 1e-05, batch size 3, dropout 0.3, loss function UNet_loss, optimization Adam, initial filters 64 and epochs 1\n","Results:\n","AUC: mean: 0.41219672560691833, standard deviation: 0.03003530018031597\n","AUPR: mean: 0.011523607186973095, standard deviation: 0.0007091897423379123\n","Loss: mean: 0.1537257879972458, standard deviation: 0.021862061694264412\n","training time per epoch: mean: 25.097254991531372\n","Evaluating parameter combination 10 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 1e-05, batch size 3, dropout 0.3, loss function UNet_loss, optimization Adam, initial filters 32 and epochs 1\n","Results:\n","AUC: mean: 0.49981236457824707, standard deviation: 0.06370273977518082\n","AUPR: mean: 0.014861301518976688, standard deviation: 0.00321387080475688\n","Loss: mean: 1.792258620262146, standard deviation: 2.292752504348755\n","training time per epoch: mean: 12.315416177113852\n","Evaluating parameter combination 11 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 1e-05, batch size 3, dropout 0.5, loss function UNet_loss, optimization Adam, initial filters 64 and epochs 1\n","Results:\n","AUC: mean: 0.4797321856021881, standard deviation: 0.04687276855111122\n","AUPR: mean: 0.014382384717464447, standard deviation: 0.002777027664706111\n","Loss: mean: 0.25488317012786865, standard deviation: 0.12751303613185883\n","training time per epoch: mean: 24.804797013600666\n","Evaluating parameter combination 12 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 1e-05, batch size 3, dropout 0.5, loss function UNet_loss, optimization Adam, initial filters 32 and epochs 1\n","Results:\n","AUC: mean: 0.4062383472919464, standard deviation: 0.0872485339641571\n","AUPR: mean: 0.011801903136074543, standard deviation: 0.001714368350803852\n","Loss: mean: 0.18740902841091156, standard deviation: 0.0594748929142952\n","training time per epoch: mean: 12.071888049443563\n","Evaluating parameter combination 13 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 1e-05, batch size 6, dropout 0.3, loss function UNet_loss, optimization Adam, initial filters 64 and epochs 1\n","Results:\n","AUC: mean: 0.5420031547546387, standard deviation: 0.1016678735613823\n","AUPR: mean: 0.02698107250034809, standard deviation: 0.01999242976307869\n","Loss: mean: 0.4026283919811249, standard deviation: 0.28180018067359924\n","training time per epoch: mean: 24.19850738843282\n","Evaluating parameter combination 14 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 1e-05, batch size 6, dropout 0.3, loss function UNet_loss, optimization Adam, initial filters 32 and epochs 1\n","Results:\n","AUC: mean: 0.4541665017604828, standard deviation: 0.07305436581373215\n","AUPR: mean: 0.013147328980267048, standard deviation: 0.0022365853656083345\n","Loss: mean: 0.4553636610507965, standard deviation: 0.39755779504776\n","training time per epoch: mean: 12.63329807917277\n","Evaluating parameter combination 15 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 1e-05, batch size 6, dropout 0.5, loss function UNet_loss, optimization Adam, initial filters 64 and epochs 1\n","Results:\n","AUC: mean: 0.5039947628974915, standard deviation: 0.13280470669269562\n","AUPR: mean: 0.016173915937542915, standard deviation: 0.006374447140842676\n","Loss: mean: 1.8060942888259888, standard deviation: 2.0066006183624268\n","training time per epoch: mean: 25.12213929494222\n","Evaluating parameter combination 16 ...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","Testing\n","Hyperparameter combination:\n","learn rate 1e-05, batch size 6, dropout 0.5, loss function UNet_loss, optimization Adam, initial filters 32 and epochs 1\n","Results:\n","AUC: mean: 0.5348796248435974, standard deviation: 0.06655510514974594\n","AUPR: mean: 0.01580343395471573, standard deviation: 0.002905256114900112\n","Loss: mean: 5.465950012207031, standard deviation: 4.093223571777344\n","training time per epoch: mean: 12.587733427683512\n","The gridsearch took 887.9672536849976s\n"],"name":"stdout"}]}]}