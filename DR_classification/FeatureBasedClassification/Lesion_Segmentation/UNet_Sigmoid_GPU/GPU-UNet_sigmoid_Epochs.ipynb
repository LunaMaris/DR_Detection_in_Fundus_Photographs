{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GPU-UNet_sigmoid_Epochs.ipynb","provenance":[{"file_id":"1WjRMQp-rpEkDY35wu2TOPuBmNDB2NKAW","timestamp":1597668173779},{"file_id":"1fIfh4nn9-IfHf4Fv0K3RCbiokXtkV2oh","timestamp":1596537549611},{"file_id":"1QMAMVVMbSl6bTe6A09aprdmgYrNBEcrI","timestamp":1596124792763},{"file_id":"1cPo0a7rxvx8xVA1rLbdlTcdllub8HVn5","timestamp":1596020580331},{"file_id":"1N09tXe6iDGW9tt3_cWA_K-sYrT1Q98qq","timestamp":1596016036593},{"file_id":"1pvSj2cOo177dqWp1y6lbiwohGTGeGVm1","timestamp":1596012829809},{"file_id":"1ko8HCMMTNlaR1MCC-1f6QL1uAaqzHXwO","timestamp":1595928299309},{"file_id":"1cjKN36peUbhC36sgbu1cp5rwTPmzAtcg","timestamp":1595852695030}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ga1Mx266ltfH"},"source":["This script is used to perform lesion segmentation in fundus photographs. The lesions that can be segmented are hard exudates, soft exudates, microaneurysms and hemorrhages. The segmentation is based on a UNet, a CNN that takes an image as an input and that outputs a probability map indicating for every pixel the probability of belonging to a certain type of lesion or not."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9eyAksxJOFsi","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597745465147,"user_tz":-120,"elapsed":3001,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"9fde5372-0103-47fa-9518-cc4d9b5a7c1e"},"source":["# import necessary libraries\n","\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.model_selection import train_test_split\n","\n","import time\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6nLLjCoQERfU","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597745466623,"user_tz":-120,"elapsed":4463,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"c724ed12-8401-4bd7-b720-28ccf11762eb"},"source":["# define how many gpus are available and set a memmory limit\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","print(\"Number of GPUs Available: \", len(gpus))\n","for i in range(len(gpus)):\n","    tf.config.experimental.set_virtual_device_configuration(gpus[i], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7900)]) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of GPUs Available:  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DuaG1Xu9s5wW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597745470901,"user_tz":-120,"elapsed":8731,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"a1d64ec9-9e42-41a9-cc03-ce4162350340"},"source":["strategy = tf.distribute.MirroredStrategy()\n","# the number of replicas that is created by the strategy should be equal to the number of GPU's available\n","print ('Number of synchronized replicas created: {}'.format(strategy.num_replicas_in_sync))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","Number of synchronized replicas created: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GnA-4iHSuYJf","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1597745490873,"user_tz":-120,"elapsed":28696,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"25ec819d-83d3-4b53-ec72-9c5fdcd810bf"},"source":["# read in train and test data in case Google DRIVE is used\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x4SbTTqBltfN","colab":{}},"source":["# read in the train and test data for a certain lesion type\n","\n","# Basepath depends on the lesion\n","# LesionType = 'SoftExudates'\n","LesionType = 'HardExudates'\n","# LesionType = 'Microaneurysms'\n","# LesionType = 'Hemorrhages'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Na905WmrltfS","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1597745496122,"user_tz":-120,"elapsed":33935,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"10626553832555274819"}},"outputId":"ff3b8901-fa08-45af-f286-09e5cdacaef1"},"source":["# Basepath for Google DRIVE:\n","Basepath = '/content/drive/My Drive/Stage_ENT_Studios/Data/IDRiD/' + LesionType + '/Arrays/'\n","\n","# Basepath for Jupyter notebooks:\n","# Basepath = 'C:/Users/lunam/Documents/1steMaster/Stage/Data_FinalArrays/IDRiD/'+ LesionType+'/Arrays/'\n","\n","# Basepath for KILI\n","# Basepath = '/home/kili/Desktop/Data_FinalArrays/IDRiD/'+ LesionType+'/Arrays/'\n","\n","# train data\n","train_images = np.load(Basepath + 'train_images_Final.npy')\n","print('Shape train images: {}'.format(train_images.shape))\n","\n","train_annotations =  np.load(Basepath + 'train_annotations_Final.npy')\n","# train_annotations = np.expand_dims(train_annotations, axis = 3)\n","print('Shape train annotations: {}'.format(train_annotations.shape))\n","\n","train_images, test_images, train_annotations, test_annotations = train_test_split(train_images, train_annotations, test_size=0.33, random_state=42)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape train images: (324, 256, 256, 3)\n","Shape train annotations: (324, 256, 256)\n","Shape test images: (156, 256, 256, 3)\n","Shape test annotations: (156, 256, 256)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WWjV59oPltfZ","colab":{}},"source":["# save directions for the model and the tensorboard logs\n","\n","# Base path for Google DRIVE:\n","base_path = '/content/drive/My Drive/Stage_ENT_Studios/Unet/Logs/'\n","\n","# Base path for jupyter notebooks:\n","# base_path = 'C:/Users/lunam/Documents/1steMaster/Stage/Code_Final/DR_classification/FeatureBasedClassification/Lesion_Segmentation/UNet_Sigmoid_GPU/Logs/' + LesionType + '/'\n","\n","# Base path for Kili\n","# base_path = '/home/kili/Desktop/FeatureBasedClassification/Lesion_Segmentation/UNet_Sigmoid_GPU/Logs/' + LesionType + '/'\n","\n","# direction where the tensorboard files will be stored\n","log_dir_tens = base_path + 'Tensorboard_Logs/'\n","# direction where the trained models will be stored\n","log_dir_model = base_path + 'Trained_Model/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FICWJHVxltfh","colab":{}},"source":["# The UNet network\n","def UNet(drop_prob = 0.1, init_filters = 64):\n","    '''This function defines the original UNet network'''\n","  \n","    # initialization of the weights\n","    W_init = tf.initializers.GlorotUniform()\n","    \n","    # Unet network\n","            \n","    # LEFT part\n","    # input layer\n","    # input_image = tf.keras.layers.InputLayer(input_shape = (512,512,3))\n","    input_image = tf.keras.layers.Input(shape = (256,256,3))\n","        \n","    # Convolutional block 1\n","    conv2d_1 = tf.keras.layers.Conv2D(init_filters, (3, 3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(input_image)\n","    conv2d_2 = tf.keras.layers.Conv2D(init_filters, (3, 3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_1)\n","    pool_1 = tf.keras.layers.MaxPool2D((2, 2), (2, 2))(conv2d_2)\n","    dropout_1 = tf.keras.layers.Dropout(rate = drop_prob)(pool_1)\n","            \n","    # Convolutional block 2\n","    conv2d_3 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_1)\n","    conv2d_4 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_3)\n","    pool_2 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_4)\n","    dropout_2 = tf.keras.layers.Dropout(rate = drop_prob)(pool_2 )\n","            \n","    # Convolutional block 3\n","    conv2d_5 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_2)\n","    conv2d_6 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_5)\n","    pool_3 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_6)\n","    dropout_3 = tf.keras.layers.Dropout(rate = drop_prob)(pool_3)\n","            \n","    # Convolutional block 4\n","    conv2d_7 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_3)\n","    conv2d_8 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_7)\n","    pool_4 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_8)\n","    dropout_4 = tf.keras.layers.Dropout(rate = drop_prob)(pool_4)\n","            \n","    # MIDDLE part\n","    conv2d_9 = tf.keras.layers.Conv2D(16*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_4)\n","    conv2d_10 = tf.keras.layers.Conv2D(16*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_9)\n","            \n","            \n","    # RIGHT part\n","    # Convolutional block 1\n","    upsampling_1 = tf.keras.layers.UpSampling2D((2,2))(conv2d_10)\n","    concat_1 = tf.keras.layers.Concatenate(3)([upsampling_1, conv2d_8])\n","    dropout_5 = tf.keras.layers.Dropout(rate = drop_prob)(concat_1)\n","    conv2d_11 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_5)\n","    conv2d_12 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_11)\n","            \n","    # Convolutional block 2\n","    upsampling_2 = tf.keras.layers.UpSampling2D((2,2))(conv2d_12)\n","    concat_2 = tf.keras.layers.Concatenate(3)([upsampling_2, conv2d_6])\n","    dropout_6 = tf.keras.layers.Dropout(rate = drop_prob)(concat_2)\n","    conv2d_13 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_6)\n","    conv2d_14 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_13)\n","        \n","            \n","    # Convolutional block 3\n","    upsampling_3 = tf.keras.layers.UpSampling2D((2,2))(conv2d_14)\n","    concat_3 = tf.keras.layers.Concatenate(3)([upsampling_3,conv2d_4])\n","    dropout_7 = tf.keras.layers.Dropout(rate = drop_prob)(concat_3)\n","    conv2d_15 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_7)\n","    conv2d_16 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_15)\n","            \n","    # Convolutional block 4\n","    upsampling_4 = tf.keras.layers.UpSampling2D((2,2))(conv2d_16)\n","    concat_4 = tf.keras.layers.Concatenate(3)([upsampling_4,conv2d_2])\n","    dropout_8 = tf.keras.layers.Dropout(rate = drop_prob)(concat_4)\n","    conv2d_17 = tf.keras.layers.Conv2D(init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_8)\n","    conv2d_18 = tf.keras.layers.Conv2D(init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_17)\n","            \n","    # ouput layer\n","    output_image = tf.keras.layers.Conv2D(1, (1,1), kernel_initializer= W_init, padding = 'same')(conv2d_18)\n","\n","    # define the model\n","    model = tf.keras.Model(inputs=input_image, outputs=output_image)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uXe-N1A0JgM5","colab":{}},"source":["# define some different losses\n","# loss defined as in the original UNet paper\n","# The energy function is computed by a pixel-wise soft-max over the final feature map combined with the cross-entropy loss function\n","def loss_UNet(predicted_logits, real_annotations, GlobalBatchSize):\n","    real_annotations = tf.expand_dims(real_annotations, axis = 3)\n","    # real_annotations = tf.convert_to_tensor(real_annotations)\n","    real_annotations = tf.cast(real_annotations, tf.float32)\n","    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels= real_annotations, logits= predicted_logits)\n","\n","    # take the mean over all pixels \n","    loss = tf.reduce_mean(loss, axis = (1,2))\n","    return tf.reduce_sum(loss) * (1. / GlobalBatchSize)\n","\n","# define the focal cross entropy loss function\n","# loss function has to be defined in a way to avoid data inbalance\n","def loss_sfce(predicted_logits, real_annotations, GlobalBatchSize, alpha = 0.25, gamma = 2.0):\n","    '''\n","    This type of loss function tries to avoid data imbalance in image segmentation\n","    There are two parameters alpha and gamma, the default values are indicated\n","    gamma should always be greater than or equal to 0\n","    '''\n","    real_annotations = tf.expand_dims(real_annotations, axis = 3)\n","    real_annotations = tf.cast(real_annotations, tf.float32)\n","  \n","    # classic binary cross_entropy is calculated\n","    ce = K.binary_crossentropy(real_annotations, predicted_logits, from_logits= True)\n","  \n","    # binary cross-entropy is multiplied with two factors: alpha and modulating factor\n","    # convert the logits predictions into probabilities\n","    alpha_factor = 1.0\n","    modulating_factor = 1.0\n","    pred_prob = tf.sigmoid(predicted_logits)\n","  \n","    if alpha:\n","        alpha = tf.convert_to_tensor(alpha, dtype=K.floatx())\n","        alpha_factor = real_annotations * alpha + (1 - real_annotations) * (1 - alpha)\n","\n","    p_t = (real_annotations * pred_prob) + ((1 - real_annotations) * (1 - pred_prob))\n","    if gamma:\n","        gamma = tf.convert_to_tensor(gamma, dtype=K.floatx())\n","        modulating_factor = tf.pow((1.0 - p_t), gamma)\n","  \n","    # compute the final loss and return\n","    loss = alpha_factor * modulating_factor * ce\n","    loss = tf.reduce_mean(loss, axis =(1,2))\n","    return tf.reduce_sum(loss) * (1. / GlobalBatchSize)\n","\n","\n","# Asymmetric similarity loss function, to balance recall and precision\n","# the larger beta, the more important the recall becomes relative to the precision\n","def loss_asl(predicted_logits, real_annotations, GlobalBatchSize, beta = 2):\n","    real_annotations = tf.cast(real_annotations, tf.float32)\n","    pred_prob = tf.nn.sigmoid(predicted_logits)[:,:,:,0]\n","  \n","    prod_pos = pred_prob * real_annotations\n","    sum_prod_pos = tf.reduce_sum(tf.reduce_sum(prod_pos, axis = 2), axis = 1)\n","    prod_neg_pred = (1-pred_prob) * real_annotations\n","    sum_prod_neg_pred = tf.reduce_sum(tf.reduce_sum(prod_neg_pred, axis = 2), axis = 1)\n","    prod_neg_real = (pred_prob) * (1-real_annotations)\n","    sum_prod_neg_real = tf.reduce_sum(tf.reduce_sum(prod_neg_real, axis = 2), axis = 1)\n","\n","    beta = tf.convert_to_tensor(beta, dtype=K.floatx())\n","\n","    num = (1+beta**2) * sum_prod_pos\n","    denom = (1+beta**2) *sum_prod_pos + beta**2 * sum_prod_neg_pred + sum_prod_neg_real\n","\n","    loss = num/denom\n","    return tf.reduce_sum(loss) * (1. / GlobalBatchSize)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VQg7qWs9Wo9r","colab":{}},"source":["def train_network(TrainImages, TrainAnnotations, TestImages, TestAnnotations, \n","                  Drop_Prob = 0.1, Init_Filters = 64, batch_size = 3, loss_function = 'UNet_loss', optim = 'Adam', \n","                  learning_rate = tf.Variable(1e-5, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True, print_freq = 1):\n","    '''\n","    This function trains the UNet on the indicated train data with corresponding annotations\n","    At the end the trained model is being saved\n","    '''\n","    # setting up saver for the tensorboard logs\n","    if SaveResults:\n","        # creating summary which stores the results that can be visualised with tensorboard\n","        print(\"Setting up summary writer for tensorboard...\")\n","        summary_writer = tf.summary.create_file_writer(log_dir_tens)\n","\n","    # define the train and test batches that can be fed into the network\n","    # global batch size defines the batch size over all availabel GPU's\n","    print('Creating distributed data')\n","    Global_batch_size = batch_size * strategy.num_replicas_in_sync\n","    train_batch_data  = tf.data.Dataset.from_tensor_slices((TrainImages, TrainAnnotations)).shuffle(TrainImages.shape[0]).batch(Global_batch_size) \n","    test_batch_data = tf.data.Dataset.from_tensor_slices((TestImages, TestAnnotations)).batch(Global_batch_size) \n","\n","    # distribute the data over the different GPU's\n","    train_dist_data =  strategy.experimental_distribute_dataset(train_batch_data)\n","    test_dist_data =  strategy.experimental_distribute_dataset(test_batch_data)\n","\n","    # define the model that will be used for training and for testing\n","    # the model, optimisation and loss have to be distributed among GPU's\n","    tf.compat.v1.reset_default_graph()\n","    with strategy.scope():\n","        # model\n","        print('Defining the model')\n","        model = UNet(drop_prob = Drop_Prob, init_filters = Init_Filters)\n","    \n","        # loss\n","        print('Defining loss')\n","        def compute_loss(logits, annotations):\n","            if loss_function == 'UNet_loss':\n","                loss = loss_UNet(logits, annotations, Global_batch_size)\n","            elif loss_function == 'Sfce_loss':\n","                loss = loss_sfce(logits, annotations, Global_batch_size)\n","            elif loss_function == 'Asl_loss':\n","                loss = loss_asl(logits, annotations, Global_batch_size)\n","            return loss\n","\n","        # optimization\n","        # a decaying learning rate is used\n","        steps_per_epoch = int(TrainImages.shape[0]/Global_batch_size)\n","        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate= learning_rate, decay_steps= MAX_EPOCH*steps_per_epoch*0.25, decay_rate=0.2, staircase = True)\n","        print('Defining optimization')\n","        if optim == 'Adam':\n","            train_op = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n","        elif optim == 'sgd':\n","            train_op = tf.keras.optimizers.SGD(learning_rate = lr_schedule)\n","\n","        # defining the metrics\n","        print('Defining the metrics')\n","        train_roc_auc = tf.keras.metrics.AUC(curve = 'ROC')\n","        train_pr_auc = tf.keras.metrics.AUC(curve = 'PR')\n","        test_roc_auc = tf.keras.metrics.AUC(curve = 'ROC')\n","        test_pr_auc = tf.keras.metrics.AUC(curve = 'PR')\n","\n","    \n","    # one train step is a step in which one batch of data is fed to every GPU\n","    # one train step is a step in which one batch of data is fed to every GPU\n","    def Train_Step(input):\n","        \n","        with tf.GradientTape() as tape:\n","            train_images_batch, train_annotations_batch = input\n","            # make prediction with model\n","            pred_logits = model(train_images_batch, training = True)\n","            # compute loss\n","            train_err = compute_loss(pred_logits, train_annotations_batch)\n","      \n","        # update model\n","        train_weights = model.trainable_variables\n","        gradients = tape.gradient(train_err, train_weights)\n","        train_op.apply_gradients(zip(gradients, train_weights))\n","\n","        # turn into probability map\n","        train_pos_prob = tf.nn.sigmoid(pred_logits)\n","        # compute auc and aupr score\n","        temp_train_annotations = tf.reshape(train_annotations_batch,[-1])\n","        temp_train_positive_prob = tf.reshape(train_pos_prob,[-1])\n","        train_roc_auc.update_state(temp_train_annotations, temp_train_positive_prob)\n","        train_pr_auc.update_state(temp_train_annotations, temp_train_positive_prob)\n","\n","        # the error per replica is returned\n","        return train_err, train_roc_auc.result(), train_pr_auc.result()\n","\n","    # for the last epoch some testing has to be done, in a test step one batch of test data is fed to every GPU\n","    def Test_Step(input):\n","        test_images_batch, test_annotations_batch = input\n","\n","        # make prediction with model\n","        pred_logits = model(test_images_batch, training = False)\n","        # compute loss\n","        test_err = compute_loss(pred_logits, test_annotations_batch)\n","\n","        # turn into probability map\n","        test_pos_prob = tf.nn.sigmoid(pred_logits)\n","        # compute auc and aupr score\n","        temp_test_annotations = tf.reshape(test_annotations_batch,[-1])\n","        temp_test_positive_prob = tf.reshape(test_pos_prob,[-1])\n","        test_roc_auc.update_state(temp_test_annotations, temp_test_positive_prob)\n","        test_pr_auc.update_state(temp_test_annotations, temp_test_positive_prob)\n","    \n","        # the error per replica is returned\n","        return test_err, test_roc_auc.result(), test_pr_auc.result()\n","\n","    @tf.function\n","    def distributed_train_step(dataset_inputs):\n","        per_replica_losses, per_replica_roc_auc, per_replica_pr_auc = strategy.run(Train_Step, args=(dataset_inputs,))\n","        return per_replica_losses, per_replica_roc_auc, per_replica_pr_auc\n","\n","    @tf.function\n","    def distributed_test_step(dataset_inputs):\n","        per_replica_losses, per_replica_roc_auc, per_replica_pr_auc = strategy.run(Test_Step, args=(dataset_inputs,))\n","        return per_replica_losses, per_replica_roc_auc, per_replica_pr_auc\n","\n","\n","    # the train and test steps now have to be performed with the distributed strategy\n","    print('Training')\n","    for epo in range(1,MAX_EPOCH+1):\n","        start_time = time.time()\n","    \n","        n_train_steps = 0\n","        total_train_loss = 0\n","        total_train_auc = 0\n","        total_train_aupr = 0\n","        # go over all global batches\n","        for train_input_data in train_dist_data:\n","            n_train_steps+=1\n","            per_replica_train_losses, per_replica_train_roc_auc, per_replica_train_pr_auc = distributed_train_step(train_input_data)\n","            total_train_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_train_losses, axis=None)\n","            total_train_auc += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_train_roc_auc, axis=None)\n","            total_train_aupr += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_train_pr_auc, axis=None)\n","\n","        # every print frequency the train and test resutls are printed out\n","        if epo % print_freq == 0 or epo == 1 or epo == (MAX_EPOCH):\n","            \n","            # calculate the final training results for this epoch\n","            total_train_loss = total_train_loss/n_train_steps\n","            total_train_auc =  total_train_auc/n_train_steps\n","            total_train_aupr =  total_train_aupr/n_train_steps \n","\n","            # print out the train results\n","            print('epoch {} took {}s'.format(epo, time.time() - start_time))\n","            print('   train loss: {}'.format(total_train_loss))\n","            print('   train auc: {}'.format(total_train_auc))\n","            print('   train aupr: {}'.format(total_train_aupr))\n","\n","            if SaveResults:\n","                # save these values to visualize them later with tensorboard\n","                with summary_writer.as_default():\n","                    tf.summary.scalar('train_loss', total_train_loss, step = epo)\n","                    tf.summary.scalar('train_roc_auc', total_train_auc, step = epo)\n","                    tf.summary.scalar('train_pr_auc', total_train_aupr, step = epo)\n","                    # tf.summary.flush()   \n","\n","            # some testing has to be done at these print frequencies\n","            print('Testing')\n","            n_test_steps = 0\n","            total_test_loss = 0\n","            total_test_auc = 0\n","            total_test_aupr = 0\n","      \n","            for test_input_data in test_dist_data:\n","                n_test_steps+=1\n","                per_replica_test_losses, per_replica_test_roc_auc, per_replica_test_pr_auc = distributed_test_step(test_input_data)\n","                total_test_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_test_losses, axis=None)\n","                total_test_auc += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_test_roc_auc, axis=None)\n","                total_test_aupr += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_test_pr_auc, axis=None)\n","\n","            total_test_loss = total_test_loss/n_test_steps\n","            total_test_auc =  total_test_auc/n_test_steps\n","            total_test_aupr =  total_test_aupr/n_test_steps\n","\n","            # print out the test results      \n","            print('   validation loss: {}'.format(total_test_loss))\n","            print('   validation auc: {}'.format(total_test_auc))\n","            print('   validation aupr: {}'.format(total_test_aupr))\n","\n","            if SaveResults:\n","                with summary_writer.as_default():\n","                    tf.summary.scalar('validation_loss', total_test_loss, step = epo)\n","                    tf.summary.scalar('validation_roc_auc', total_test_auc, step = epo)\n","                    tf.summary.scalar('validation_pr_auc', total_test_aupr, step = epo)  \n","                    # tf.summary.flush()   \n","\n","        if SaveResults:\n","            # storing the model weights at two time-points\n","            if epo == int(MAX_EPOCH/2):\n","                print('Saving the intermediate model weights...')\n","                model.save_weights(log_dir_model + 'UNet_Softmax_' + str(epo) +'_epochs')\n","                print('Done')\n","                \n","        if epo == MAX_EPOCH:\n","            print('Saving the model weights...')\n","            model.save_weights(log_dir_model + 'UNet_Softmax_' + str(epo) +'_epochs')\n","            print('Done')\n","\n","    tf.summary.flush()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"14ZvJ6cp2zT9","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"663aa9de-cd0e-4840-c42f-d8cae0b9303e"},"source":["train_network(train_images, train_annotations, test_images, test_annotations, Drop_Prob = 0.3, Init_Filters = 64, batch_size = 3, loss_function = 'UNet_loss', optim = 'Adam', \n","                  learning_rate = tf.Variable(1e-3, dtype=tf.float32), MAX_EPOCH = 1000, SaveResults = True)\n","\n","# TrainImages, TrainAnnotations, TestImages, TestAnnotations, \n","#                   Drop_Prob = 0.1, Init_Filters = 64, batch_size = 3, loss_function = 'UNet_loss', optim = 'Adam', \n","#                   learning_rate = tf.Variable(1e-3, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setting up summary writer for tensorboard...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Iterator.get_next_as_optional()` instead.\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","epoch 1 took 38.75355672836304s\n","   train loss: 10.434822082519531\n","   train auc: 0.4732438623905182\n","   train aupr: 0.012881619855761528\n","Testing\n","   test loss: 0.11499778926372528\n","   test auc: 0.6687511205673218\n","   test aupr: 0.09188660234212875\n","epoch 2 took 25.775811672210693s\n","   train loss: 0.05683567747473717\n","   train auc: 0.6262312531471252\n","   train aupr: 0.025406623259186745\n","Testing\n","   test loss: 0.08027283847332001\n","   test auc: 0.6620328426361084\n","   test aupr: 0.1324353963136673\n","epoch 3 took 25.282042026519775s\n","   train loss: 0.050468023866415024\n","   train auc: 0.703255832195282\n","   train aupr: 0.05007961019873619\n","Testing\n","   test loss: 0.10442329943180084\n","   test auc: 0.665538489818573\n","   test aupr: 0.15992210805416107\n","epoch 4 took 25.225836753845215s\n","   train loss: 0.05121860280632973\n","   train auc: 0.7427857518196106\n","   train aupr: 0.08009767532348633\n","Testing\n","   test loss: 0.09260442852973938\n","   test auc: 0.66864013671875\n","   test aupr: 0.15140341222286224\n","epoch 5 took 25.523322343826294s\n","   train loss: 0.048900704830884933\n","   train auc: 0.761913537979126\n","   train aupr: 0.1065109372138977\n","Testing\n","   test loss: 0.0716441199183464\n","   test auc: 0.6777400374412537\n","   test aupr: 0.16404542326927185\n","epoch 6 took 25.491148233413696s\n","   train loss: 0.04807395488023758\n","   train auc: 0.7793979048728943\n","   train aupr: 0.134396493434906\n","Testing\n","   test loss: 0.07841262221336365\n","   test auc: 0.6827759146690369\n","   test aupr: 0.185858815908432\n","epoch 7 took 25.46957802772522s\n","   train loss: 0.043429143726825714\n","   train auc: 0.7893776297569275\n","   train aupr: 0.15317243337631226\n","Testing\n","   test loss: 0.08169230818748474\n","   test auc: 0.6920300722122192\n","   test aupr: 0.20597657561302185\n","epoch 8 took 25.439923524856567s\n","   train loss: 0.041959501802921295\n","   train auc: 0.8031448125839233\n","   train aupr: 0.1816929578781128\n","Testing\n","   test loss: 0.07797273993492126\n","   test auc: 0.7043901085853577\n","   test aupr: 0.21948954463005066\n","epoch 9 took 25.441670656204224s\n","   train loss: 0.03945990279316902\n","   train auc: 0.8130596280097961\n","   train aupr: 0.20325212180614471\n","Testing\n","   test loss: 0.06527094542980194\n","   test auc: 0.7146822810173035\n","   test aupr: 0.229684978723526\n","epoch 10 took 25.413334369659424s\n","   train loss: 0.033138208091259\n","   train auc: 0.8247078657150269\n","   train aupr: 0.22901402413845062\n","Testing\n","   test loss: 0.05473117157816887\n","   test auc: 0.7295194864273071\n","   test aupr: 0.24848411977291107\n","epoch 11 took 25.4377863407135s\n","   train loss: 0.030366145074367523\n","   train auc: 0.8369163274765015\n","   train aupr: 0.25462546944618225\n","Testing\n","   test loss: 0.0494186207652092\n","   test auc: 0.7460662126541138\n","   test aupr: 0.27116233110427856\n","epoch 12 took 25.4085853099823s\n","   train loss: 0.03017626702785492\n","   train auc: 0.8475638628005981\n","   train aupr: 0.2804737091064453\n","Testing\n","   test loss: 0.05504194274544716\n","   test auc: 0.759418249130249\n","   test aupr: 0.2894600033760071\n","epoch 13 took 25.428335428237915s\n","   train loss: 0.0274283979088068\n","   train auc: 0.8571476936340332\n","   train aupr: 0.30310797691345215\n","Testing\n","   test loss: 0.065497487783432\n","   test auc: 0.7665106058120728\n","   test aupr: 0.29766055941581726\n","epoch 14 took 25.430190086364746s\n","   train loss: 0.02718111127614975\n","   train auc: 0.8656595945358276\n","   train aupr: 0.3243250250816345\n","Testing\n","   test loss: 0.07425478845834732\n","   test auc: 0.7685579061508179\n","   test aupr: 0.3002658784389496\n","epoch 15 took 25.574411153793335s\n","   train loss: 0.023474767804145813\n","   train auc: 0.8738113641738892\n","   train aupr: 0.34678739309310913\n","Testing\n","   test loss: 0.07572823017835617\n","   test auc: 0.7690526247024536\n","   test aupr: 0.30371522903442383\n","epoch 16 took 25.484854459762573s\n","   train loss: 0.02250494621694088\n","   train auc: 0.8810025453567505\n","   train aupr: 0.36814481019973755\n","Testing\n","   test loss: 0.06135339289903641\n","   test auc: 0.7717928290367126\n","   test aupr: 0.3112926483154297\n","epoch 17 took 25.46627140045166s\n","   train loss: 0.022529933601617813\n","   train auc: 0.8876098394393921\n","   train aupr: 0.38986000418663025\n","Testing\n","   test loss: 0.06160959228873253\n","   test auc: 0.7756854295730591\n","   test aupr: 0.3201966881752014\n","epoch 18 took 25.435149908065796s\n","   train loss: 0.023662514984607697\n","   train auc: 0.8927981853485107\n","   train aupr: 0.406257301568985\n","Testing\n","   test loss: 0.04237878695130348\n","   test auc: 0.7818288803100586\n","   test aupr: 0.3319631814956665\n","epoch 19 took 25.45699667930603s\n","   train loss: 0.021166542544960976\n","   train auc: 0.8978740572929382\n","   train aupr: 0.42359277606010437\n","Testing\n","   test loss: 0.06144411861896515\n","   test auc: 0.7874077558517456\n","   test aupr: 0.34257128834724426\n","epoch 20 took 25.45675230026245s\n","   train loss: 0.022001277655363083\n","   train auc: 0.9025434851646423\n","   train aupr: 0.44072145223617554\n","Testing\n","   test loss: 0.05433814972639084\n","   test auc: 0.7909260988235474\n","   test aupr: 0.35022276639938354\n","epoch 21 took 25.486441373825073s\n","   train loss: 0.023943500593304634\n","   train auc: 0.9061737656593323\n","   train aupr: 0.4528425335884094\n","Testing\n","   test loss: 0.060746874660253525\n","   test auc: 0.7940266728401184\n","   test aupr: 0.35811537504196167\n","epoch 22 took 25.4854097366333s\n","   train loss: 0.02031802013516426\n","   train auc: 0.9099023342132568\n","   train aupr: 0.4670686721801758\n","Testing\n","   test loss: 0.046821221709251404\n","   test auc: 0.797698974609375\n","   test aupr: 0.3664073348045349\n","epoch 23 took 25.459924459457397s\n","   train loss: 0.02472587488591671\n","   train auc: 0.9130690693855286\n","   train aupr: 0.47867852449417114\n","Testing\n","   test loss: 0.07983776926994324\n","   test auc: 0.7994598150253296\n","   test aupr: 0.3700425326824188\n","epoch 24 took 25.434592962265015s\n","   train loss: 0.029406800866127014\n","   train auc: 0.9150398373603821\n","   train aupr: 0.4856228530406952\n","Testing\n","   test loss: 0.05247862637042999\n","   test auc: 0.8006823658943176\n","   test aupr: 0.37195783853530884\n","epoch 25 took 25.447405338287354s\n","   train loss: 0.023944351822137833\n","   train auc: 0.9168083071708679\n","   train aupr: 0.49164828658103943\n","Testing\n","   test loss: 0.04721146076917648\n","   test auc: 0.8046058416366577\n","   test aupr: 0.3788217306137085\n","epoch 26 took 25.436542510986328s\n","   train loss: 0.0342588946223259\n","   train auc: 0.9180752635002136\n","   train aupr: 0.4968063533306122\n","Testing\n","   test loss: 0.049997590482234955\n","   test auc: 0.8086509108543396\n","   test aupr: 0.38423454761505127\n","epoch 27 took 25.391852617263794s\n","   train loss: 0.025635577738285065\n","   train auc: 0.91938716173172\n","   train aupr: 0.5008259415626526\n","Testing\n","   test loss: 0.06407266855239868\n","   test auc: 0.8110531568527222\n","   test aupr: 0.38605788350105286\n","epoch 28 took 25.403961420059204s\n","   train loss: 0.021872524172067642\n","   train auc: 0.9213421940803528\n","   train aupr: 0.5070185661315918\n","Testing\n","   test loss: 0.08016880601644516\n","   test auc: 0.8103384375572205\n","   test aupr: 0.3851080536842346\n","epoch 29 took 25.37856888771057s\n","   train loss: 0.02459029108285904\n","   train auc: 0.9233208298683167\n","   train aupr: 0.5143530964851379\n","Testing\n","   test loss: 0.052691418677568436\n","   test auc: 0.8105628490447998\n","   test aupr: 0.3865581154823303\n","epoch 30 took 25.404648780822754s\n","   train loss: 0.021075865253806114\n","   train auc: 0.9250859618186951\n","   train aupr: 0.521019458770752\n","Testing\n","   test loss: 0.09239494055509567\n","   test auc: 0.8100213408470154\n","   test aupr: 0.38710176944732666\n","epoch 31 took 25.361014366149902s\n","   train loss: 0.02074679546058178\n","   train auc: 0.9268841743469238\n","   train aupr: 0.5283230543136597\n","Testing\n","   test loss: 0.04154422506690025\n","   test auc: 0.8103685975074768\n","   test aupr: 0.38955527544021606\n","epoch 32 took 25.37066888809204s\n","   train loss: 0.02281414158642292\n","   train auc: 0.9286549687385559\n","   train aupr: 0.5358452200889587\n","Testing\n","   test loss: 0.042142968624830246\n","   test auc: 0.8140611052513123\n","   test aupr: 0.39686936140060425\n","epoch 33 took 25.374715328216553s\n","   train loss: 0.021709607914090157\n","   train auc: 0.9300150275230408\n","   train aupr: 0.5410956144332886\n","Testing\n","   test loss: 0.047964632511138916\n","   test auc: 0.8170556426048279\n","   test aupr: 0.40322211384773254\n","epoch 34 took 25.421633005142212s\n","   train loss: 0.021096467971801758\n","   train auc: 0.931528627872467\n","   train aupr: 0.5471543073654175\n","Testing\n","   test loss: 0.05929334834218025\n","   test auc: 0.8188467025756836\n","   test aupr: 0.40685299038887024\n","epoch 35 took 25.3981511592865s\n","   train loss: 0.020118463784456253\n","   train auc: 0.9330533146858215\n","   train aupr: 0.5539839267730713\n","Testing\n","   test loss: 0.06648765504360199\n","   test auc: 0.8193398714065552\n","   test aupr: 0.4076181650161743\n","epoch 36 took 25.306774616241455s\n","   train loss: 0.0200559813529253\n","   train auc: 0.9343962073326111\n","   train aupr: 0.5599895715713501\n","Testing\n","   test loss: 0.04158427193760872\n","   test auc: 0.8207610845565796\n","   test aupr: 0.41126516461372375\n","epoch 37 took 25.441700220108032s\n","   train loss: 0.01875811628997326\n","   train auc: 0.9357638359069824\n","   train aupr: 0.5662454962730408\n","Testing\n","   test loss: 0.03390099108219147\n","   test auc: 0.8239561319351196\n","   test aupr: 0.41875123977661133\n","epoch 38 took 25.438800573349s\n","   train loss: 0.018656311556696892\n","   train auc: 0.9371333122253418\n","   train aupr: 0.5727430582046509\n","Testing\n","   test loss: 0.034993331879377365\n","   test auc: 0.8272301554679871\n","   test aupr: 0.42570462822914124\n","epoch 39 took 25.290831327438354s\n","   train loss: 0.021351540461182594\n","   train auc: 0.9382806420326233\n","   train aupr: 0.5779554843902588\n","Testing\n","   test loss: 0.05076082423329353\n","   test auc: 0.8294156193733215\n","   test aupr: 0.43057093024253845\n","epoch 40 took 25.394221544265747s\n","   train loss: 0.017873117700219154\n","   train auc: 0.9394172430038452\n","   train aupr: 0.583101749420166\n","Testing\n","   test loss: 0.049135737121105194\n","   test auc: 0.8307759165763855\n","   test aupr: 0.43465572595596313\n","epoch 41 took 25.451738834381104s\n","   train loss: 0.017587341368198395\n","   train auc: 0.9406698942184448\n","   train aupr: 0.5894551277160645\n","Testing\n","   test loss: 0.05231790989637375\n","   test auc: 0.8320819139480591\n","   test aupr: 0.43862468004226685\n","epoch 42 took 25.427106380462646s\n","   train loss: 0.017622461542487144\n","   train auc: 0.9417870044708252\n","   train aupr: 0.5949469208717346\n","Testing\n","   test loss: 0.04569514840841293\n","   test auc: 0.8335056304931641\n","   test aupr: 0.4427397549152374\n","epoch 43 took 25.396530866622925s\n","   train loss: 0.017558477818965912\n","   train auc: 0.9429386258125305\n","   train aupr: 0.60089111328125\n","Testing\n","   test loss: 0.04691502824425697\n","   test auc: 0.8350540995597839\n","   test aupr: 0.4471368193626404\n","epoch 44 took 25.379230976104736s\n","   train loss: 0.01712869107723236\n","   train auc: 0.9438945055007935\n","   train aupr: 0.6052823662757874\n","Testing\n","   test loss: 0.05111847072839737\n","   test auc: 0.8362557291984558\n","   test aupr: 0.45095473527908325\n","epoch 45 took 25.338549613952637s\n","   train loss: 0.01865478605031967\n","   train auc: 0.9449002146720886\n","   train aupr: 0.6103678941726685\n","Testing\n","   test loss: 0.04187736287713051\n","   test auc: 0.8375712633132935\n","   test aupr: 0.4547789394855499\n","epoch 46 took 25.364189624786377s\n","   train loss: 0.017426762729883194\n","   train auc: 0.9457762837409973\n","   train aupr: 0.6144698858261108\n","Testing\n","   test loss: 0.04601743444800377\n","   test auc: 0.8390859365463257\n","   test aupr: 0.45893505215644836\n","epoch 47 took 25.34372115135193s\n","   train loss: 0.017119355499744415\n","   train auc: 0.9466747045516968\n","   train aupr: 0.6189407110214233\n","Testing\n","   test loss: 0.05768867954611778\n","   test auc: 0.8398773670196533\n","   test aupr: 0.4616052508354187\n","epoch 48 took 25.364009857177734s\n","   train loss: 0.017024358734488487\n","   train auc: 0.9475377202033997\n","   train aupr: 0.6232432126998901\n","Testing\n","   test loss: 0.042906083166599274\n","   test auc: 0.840672492980957\n","   test aupr: 0.46429336071014404\n","epoch 49 took 25.404410362243652s\n","   train loss: 0.016831664368510246\n","   train auc: 0.9484595060348511\n","   train aupr: 0.6282458305358887\n","Testing\n","   test loss: 0.05662116780877113\n","   test auc: 0.8413594961166382\n","   test aupr: 0.4670047461986542\n","epoch 50 took 25.37365198135376s\n","   train loss: 0.016259508207440376\n","   train auc: 0.949245035648346\n","   train aupr: 0.632277250289917\n","Testing\n","   test loss: 0.03701600059866905\n","   test auc: 0.8422848582267761\n","   test aupr: 0.4700479209423065\n","epoch 51 took 25.411875247955322s\n","   train loss: 0.016088031232357025\n","   train auc: 0.950096070766449\n","   train aupr: 0.6368433833122253\n","Testing\n","   test loss: 0.043080177158117294\n","   test auc: 0.8437812328338623\n","   test aupr: 0.47437456250190735\n","epoch 52 took 25.443949222564697s\n","   train loss: 0.016100063920021057\n","   train auc: 0.9508211612701416\n","   train aupr: 0.6405476331710815\n","Testing\n","   test loss: 0.03994467109441757\n","   test auc: 0.8450965881347656\n","   test aupr: 0.4784034192562103\n","epoch 53 took 25.482158184051514s\n","   train loss: 0.015873346477746964\n","   train auc: 0.9516071677207947\n","   train aupr: 0.6448829770088196\n","Testing\n","   test loss: 0.0449356772005558\n","   test auc: 0.8462938070297241\n","   test aupr: 0.4821786880493164\n","epoch 54 took 25.381381034851074s\n","   train loss: 0.018041908740997314\n","   train auc: 0.9523176550865173\n","   train aupr: 0.6486369967460632\n","Testing\n","   test loss: 0.03969046100974083\n","   test auc: 0.8475142121315002\n","   test aupr: 0.48486822843551636\n","epoch 55 took 25.301283836364746s\n","   train loss: 0.0358564555644989\n","   train auc: 0.9521793127059937\n","   train aupr: 0.6485357880592346\n","Testing\n","   test loss: 0.04770559072494507\n","   test auc: 0.8486159443855286\n","   test aupr: 0.4864661991596222\n","epoch 56 took 25.31803846359253s\n","   train loss: 0.038103774189949036\n","   train auc: 0.951737105846405\n","   train aupr: 0.6462723612785339\n","Testing\n","   test loss: 0.04305090755224228\n","   test auc: 0.8495559096336365\n","   test aupr: 0.4877316653728485\n","epoch 57 took 25.286756992340088s\n","   train loss: 0.028871702030301094\n","   train auc: 0.9514316320419312\n","   train aupr: 0.6442283391952515\n","Testing\n","   test loss: 0.04011322930455208\n","   test auc: 0.8507035374641418\n","   test aupr: 0.48889076709747314\n","epoch 58 took 25.33271026611328s\n","   train loss: 0.024495430290699005\n","   train auc: 0.9516608119010925\n","   train aupr: 0.6441072821617126\n","Testing\n","   test loss: 0.04902668669819832\n","   test auc: 0.8515574336051941\n","   test aupr: 0.4902094304561615\n","epoch 59 took 25.27113962173462s\n","   train loss: 0.022256245836615562\n","   train auc: 0.9520148038864136\n","   train aupr: 0.6448704600334167\n","Testing\n","   test loss: 0.039074499160051346\n","   test auc: 0.8524271845817566\n","   test aupr: 0.49234703183174133\n","epoch 60 took 25.275920867919922s\n","   train loss: 0.02145625837147236\n","   train auc: 0.9524750709533691\n","   train aupr: 0.6464515924453735\n","Testing\n","   test loss: 0.040922459214925766\n","   test auc: 0.8535842299461365\n","   test aupr: 0.4949437975883484\n","epoch 61 took 25.398630142211914s\n","   train loss: 0.020786823704838753\n","   train auc: 0.9528911709785461\n","   train aupr: 0.64786297082901\n","Testing\n","   test loss: 0.05607234686613083\n","   test auc: 0.8541654944419861\n","   test aupr: 0.4964350163936615\n","epoch 62 took 25.315552473068237s\n","   train loss: 0.020135052502155304\n","   train auc: 0.95334792137146\n","   train aupr: 0.6495347023010254\n","Testing\n","   test loss: 0.04294031113386154\n","   test auc: 0.8546552658081055\n","   test aupr: 0.4978331923484802\n","epoch 63 took 25.263149976730347s\n","   train loss: 0.02070053666830063\n","   train auc: 0.9537916779518127\n","   train aupr: 0.6512234210968018\n","Testing\n","   test loss: 0.04575749859213829\n","   test auc: 0.8554545044898987\n","   test aupr: 0.5000354051589966\n","epoch 64 took 25.249523878097534s\n","   train loss: 0.01962946727871895\n","   train auc: 0.9542027711868286\n","   train aupr: 0.6526656746864319\n","Testing\n","   test loss: 0.05468318238854408\n","   test auc: 0.855810821056366\n","   test aupr: 0.5015054941177368\n","epoch 65 took 25.265432834625244s\n","   train loss: 0.019109390676021576\n","   train auc: 0.9546589851379395\n","   train aupr: 0.6545379757881165\n","Testing\n","   test loss: 0.039949867874383926\n","   test auc: 0.8563134074211121\n","   test aupr: 0.5031392574310303\n","epoch 66 took 25.254916191101074s\n","   train loss: 0.020892251282930374\n","   train auc: 0.9550423622131348\n","   train aupr: 0.6560728549957275\n","Testing\n","   test loss: 0.040040310472249985\n","   test auc: 0.8572956323623657\n","   test aupr: 0.5053738355636597\n","epoch 67 took 25.27134680747986s\n","   train loss: 0.01851102150976658\n","   train auc: 0.9554579257965088\n","   train aupr: 0.6578278541564941\n","Testing\n","   test loss: 0.04470713436603546\n","   test auc: 0.858073890209198\n","   test aupr: 0.5073205232620239\n","epoch 68 took 25.328144788742065s\n","   train loss: 0.018696345388889313\n","   train auc: 0.9559116363525391\n","   train aupr: 0.6598700881004333\n","Testing\n","   test loss: 0.035979386419057846\n","   test auc: 0.8589181303977966\n","   test aupr: 0.5095639824867249\n","epoch 69 took 25.350619554519653s\n","   train loss: 0.017995234578847885\n","   train auc: 0.956322431564331\n","   train aupr: 0.6616063714027405\n","Testing\n","   test loss: 0.0518702007830143\n","   test auc: 0.8595362305641174\n","   test aupr: 0.5114153623580933\n","epoch 70 took 25.342707633972168s\n","   train loss: 0.01918787509202957\n","   train auc: 0.9567471146583557\n","   train aupr: 0.6635913252830505\n","Testing\n","   test loss: 0.044813428074121475\n","   test auc: 0.8598932027816772\n","   test aupr: 0.512954592704773\n","epoch 71 took 25.347193479537964s\n","   train loss: 0.017800476402044296\n","   train auc: 0.957125186920166\n","   train aupr: 0.6651917695999146\n","Testing\n","   test loss: 0.05055597051978111\n","   test auc: 0.8602012991905212\n","   test aupr: 0.5143658518791199\n","epoch 72 took 25.349618434906006s\n","   train loss: 0.018019791692495346\n","   train auc: 0.9575282335281372\n","   train aupr: 0.6671202182769775\n","Testing\n","   test loss: 0.03789393603801727\n","   test auc: 0.860737681388855\n","   test aupr: 0.5159920454025269\n","epoch 73 took 25.294352531433105s\n","   train loss: 0.02242213301360607\n","   train auc: 0.957857072353363\n","   train aupr: 0.6685218214988708\n","Testing\n","   test loss: 0.03551659733057022\n","   test auc: 0.8617221117019653\n","   test aupr: 0.5181992650032043\n","epoch 74 took 25.31450390815735s\n","   train loss: 0.019621502608060837\n","   train auc: 0.9581255912780762\n","   train aupr: 0.6693576574325562\n","Testing\n","   test loss: 0.03481385484337807\n","   test auc: 0.8627253174781799\n","   test aupr: 0.5204553604125977\n","epoch 75 took 25.290708780288696s\n","   train loss: 0.01896408013999462\n","   train auc: 0.9584314823150635\n","   train aupr: 0.6705571413040161\n","Testing\n","   test loss: 0.037863776087760925\n","   test auc: 0.8635966181755066\n","   test aupr: 0.5225053429603577\n","epoch 76 took 25.265305519104004s\n","   train loss: 0.018138529732823372\n","   train auc: 0.9588053822517395\n","   train aupr: 0.6722683310508728\n","Testing\n","   test loss: 0.05096898972988129\n","   test auc: 0.86407470703125\n","   test aupr: 0.5238843560218811\n","epoch 77 took 25.28336501121521s\n","   train loss: 0.01679394580423832\n","   train auc: 0.9591451287269592\n","   train aupr: 0.6738532781600952\n","Testing\n","   test loss: 0.04113379493355751\n","   test auc: 0.8644724488258362\n","   test aupr: 0.5252886414527893\n","epoch 78 took 25.29702091217041s\n","   train loss: 0.017634950578212738\n","   train auc: 0.9595180153846741\n","   train aupr: 0.6757789850234985\n","Testing\n","   test loss: 0.04376911371946335\n","   test auc: 0.8650147914886475\n","   test aupr: 0.5270008444786072\n","epoch 79 took 25.30140781402588s\n","   train loss: 0.017642639577388763\n","   train auc: 0.9598609805107117\n","   train aupr: 0.6774439811706543\n","Testing\n","   test loss: 0.05339091271162033\n","   test auc: 0.865242063999176\n","   test aupr: 0.5282659530639648\n","epoch 80 took 25.287724018096924s\n","   train loss: 0.017035745084285736\n","   train auc: 0.9601903557777405\n","   train aupr: 0.6790114045143127\n","Testing\n","   test loss: 0.054076727479696274\n","   test auc: 0.8652941584587097\n","   test aupr: 0.5292960405349731\n","epoch 81 took 25.299053192138672s\n","   train loss: 0.018041273579001427\n","   train auc: 0.9605247974395752\n","   train aupr: 0.6807565093040466\n","Testing\n","   test loss: 0.04252410680055618\n","   test auc: 0.8656243681907654\n","   test aupr: 0.5308002233505249\n","epoch 82 took 25.294520378112793s\n","   train loss: 0.02950119972229004\n","   train auc: 0.9607058167457581\n","   train aupr: 0.6818389892578125\n","Testing\n","   test loss: 0.07259554415941238\n","   test auc: 0.865288496017456\n","   test aupr: 0.5296797156333923\n","epoch 83 took 25.25023102760315s\n","   train loss: 0.038738202303647995\n","   train auc: 0.9600696563720703\n","   train aupr: 0.6791749000549316\n","Testing\n","   test loss: 0.03517774119973183\n","   test auc: 0.8651800155639648\n","   test aupr: 0.5279956459999084\n","epoch 84 took 25.264817237854004s\n","   train loss: 0.0219755619764328\n","   train auc: 0.9600012302398682\n","   train aupr: 0.6786758899688721\n","Testing\n","   test loss: 0.041363902390003204\n","   test auc: 0.8659839630126953\n","   test aupr: 0.5289509296417236\n","epoch 85 took 25.264981508255005s\n","   train loss: 0.019839271903038025\n","   train auc: 0.9602313041687012\n","   train aupr: 0.6794638633728027\n","Testing\n","   test loss: 0.035404108464717865\n","   test auc: 0.8667691349983215\n","   test aupr: 0.5306133031845093\n","epoch 86 took 25.28191566467285s\n","   train loss: 0.017982520163059235\n","   train auc: 0.9605056643486023\n","   train aupr: 0.6806102991104126\n","Testing\n","   test loss: 0.037675824016332626\n","   test auc: 0.8675333261489868\n","   test aupr: 0.5325390696525574\n","epoch 87 took 25.284761428833008s\n","   train loss: 0.018119214102625847\n","   train auc: 0.9607946872711182\n","   train aupr: 0.6819882392883301\n","Testing\n","   test loss: 0.05022451654076576\n","   test auc: 0.8679426908493042\n","   test aupr: 0.5339795351028442\n","epoch 88 took 25.2822048664093s\n","   train loss: 0.018854524940252304\n","   train auc: 0.9610415697097778\n","   train aupr: 0.6830971837043762\n","Testing\n","   test loss: 0.03568734601140022\n","   test auc: 0.8683847784996033\n","   test aupr: 0.5355198383331299\n","epoch 89 took 25.26665735244751s\n","   train loss: 0.018808389082551003\n","   train auc: 0.9612981081008911\n","   train aupr: 0.6843863129615784\n","Testing\n","   test loss: 0.03455199673771858\n","   test auc: 0.8691632151603699\n","   test aupr: 0.5376197695732117\n","epoch 90 took 25.26831364631653s\n","   train loss: 0.018044011667370796\n","   train auc: 0.9615402221679688\n","   train aupr: 0.6855112910270691\n","Testing\n","   test loss: 0.04658884555101395\n","   test auc: 0.869740903377533\n","   test aupr: 0.5392149686813354\n","epoch 91 took 25.274041414260864s\n","   train loss: 0.01893230527639389\n","   train auc: 0.9617836475372314\n","   train aupr: 0.6866382360458374\n","Testing\n","   test loss: 0.033798690885305405\n","   test auc: 0.8703193664550781\n","   test aupr: 0.5408597588539124\n","epoch 92 took 25.25074005126953s\n","   train loss: 0.016187019646167755\n","   train auc: 0.9620593190193176\n","   train aupr: 0.6881013512611389\n","Testing\n","   test loss: 0.03797676041722298\n","   test auc: 0.8710321187973022\n","   test aupr: 0.5428860783576965\n","epoch 93 took 25.27142572402954s\n","   train loss: 0.017139006406068802\n","   train auc: 0.9623234868049622\n","   train aupr: 0.6894461512565613\n","Testing\n","   test loss: 0.029654664918780327\n","   test auc: 0.8718112111091614\n","   test aupr: 0.5449236631393433\n","epoch 94 took 25.259422302246094s\n","   train loss: 0.015749491751194\n","   train auc: 0.9626063108444214\n","   train aupr: 0.6909874677658081\n","Testing\n","   test loss: 0.03714720904827118\n","   test auc: 0.8725562691688538\n","   test aupr: 0.5468704104423523\n","epoch 95 took 25.28861141204834s\n","   train loss: 0.015610486268997192\n","   train auc: 0.9628831148147583\n","   train aupr: 0.6925311088562012\n","Testing\n","   test loss: 0.03742988407611847\n","   test auc: 0.8731029033660889\n","   test aupr: 0.5485106706619263\n","epoch 96 took 25.266761541366577s\n","   train loss: 0.015041611157357693\n","   train auc: 0.9631612300872803\n","   train aupr: 0.6941704750061035\n","Testing\n","   test loss: 0.046481940895318985\n","   test auc: 0.8734637498855591\n","   test aupr: 0.5498668551445007\n","epoch 97 took 25.28152585029602s\n","   train loss: 0.016080813482403755\n","   train auc: 0.963476300239563\n","   train aupr: 0.6961585879325867\n","Testing\n","   test loss: 0.036085013300180435\n","   test auc: 0.8738834857940674\n","   test aupr: 0.5512329339981079\n","epoch 98 took 25.252439260482788s\n","   train loss: 0.016544876620173454\n","   train auc: 0.9637042880058289\n","   train aupr: 0.6972640752792358\n","Testing\n","   test loss: 0.042312487959861755\n","   test auc: 0.8743982911109924\n","   test aupr: 0.5525929927825928\n","epoch 99 took 25.261189460754395s\n","   train loss: 0.015524203889071941\n","   train auc: 0.9639557003974915\n","   train aupr: 0.698677122592926\n","Testing\n","   test loss: 0.029050659388303757\n","   test auc: 0.8750410079956055\n","   test aupr: 0.5542889833450317\n","epoch 100 took 25.269624948501587s\n","   train loss: 0.017431877553462982\n","   train auc: 0.964211642742157\n","   train aupr: 0.7001125812530518\n","Testing\n","   test loss: 0.03350691869854927\n","   test auc: 0.8758379220962524\n","   test aupr: 0.5563069581985474\n","epoch 101 took 25.251850366592407s\n","   train loss: 0.015878817066550255\n","   train auc: 0.9644300937652588\n","   train aupr: 0.7011721730232239\n","Testing\n","   test loss: 0.046212367713451385\n","   test auc: 0.8762894868850708\n","   test aupr: 0.5575803518295288\n","epoch 102 took 25.26316547393799s\n","   train loss: 0.015466460026800632\n","   train auc: 0.9646766185760498\n","   train aupr: 0.7025955319404602\n","Testing\n","   test loss: 0.03702431172132492\n","   test auc: 0.8766344785690308\n","   test aupr: 0.558709979057312\n","epoch 103 took 25.291094541549683s\n","   train loss: 0.01517210528254509\n","   train auc: 0.964922308921814\n","   train aupr: 0.7040528059005737\n","Testing\n","   test loss: 0.038266077637672424\n","   test auc: 0.8771017789840698\n","   test aupr: 0.5600826740264893\n","epoch 104 took 25.268595218658447s\n","   train loss: 0.014668278396129608\n","   train auc: 0.9651644229888916\n","   train aupr: 0.7054621577262878\n","Testing\n","   test loss: 0.045777592808008194\n","   test auc: 0.87739098072052\n","   test aupr: 0.5612002611160278\n","epoch 105 took 25.256651639938354s\n","   train loss: 0.014623337425291538\n","   train auc: 0.9654269814491272\n","   train aupr: 0.7070956230163574\n","Testing\n","   test loss: 0.04407987371087074\n","   test auc: 0.8775755763053894\n","   test aupr: 0.5621983408927917\n","epoch 106 took 25.27079153060913s\n","   train loss: 0.014569609425961971\n","   train auc: 0.9656826257705688\n","   train aupr: 0.7086496949195862\n","Testing\n","   test loss: 0.046280886977910995\n","   test auc: 0.8777355551719666\n","   test aupr: 0.5631192922592163\n","epoch 107 took 25.25898790359497s\n","   train loss: 0.014429797418415546\n","   train auc: 0.9659228324890137\n","   train aupr: 0.7100858092308044\n","Testing\n","   test loss: 0.06833875179290771\n","   test auc: 0.8775052428245544\n","   test aupr: 0.5633312463760376\n","epoch 108 took 25.269713640213013s\n","   train loss: 0.01560937985777855\n","   train auc: 0.9661542177200317\n","   train aupr: 0.7114459872245789\n","Testing\n","   test loss: 0.042568616569042206\n","   test auc: 0.8773982524871826\n","   test aupr: 0.5635349750518799\n","epoch 109 took 25.24859380722046s\n","   train loss: 0.030201179906725883\n","   train auc: 0.9661731719970703\n","   train aupr: 0.7116132378578186\n","Testing\n","   test loss: 0.0517294816672802\n","   test auc: 0.8775274753570557\n","   test aupr: 0.5638843178749084\n","epoch 110 took 25.256701946258545s\n","   train loss: 0.020390773192048073\n","   train auc: 0.9661414623260498\n","   train aupr: 0.7112160325050354\n","Testing\n","   test loss: 0.03274444863200188\n","   test auc: 0.8778129816055298\n","   test aupr: 0.564501941204071\n","epoch 111 took 25.268677473068237s\n","   train loss: 0.023038480430841446\n","   train auc: 0.9662208557128906\n","   train aupr: 0.7113673090934753\n","Testing\n","   test loss: 0.03541538491845131\n","   test auc: 0.8783978223800659\n","   test aupr: 0.5656996965408325\n","epoch 112 took 25.263758659362793s\n","   train loss: 0.03029743768274784\n","   train auc: 0.9661418795585632\n","   train aupr: 0.7109788060188293\n","Testing\n","   test loss: 0.056290153414011\n","   test auc: 0.8785566091537476\n","   test aupr: 0.5658372640609741\n","epoch 113 took 25.149207830429077s\n","   train loss: 0.02052377723157406\n","   train auc: 0.9661387205123901\n","   train aupr: 0.7107527256011963\n","Testing\n","   test loss: 0.030526284128427505\n","   test auc: 0.8787807822227478\n","   test aupr: 0.5660491585731506\n","epoch 114 took 25.130282878875732s\n","   train loss: 0.018382824957370758\n","   train auc: 0.9662700295448303\n","   train aupr: 0.7112006545066833\n","Testing\n","   test loss: 0.05265180021524429\n","   test auc: 0.8790327310562134\n","   test aupr: 0.5667285323143005\n","epoch 115 took 25.277947425842285s\n","   train loss: 0.016740504652261734\n","   train auc: 0.966435968875885\n","   train aupr: 0.7120714783668518\n","Testing\n","   test loss: 0.05926402285695076\n","   test auc: 0.8788642287254333\n","   test aupr: 0.5669103860855103\n","epoch 116 took 25.226852655410767s\n","   train loss: 0.018561173230409622\n","   train auc: 0.9666073322296143\n","   train aupr: 0.7128869295120239\n","Testing\n","   test loss: 0.043134547770023346\n","   test auc: 0.8788677453994751\n","   test aupr: 0.5673617720603943\n","epoch 117 took 25.171167135238647s\n","   train loss: 0.01743321679532528\n","   train auc: 0.966758668422699\n","   train aupr: 0.7135124206542969\n","Testing\n","   test loss: 0.031708359718322754\n","   test auc: 0.879277229309082\n","   test aupr: 0.5684945583343506\n","epoch 118 took 25.223602533340454s\n","   train loss: 0.015444106422364712\n","   train auc: 0.9669226408004761\n","   train aupr: 0.7143574357032776\n","Testing\n","   test loss: 0.04213595762848854\n","   test auc: 0.8796701431274414\n","   test aupr: 0.5697274804115295\n","epoch 119 took 25.277117252349854s\n","   train loss: 0.015156764537096024\n","   train auc: 0.9671149253845215\n","   train aupr: 0.715493381023407\n","Testing\n","   test loss: 0.04010188952088356\n","   test auc: 0.8799085021018982\n","   test aupr: 0.5707535743713379\n","epoch 120 took 25.275301933288574s\n","   train loss: 0.014778660610318184\n","   train auc: 0.9673217535018921\n","   train aupr: 0.7168182134628296\n","Testing\n","   test loss: 0.04402308911085129\n","   test auc: 0.8801212310791016\n","   test aupr: 0.5717611312866211\n","epoch 121 took 25.26019024848938s\n","   train loss: 0.014718888327479362\n","   train auc: 0.9675171375274658\n","   train aupr: 0.7180037498474121\n","Testing\n","   test loss: 0.038531143218278885\n","   test auc: 0.8803505301475525\n","   test aupr: 0.5727885961532593\n","epoch 122 took 25.2533540725708s\n","   train loss: 0.014842177741229534\n","   train auc: 0.9677144289016724\n","   train aupr: 0.7192103266716003\n","Testing\n","   test loss: 0.032226040959358215\n","   test auc: 0.8807389736175537\n","   test aupr: 0.5740077495574951\n","epoch 123 took 25.26125431060791s\n","   train loss: 0.016389889642596245\n","   train auc: 0.9678942561149597\n","   train aupr: 0.7202664017677307\n","Testing\n","   test loss: 0.03485754132270813\n","   test auc: 0.8812103867530823\n","   test aupr: 0.575080156326294\n","epoch 124 took 25.24501347541809s\n","   train loss: 0.025186143815517426\n","   train auc: 0.9679224491119385\n","   train aupr: 0.7204214930534363\n","Testing\n","   test loss: 0.02879764698445797\n","   test auc: 0.8818026185035706\n","   test aupr: 0.5761407613754272\n","epoch 125 took 25.225926160812378s\n","   train loss: 0.01814154163002968\n","   train auc: 0.9679805636405945\n","   train aupr: 0.720485270023346\n","Testing\n","   test loss: 0.03696921095252037\n","   test auc: 0.8823487758636475\n","   test aupr: 0.5773382782936096\n","epoch 126 took 25.23194408416748s\n","   train loss: 0.01619166135787964\n","   train auc: 0.9681427478790283\n","   train aupr: 0.7214276194572449\n","Testing\n","   test loss: 0.05154135078191757\n","   test auc: 0.8825593590736389\n","   test aupr: 0.5780179500579834\n","epoch 127 took 25.230896472930908s\n","   train loss: 0.01585407741367817\n","   train auc: 0.9682883620262146\n","   train aupr: 0.7222614884376526\n","Testing\n","   test loss: 0.0385529063642025\n","   test auc: 0.882725179195404\n","   test aupr: 0.5786440968513489\n","epoch 128 took 25.236934185028076s\n","   train loss: 0.014994345605373383\n","   train auc: 0.9684391021728516\n","   train aupr: 0.723098874092102\n","Testing\n","   test loss: 0.03234671801328659\n","   test auc: 0.8831321001052856\n","   test aupr: 0.5798951387405396\n","epoch 129 took 25.21214532852173s\n","   train loss: 0.03059280291199684\n","   train auc: 0.9685734510421753\n","   train aupr: 0.7240370512008667\n","Testing\n","   test loss: 0.10156024992465973\n","   test auc: 0.8826186060905457\n","   test aupr: 0.579060435295105\n","epoch 130 took 25.081956148147583s\n","   train loss: 0.03882840648293495\n","   train auc: 0.9681562781333923\n","   train aupr: 0.722078800201416\n","Testing\n","   test loss: 0.03938605263829231\n","   test auc: 0.8820800185203552\n","   test aupr: 0.5772785544395447\n","epoch 131 took 25.22922420501709s\n","   train loss: 0.02327618934214115\n","   train auc: 0.9680235981941223\n","   train aupr: 0.7211390733718872\n","Testing\n","   test loss: 0.03198552131652832\n","   test auc: 0.8825206160545349\n","   test aupr: 0.577419638633728\n","epoch 132 took 25.23847270011902s\n","   train loss: 0.01961173675954342\n","   train auc: 0.9680895805358887\n","   train aupr: 0.7212297916412354\n","Testing\n","   test loss: 0.04532361775636673\n","   test auc: 0.8828350901603699\n","   test aupr: 0.5781310796737671\n","epoch 133 took 25.254045248031616s\n","   train loss: 0.017979886382818222\n","   train auc: 0.9682003259658813\n","   train aupr: 0.7217060923576355\n","Testing\n","   test loss: 0.03395286202430725\n","   test auc: 0.8831202983856201\n","   test aupr: 0.5790135860443115\n","epoch 134 took 25.25841736793518s\n","   train loss: 0.01729927770793438\n","   train auc: 0.9683305025100708\n","   train aupr: 0.7223994731903076\n","Testing\n","   test loss: 0.03046284429728985\n","   test auc: 0.8836027383804321\n","   test aupr: 0.5803002715110779\n","epoch 135 took 25.26799964904785s\n","   train loss: 0.015379751101136208\n","   train auc: 0.968474268913269\n","   train aupr: 0.7232197523117065\n","Testing\n","   test loss: 0.038712844252586365\n","   test auc: 0.883998453617096\n","   test aupr: 0.581475019454956\n","epoch 136 took 25.295365810394287s\n","   train loss: 0.015158750116825104\n","   train auc: 0.9686186909675598\n","   train aupr: 0.724015474319458\n","Testing\n","   test loss: 0.03060620278120041\n","   test auc: 0.8843772411346436\n","   test aupr: 0.5826114416122437\n","epoch 137 took 25.265318155288696s\n","   train loss: 0.015070701949298382\n","   train auc: 0.9687813520431519\n","   train aupr: 0.7250147461891174\n","Testing\n","   test loss: 0.034645285457372665\n","   test auc: 0.8847891092300415\n","   test aupr: 0.5837885141372681\n","epoch 138 took 25.222074031829834s\n","   train loss: 0.02947213687002659\n","   train auc: 0.9688276648521423\n","   train aupr: 0.7254738211631775\n","Testing\n","   test loss: 0.05593225732445717\n","   test auc: 0.8848351240158081\n","   test aupr: 0.5837612748146057\n","epoch 139 took 25.133333921432495s\n","   train loss: 0.037781182676553726\n","   train auc: 0.9685452580451965\n","   train aupr: 0.7243510484695435\n","Testing\n","   test loss: 0.0421912856400013\n","   test auc: 0.8848777413368225\n","   test aupr: 0.583130955696106\n","epoch 140 took 25.14321255683899s\n","   train loss: 0.027936603873968124\n","   train auc: 0.9683096408843994\n","   train aupr: 0.7231287956237793\n","Testing\n","   test loss: 0.05543994531035423\n","   test auc: 0.8849529027938843\n","   test aupr: 0.5827820301055908\n","epoch 141 took 25.124652862548828s\n","   train loss: 0.02260139212012291\n","   train auc: 0.9682961702346802\n","   train aupr: 0.7228845953941345\n","Testing\n","   test loss: 0.04057292640209198\n","   test auc: 0.8850011825561523\n","   test aupr: 0.582808256149292\n","epoch 142 took 25.13504385948181s\n","   train loss: 0.023870430886745453\n","   train auc: 0.9683260917663574\n","   train aupr: 0.722802996635437\n","Testing\n","   test loss: 0.031312424689531326\n","   test auc: 0.8853999376296997\n","   test aupr: 0.5833691954612732\n","epoch 143 took 25.152753829956055s\n","   train loss: 0.020722731947898865\n","   train auc: 0.9683636426925659\n","   train aupr: 0.7226887345314026\n","Testing\n","   test loss: 0.03625505045056343\n","   test auc: 0.8858236074447632\n","   test aupr: 0.5840376615524292\n","epoch 144 took 25.228591918945312s\n","   train loss: 0.018843399360775948\n","   train auc: 0.9684413075447083\n","   train aupr: 0.7229573130607605\n","Testing\n","   test loss: 0.04349764809012413\n","   test auc: 0.886053740978241\n","   test aupr: 0.5846213102340698\n","epoch 145 took 25.206812381744385s\n","   train loss: 0.01842311955988407\n","   train auc: 0.9685432314872742\n","   train aupr: 0.7234930396080017\n","Testing\n","   test loss: 0.04826520383358002\n","   test auc: 0.8861309289932251\n","   test aupr: 0.584884524345398\n","epoch 146 took 25.195093393325806s\n","   train loss: 0.01827196590602398\n","   train auc: 0.9686415791511536\n","   train aupr: 0.7239348888397217\n","Testing\n","   test loss: 0.04232197254896164\n","   test auc: 0.8862162828445435\n","   test aupr: 0.5851774215698242\n","epoch 147 took 25.20443105697632s\n","   train loss: 0.019006023183465004\n","   train auc: 0.9687468409538269\n","   train aupr: 0.7244758605957031\n","Testing\n","   test loss: 0.03750154748558998\n","   test auc: 0.8864516615867615\n","   test aupr: 0.5857571363449097\n","epoch 148 took 25.223079919815063s\n","   train loss: 0.01817912422120571\n","   train auc: 0.9688290953636169\n","   train aupr: 0.7247188091278076\n","Testing\n","   test loss: 0.039541780948638916\n","   test auc: 0.8867374658584595\n","   test aupr: 0.5863813161849976\n","epoch 149 took 25.177955389022827s\n","   train loss: 0.01703229360282421\n","   train auc: 0.9689426422119141\n","   train aupr: 0.7252968549728394\n","Testing\n","   test loss: 0.03921978175640106\n","   test auc: 0.8869993686676025\n","   test aupr: 0.5870069861412048\n","epoch 150 took 25.1820809841156s\n","   train loss: 0.016460824757814407\n","   train auc: 0.9690563678741455\n","   train aupr: 0.7259261012077332\n","Testing\n","   test loss: 0.040570616722106934\n","   test auc: 0.8872287273406982\n","   test aupr: 0.5876428484916687\n","epoch 151 took 25.14368462562561s\n","   train loss: 0.016006555408239365\n","   train auc: 0.969182550907135\n","   train aupr: 0.7266784310340881\n","Testing\n","   test loss: 0.03666030243039131\n","   test auc: 0.8874629735946655\n","   test aupr: 0.5883901119232178\n","epoch 152 took 25.15439009666443s\n","   train loss: 0.016950182616710663\n","   train auc: 0.9693032503128052\n","   train aupr: 0.7273616194725037\n","Testing\n","   test loss: 0.03897649794816971\n","   test auc: 0.887691855430603\n","   test aupr: 0.5890608429908752\n","epoch 153 took 25.173662900924683s\n","   train loss: 0.01588498242199421\n","   train auc: 0.9694111347198486\n","   train aupr: 0.7279567718505859\n","Testing\n","   test loss: 0.03806415572762489\n","   test auc: 0.8878991007804871\n","   test aupr: 0.5897061824798584\n","epoch 154 took 25.15146517753601s\n","   train loss: 0.015339814126491547\n","   train auc: 0.9695343971252441\n","   train aupr: 0.7287101149559021\n","Testing\n","   test loss: 0.041791994124650955\n","   test auc: 0.8880739808082581\n","   test aupr: 0.5904080867767334\n","epoch 155 took 25.159226894378662s\n","   train loss: 0.01759365014731884\n","   train auc: 0.96965491771698\n","   train aupr: 0.7294303774833679\n","Testing\n","   test loss: 0.049450140446424484\n","   test auc: 0.888126015663147\n","   test aupr: 0.5907666087150574\n","epoch 156 took 25.173564910888672s\n","   train loss: 0.01676008477807045\n","   train auc: 0.9697529077529907\n","   train aupr: 0.7299157381057739\n","Testing\n","   test loss: 0.028792228549718857\n","   test auc: 0.8883543610572815\n","   test aupr: 0.591168999671936\n","epoch 157 took 25.167410612106323s\n","   train loss: 0.015350151807069778\n","   train auc: 0.9698662161827087\n","   train aupr: 0.7305688261985779\n","Testing\n","   test loss: 0.03923992067575455\n","   test auc: 0.8886722326278687\n","   test aupr: 0.5918659567832947\n","epoch 158 took 25.165923357009888s\n","   train loss: 0.015647754073143005\n","   train auc: 0.9699902534484863\n","   train aupr: 0.7313197255134583\n","Testing\n","   test loss: 0.04094266518950462\n","   test auc: 0.8888524770736694\n","   test aupr: 0.5924814939498901\n","epoch 159 took 25.145967960357666s\n","   train loss: 0.014899843372404575\n","   train auc: 0.9701170921325684\n","   train aupr: 0.7320863008499146\n","Testing\n","   test loss: 0.03511149436235428\n","   test auc: 0.8890876770019531\n","   test aupr: 0.5932219624519348\n","epoch 160 took 25.182613134384155s\n","   train loss: 0.014585733413696289\n","   train auc: 0.9702423214912415\n","   train aupr: 0.7328604459762573\n","Testing\n","   test loss: 0.04291454702615738\n","   test auc: 0.8892691135406494\n","   test aupr: 0.59389328956604\n","epoch 161 took 25.195290327072144s\n","   train loss: 0.01469064224511385\n","   train auc: 0.9703670144081116\n","   train aupr: 0.7336429357528687\n","Testing\n","   test loss: 0.03522282838821411\n","   test auc: 0.8894427418708801\n","   test aupr: 0.5945473909378052\n","epoch 162 took 25.16589069366455s\n","   train loss: 0.015050232410430908\n","   train auc: 0.9704947471618652\n","   train aupr: 0.7344399690628052\n","Testing\n","   test loss: 0.03255421295762062\n","   test auc: 0.8897355198860168\n","   test aupr: 0.5953859090805054\n","epoch 163 took 25.172903299331665s\n","   train loss: 0.019813723862171173\n","   train auc: 0.970587432384491\n","   train aupr: 0.7349339127540588\n","Testing\n","   test loss: 0.05117311701178551\n","   test auc: 0.8898067474365234\n","   test aupr: 0.5957751870155334\n","epoch 164 took 25.127750635147095s\n","   train loss: 0.020143676549196243\n","   train auc: 0.9706282615661621\n","   train aupr: 0.7350416779518127\n","Testing\n","   test loss: 0.060380224138498306\n","   test auc: 0.8895353078842163\n","   test aupr: 0.5955364108085632\n","epoch 165 took 25.10824751853943s\n","   train loss: 0.026591047644615173\n","   train auc: 0.9706314206123352\n","   train aupr: 0.7349333763122559\n","Testing\n","   test loss: 0.07775124907493591\n","   test auc: 0.888870120048523\n","   test aupr: 0.594232976436615\n","epoch 166 took 25.06868267059326s\n","   train loss: 0.030896766111254692\n","   train auc: 0.9704608917236328\n","   train aupr: 0.7340512871742249\n","Testing\n","   test loss: 0.040938906371593475\n","   test auc: 0.8885504007339478\n","   test aupr: 0.5933012366294861\n","epoch 167 took 25.0925874710083s\n","   train loss: 0.03494669497013092\n","   train auc: 0.9703608155250549\n","   train aupr: 0.7334556579589844\n","Testing\n","   test loss: 0.049898918718099594\n","   test auc: 0.8886846303939819\n","   test aupr: 0.5931558012962341\n","epoch 168 took 25.04725217819214s\n","   train loss: 0.03686948120594025\n","   train auc: 0.9700416326522827\n","   train aupr: 0.7319649457931519\n","Testing\n","   test loss: 0.03407347574830055\n","   test auc: 0.888867974281311\n","   test aupr: 0.5932608246803284\n","epoch 169 took 25.06244421005249s\n","   train loss: 0.020668337121605873\n","   train auc: 0.9699965119361877\n","   train aupr: 0.7315700650215149\n","Testing\n","   test loss: 0.03006802126765251\n","   test auc: 0.889235258102417\n","   test aupr: 0.5940430164337158\n","epoch 170 took 25.077258825302124s\n","   train loss: 0.018373919650912285\n","   train auc: 0.9700654149055481\n","   train aupr: 0.7319220900535583\n","Testing\n","   test loss: 0.03595782443881035\n","   test auc: 0.8895420432090759\n","   test aupr: 0.5948005318641663\n","epoch 171 took 25.105972051620483s\n","   train loss: 0.017540108412504196\n","   train auc: 0.9701351523399353\n","   train aupr: 0.732227087020874\n","Testing\n","   test loss: 0.03913789615035057\n","   test auc: 0.8897556066513062\n","   test aupr: 0.5953748226165771\n","epoch 172 took 25.096689224243164s\n","   train loss: 0.022559497505426407\n","   train auc: 0.9702114462852478\n","   train aupr: 0.7326690554618835\n","Testing\n","   test loss: 0.04558996483683586\n","   test auc: 0.8898231983184814\n","   test aupr: 0.5951729416847229\n","epoch 173 took 25.0697762966156s\n","   train loss: 0.021703211590647697\n","   train auc: 0.9701680541038513\n","   train aupr: 0.7324659824371338\n","Testing\n","   test loss: 0.03191421180963516\n","   test auc: 0.8899596929550171\n","   test aupr: 0.5951023697853088\n","epoch 174 took 25.098600387573242s\n","   train loss: 0.01867886632680893\n","   train auc: 0.9702258706092834\n","   train aupr: 0.732730507850647\n","Testing\n","   test loss: 0.04298628866672516\n","   test auc: 0.8901477456092834\n","   test aupr: 0.5955759882926941\n","epoch 175 took 25.191629648208618s\n","   train loss: 0.017959315329790115\n","   train auc: 0.9702975749969482\n","   train aupr: 0.7331126928329468\n","Testing\n","   test loss: 0.042765062302351\n","   test auc: 0.8902400135993958\n","   test aupr: 0.595875084400177\n","epoch 176 took 25.067403316497803s\n","   train loss: 0.017116878181695938\n","   train auc: 0.9703700542449951\n","   train aupr: 0.7334833741188049\n","Testing\n","   test loss: 0.03951860964298248\n","   test auc: 0.8903669714927673\n","   test aupr: 0.596315860748291\n","epoch 177 took 25.111978769302368s\n","   train loss: 0.016642920672893524\n","   train auc: 0.9704619646072388\n","   train aupr: 0.7340503334999084\n","Testing\n","   test loss: 0.04100758209824562\n","   test auc: 0.8904966711997986\n","   test aupr: 0.5968204736709595\n","epoch 178 took 25.23263430595398s\n","   train loss: 0.016305601224303246\n","   train auc: 0.9705420136451721\n","   train aupr: 0.7344731688499451\n","Testing\n","   test loss: 0.03813080117106438\n","   test auc: 0.8906316161155701\n","   test aupr: 0.5973718166351318\n","epoch 179 took 25.14545965194702s\n","   train loss: 0.016234759241342545\n","   train auc: 0.9706456065177917\n","   train aupr: 0.735145628452301\n","Testing\n","   test loss: 0.045632973313331604\n","   test auc: 0.8907102346420288\n","   test aupr: 0.5978686809539795\n","epoch 180 took 25.086283922195435s\n","   train loss: 0.015990691259503365\n","   train auc: 0.9707352519035339\n","   train aupr: 0.7356607913970947\n","Testing\n","   test loss: 0.03997316211462021\n","   test auc: 0.8907792568206787\n","   test aupr: 0.5983035564422607\n","epoch 181 took 25.164751529693604s\n","   train loss: 0.017208460718393326\n","   train auc: 0.970827043056488\n","   train aupr: 0.7362222075462341\n","Testing\n","   test loss: 0.037428781390190125\n","   test auc: 0.8909308910369873\n","   test aupr: 0.5987942218780518\n","epoch 182 took 25.186628103256226s\n","   train loss: 0.020127292722463608\n","   train auc: 0.9708788990974426\n","   train aupr: 0.7364804148674011\n","Testing\n","   test loss: 0.04133082181215286\n","   test auc: 0.8910775780677795\n","   test aupr: 0.5992127060890198\n","epoch 183 took 25.13622260093689s\n","   train loss: 0.017192313447594643\n","   train auc: 0.97093266248703\n","   train aupr: 0.7367196083068848\n","Testing\n","   test loss: 0.034118227660655975\n","   test auc: 0.8912757039070129\n","   test aupr: 0.5997796654701233\n","epoch 184 took 25.153597593307495s\n","   train loss: 0.01612543687224388\n","   train auc: 0.9710143208503723\n","   train aupr: 0.7371587157249451\n","Testing\n","   test loss: 0.038659706711769104\n","   test auc: 0.8914892673492432\n","   test aupr: 0.6004471182823181\n","epoch 185 took 25.17195224761963s\n","   train loss: 0.015475532971322536\n","   train auc: 0.9710994362831116\n","   train aupr: 0.7376463413238525\n","Testing\n","   test loss: 0.03891505300998688\n","   test auc: 0.8916338682174683\n","   test aupr: 0.6010127067565918\n","epoch 186 took 25.147714614868164s\n","   train loss: 0.01543556246906519\n","   train auc: 0.971195638179779\n","   train aupr: 0.7382632493972778\n","Testing\n","   test loss: 0.04012656956911087\n","   test auc: 0.8917532563209534\n","   test aupr: 0.6015564799308777\n","epoch 187 took 25.15973472595215s\n","   train loss: 0.015203094109892845\n","   train auc: 0.9712944626808167\n","   train aupr: 0.7388991713523865\n","Testing\n","   test loss: 0.04401247203350067\n","   test auc: 0.8918183445930481\n","   test aupr: 0.6019951701164246\n","epoch 188 took 25.18523120880127s\n","   train loss: 0.01655479706823826\n","   train auc: 0.9713863730430603\n","   train aupr: 0.7394488453865051\n","Testing\n","   test loss: 0.03283306956291199\n","   test auc: 0.8919621109962463\n","   test aupr: 0.6025577783584595\n","epoch 189 took 25.158425331115723s\n","   train loss: 0.01538451761007309\n","   train auc: 0.9714661836624146\n","   train aupr: 0.7399086356163025\n","Testing\n","   test loss: 0.04698997363448143\n","   test auc: 0.8920794129371643\n","   test aupr: 0.6030808091163635\n","epoch 190 took 25.149773120880127s\n","   train loss: 0.015293451026082039\n","   train auc: 0.9715563654899597\n","   train aupr: 0.7404541969299316\n","Testing\n","   test loss: 0.04249370098114014\n","   test auc: 0.8920916318893433\n","   test aupr: 0.6034173965454102\n","epoch 191 took 25.130298376083374s\n","   train loss: 0.015126918442547321\n","   train auc: 0.9716557264328003\n","   train aupr: 0.7411062717437744\n","Testing\n","   test loss: 0.033755719661712646\n","   test auc: 0.8922265768051147\n","   test aupr: 0.6039589047431946\n","epoch 192 took 25.147807598114014s\n","   train loss: 0.015095840208232403\n","   train auc: 0.9717475771903992\n","   train aupr: 0.7417022585868835\n","Testing\n","   test loss: 0.03965184465050697\n","   test auc: 0.8923846483230591\n","   test aupr: 0.6045453548431396\n","epoch 193 took 25.139894485473633s\n","   train loss: 0.0147281838580966\n","   train auc: 0.9718360304832458\n","   train aupr: 0.7422438263893127\n","Testing\n","   test loss: 0.0348910391330719\n","   test auc: 0.8925338983535767\n","   test aupr: 0.6051037311553955\n","epoch 194 took 25.144625663757324s\n","   train loss: 0.014712701551616192\n","   train auc: 0.9719291925430298\n","   train aupr: 0.7428327202796936\n","Testing\n","   test loss: 0.04407196491956711\n","   test auc: 0.8926398754119873\n","   test aupr: 0.6056162118911743\n","epoch 195 took 25.154942274093628s\n","   train loss: 0.014815079048275948\n","   train auc: 0.9720143675804138\n","   train aupr: 0.7433428168296814\n","Testing\n","   test loss: 0.041187185794115067\n","   test auc: 0.8926770687103271\n","   test aupr: 0.6060395836830139\n","epoch 196 took 25.157180309295654s\n","   train loss: 0.014365197159349918\n","   train auc: 0.9721164703369141\n","   train aupr: 0.7440348863601685\n","Testing\n","   test loss: 0.03635290265083313\n","   test auc: 0.8927937746047974\n","   test aupr: 0.6065810322761536\n","epoch 197 took 25.146629810333252s\n","   train loss: 0.015272541902959347\n","   train auc: 0.972205638885498\n","   train aupr: 0.7445887923240662\n","Testing\n","   test loss: 0.03534410893917084\n","   test auc: 0.8929589986801147\n","   test aupr: 0.6071777939796448\n","epoch 198 took 25.11475920677185s\n","   train loss: 0.017337653785943985\n","   train auc: 0.9722805023193359\n","   train aupr: 0.7450711131095886\n","Testing\n","   test loss: 0.053120438009500504\n","   test auc: 0.8931118845939636\n","   test aupr: 0.6057335138320923\n","epoch 199 took 25.08933115005493s\n","   train loss: 0.021675216034054756\n","   train auc: 0.9722918272018433\n","   train aupr: 0.7450873255729675\n","Testing\n","   test loss: 0.03320278972387314\n","   test auc: 0.8932961821556091\n","   test aupr: 0.6046066284179688\n","epoch 200 took 25.11028003692627s\n","   train loss: 0.020090308040380478\n","   train auc: 0.9723332524299622\n","   train aupr: 0.7452690005302429\n","Testing\n","   test loss: 0.029932113364338875\n","   test auc: 0.8935887217521667\n","   test aupr: 0.6052612662315369\n","epoch 201 took 25.10983180999756s\n","   train loss: 0.017241202294826508\n","   train auc: 0.9723589420318604\n","   train aupr: 0.7453707456588745\n","Testing\n","   test loss: 0.034128040075302124\n","   test auc: 0.8938685655593872\n","   test aupr: 0.6059293746948242\n","epoch 202 took 25.09699535369873s\n","   train loss: 0.015286670066416264\n","   train auc: 0.9724330902099609\n","   train aupr: 0.7458187341690063\n","Testing\n","   test loss: 0.03899560123682022\n","   test auc: 0.8940355777740479\n","   test aupr: 0.6064876317977905\n","epoch 203 took 25.120505571365356s\n","   train loss: 0.01472628116607666\n","   train auc: 0.9725170731544495\n","   train aupr: 0.7463588118553162\n","Testing\n","   test loss: 0.040208496153354645\n","   test auc: 0.8941401839256287\n","   test aupr: 0.6069728136062622\n","epoch 204 took 25.134787797927856s\n","   train loss: 0.014331312850117683\n","   train auc: 0.9726055860519409\n","   train aupr: 0.7469178438186646\n","Testing\n","   test loss: 0.03864511474967003\n","   test auc: 0.8942461013793945\n","   test aupr: 0.6074920892715454\n","epoch 205 took 25.148082733154297s\n","   train loss: 0.014075424522161484\n","   train auc: 0.9726959466934204\n","   train aupr: 0.7475200891494751\n","Testing\n","   test loss: 0.0416085347533226\n","   test auc: 0.894328773021698\n","   test aupr: 0.6079758405685425\n","epoch 206 took 25.153987884521484s\n","   train loss: 0.013885118067264557\n","   train auc: 0.9727851748466492\n","   train aupr: 0.748100221157074\n","Testing\n","   test loss: 0.04021377116441727\n","   test auc: 0.8944032192230225\n","   test aupr: 0.6084336638450623\n","epoch 207 took 25.14762592315674s\n","   train loss: 0.013768802396953106\n","   train auc: 0.9728760719299316\n","   train aupr: 0.7487027049064636\n","Testing\n","   test loss: 0.042805057018995285\n","   test auc: 0.89445960521698\n","   test aupr: 0.6088532209396362\n","epoch 208 took 25.154624700546265s\n","   train loss: 0.013659091666340828\n","   train auc: 0.9729721546173096\n","   train aupr: 0.7493555545806885\n","Testing\n","   test loss: 0.038163069635629654\n","   test auc: 0.8945356011390686\n","   test aupr: 0.6093489527702332\n","epoch 209 took 25.197481155395508s\n","   train loss: 0.013522879220545292\n","   train auc: 0.9730637073516846\n","   train aupr: 0.7499514818191528\n","Testing\n","   test loss: 0.03904469311237335\n","   test auc: 0.8946317434310913\n","   test aupr: 0.6098949909210205\n","epoch 210 took 25.221977949142456s\n","   train loss: 0.013425610028207302\n","   train auc: 0.9731554985046387\n","   train aupr: 0.7505872249603271\n","Testing\n","   test loss: 0.04297604784369469\n","   test auc: 0.8946864008903503\n","   test aupr: 0.6103203296661377\n","epoch 211 took 25.228260040283203s\n","   train loss: 0.01340238656848669\n","   train auc: 0.973242461681366\n","   train aupr: 0.7511411309242249\n","Testing\n","   test loss: 0.04416222125291824\n","   test auc: 0.8947003483772278\n","   test aupr: 0.6106753945350647\n","epoch 212 took 25.24514412879944s\n","   train loss: 0.013318858109414577\n","   train auc: 0.9733400940895081\n","   train aupr: 0.7518272995948792\n","Testing\n","   test loss: 0.03851022198796272\n","   test auc: 0.8947494029998779\n","   test aupr: 0.6111055612564087\n","epoch 213 took 25.22988724708557s\n","   train loss: 0.013461037538945675\n","   train auc: 0.9734264612197876\n","   train aupr: 0.7523989081382751\n","Testing\n","   test loss: 0.05138453468680382\n","   test auc: 0.8947548270225525\n","   test aupr: 0.6114667654037476\n","epoch 214 took 25.246408224105835s\n","   train loss: 0.013304410502314568\n","   train auc: 0.973517894744873\n","   train aupr: 0.7530368566513062\n","Testing\n","   test loss: 0.04917827621102333\n","   test auc: 0.8946830034255981\n","   test aupr: 0.6116982698440552\n","epoch 215 took 25.238142728805542s\n","   train loss: 0.013288430869579315\n","   train auc: 0.97360759973526\n","   train aupr: 0.7536373138427734\n","Testing\n","   test loss: 0.0383455790579319\n","   test auc: 0.8947036266326904\n","   test aupr: 0.6120878458023071\n","epoch 216 took 25.235082864761353s\n","   train loss: 0.013209523633122444\n","   train auc: 0.9736961722373962\n","   train aupr: 0.7542186975479126\n","Testing\n","   test loss: 0.0472017265856266\n","   test auc: 0.8947295546531677\n","   test aupr: 0.6124961972236633\n","epoch 217 took 25.212514400482178s\n","   train loss: 0.013062817044556141\n","   train auc: 0.9737889170646667\n","   train aupr: 0.7548434138298035\n","Testing\n","   test loss: 0.044116292148828506\n","   test auc: 0.8947120904922485\n","   test aupr: 0.6128320097923279\n","epoch 218 took 25.238868236541748s\n","   train loss: 0.012929552234709263\n","   train auc: 0.9738720059394836\n","   train aupr: 0.75538569688797\n","Testing\n"],"name":"stdout"}]}]}