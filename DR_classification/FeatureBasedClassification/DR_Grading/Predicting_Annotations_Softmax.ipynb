{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the train and test data for a certain lesion type\n",
    "\n",
    "# Basepath depends on the lesion\n",
    "# LesionType = 'SoftExudates'\n",
    "LesionType = 'HardExudates'\n",
    "# LesionType = 'Microaneurysms'\n",
    "# LesionType = 'Hemorrhages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basepath for Jupyter notebooks:\n",
    "Basepath = 'C:/Users/lunam/Documents/1steMaster/Stage/Data_FinalArrays/Kaggle/Arrays_10GB/'\n",
    "\n",
    "# test data\n",
    "test_images = np.float32(np.load(Basepath + 'test_images_Final.npy'))\n",
    "print('Shape test images: {}'.format(test_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where the model was stored\n",
    "\n",
    "# basepath for jupyter notebooks\n",
    "base_path = 'C:/Users/lunam/Documents/1steMaster/Stage/Code_Final/DR_classification/FeatureBasedClassification/UNet_Softmax_GPU/Logs/' + LesionType + '/'\n",
    "\n",
    "# direction where the trained models are stored\n",
    "log_dir_model = base_path + 'Trained_Model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The UNet network\n",
    "def UNet(drop_prob = 0.1, init_filters = 64):\n",
    "    '''This function defines the original UNet network'''\n",
    "  \n",
    "    # initialization of the weights\n",
    "    W_init = tf.initializers.GlorotUniform()\n",
    "    \n",
    "    # Unet network\n",
    "            \n",
    "    # LEFT part\n",
    "    # input layer\n",
    "    # input_image = tf.keras.layers.InputLayer(input_shape = (512,512,3))\n",
    "    input_image = tf.keras.layers.Input(shape = (512,512,3))\n",
    "        \n",
    "    # Convolutional block 1\n",
    "    conv2d_1 = tf.keras.layers.Conv2D(init_filters, (3, 3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(input_image)\n",
    "    conv2d_2 = tf.keras.layers.Conv2D(init_filters, (3, 3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_1)\n",
    "    pool_1 = tf.keras.layers.MaxPool2D((2, 2), (2, 2))(conv2d_2)\n",
    "    dropout_1 = tf.keras.layers.Dropout(rate = drop_prob)(pool_1)\n",
    "            \n",
    "    # Convolutional block 2\n",
    "    conv2d_3 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_1)\n",
    "    conv2d_4 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_3)\n",
    "    pool_2 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_4)\n",
    "    dropout_2 = tf.keras.layers.Dropout(rate = drop_prob)(pool_2 )\n",
    "            \n",
    "    # Convolutional block 3\n",
    "    conv2d_5 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_2)\n",
    "    conv2d_6 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_5)\n",
    "    pool_3 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_6)\n",
    "    dropout_3 = tf.keras.layers.Dropout(rate = drop_prob)(pool_3)\n",
    "            \n",
    "    # Convolutional block 4\n",
    "    conv2d_7 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_3)\n",
    "    conv2d_8 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_7)\n",
    "    pool_4 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_8)\n",
    "    dropout_4 = tf.keras.layers.Dropout(rate = drop_prob)(pool_4)\n",
    "            \n",
    "    # MIDDLE part\n",
    "    conv2d_9 = tf.keras.layers.Conv2D(16*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_4)\n",
    "    conv2d_10 = tf.keras.layers.Conv2D(16*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_9)\n",
    "            \n",
    "            \n",
    "    # RIGHT part\n",
    "    # Convolutional block 1\n",
    "    upsampling_1 = tf.keras.layers.UpSampling2D((2,2))(conv2d_10)\n",
    "    concat_1 = tf.keras.layers.Concatenate(3)([upsampling_1, conv2d_8])\n",
    "    dropout_5 = tf.keras.layers.Dropout(rate = drop_prob)(concat_1)\n",
    "    conv2d_11 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_5)\n",
    "    conv2d_12 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_11)\n",
    "            \n",
    "    # Convolutional block 2\n",
    "    upsampling_2 = tf.keras.layers.UpSampling2D((2,2))(conv2d_12)\n",
    "    concat_2 = tf.keras.layers.Concatenate(3)([upsampling_2, conv2d_6])\n",
    "    dropout_6 = tf.keras.layers.Dropout(rate = drop_prob)(concat_2)\n",
    "    conv2d_13 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_6)\n",
    "    conv2d_14 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_13)\n",
    "        \n",
    "            \n",
    "    # Convolutional block 3\n",
    "    upsampling_3 = tf.keras.layers.UpSampling2D((2,2))(conv2d_14)\n",
    "    concat_3 = tf.keras.layers.Concatenate(3)([upsampling_3,conv2d_4])\n",
    "    dropout_7 = tf.keras.layers.Dropout(rate = drop_prob)(concat_3)\n",
    "    conv2d_15 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_7)\n",
    "    conv2d_16 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_15)\n",
    "            \n",
    "    # Convolutional block 4\n",
    "    upsampling_4 = tf.keras.layers.UpSampling2D((2,2))(conv2d_16)\n",
    "    concat_4 = tf.keras.layers.Concatenate(3)([upsampling_4,conv2d_2])\n",
    "    dropout_8 = tf.keras.layers.Dropout(rate = drop_prob)(concat_4)\n",
    "    conv2d_17 = tf.keras.layers.Conv2D(init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_8)\n",
    "    conv2d_18 = tf.keras.layers.Conv2D(init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_17)\n",
    "            \n",
    "    # ouput layer\n",
    "    output_image = tf.keras.layers.Conv2D(2, (1,1), kernel_initializer= W_init, padding = 'same')(conv2d_18)\n",
    "\n",
    "    # define the model\n",
    "    model = tf.keras.Model(inputs=input_image, outputs=output_image)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reload_and_Predict(TestImages, Drop_Prob = 0.1, Init_Filters = 64, log_dir_trained_model = log_dir_model, test_batch = 6, epochs = 10):\n",
    "    '''This function reloads a trained model and predicts the annotations of images'''\n",
    "\n",
    "    # Define the model for testing\n",
    "    print('Define the model for testing')\n",
    "    model = UNet(drop_prob = Drop_Prob, init_filters = Init_Filters)\n",
    "\n",
    "    # Load the stored weights of the trained model\n",
    "    print('Restoring weight parameters...')\n",
    "    model.load_weights(log_dir_trained_model + 'UNet_Softmax_'+ str(epochs) +'_epochs')\n",
    "    print('Model restored...')\n",
    "\n",
    "    # predict the annotations for the test images\n",
    "    print('Predicting the test cases...')\n",
    "\n",
    "    predicted_prob_maps = []\n",
    "    n_batches = int(TestImages.shape[0] / test_batch)\n",
    "    if TestImages.shape[0] % test_batch != 0:\n",
    "        print('Amount of test images is not dividable by the chosen test batch size!')\n",
    "        print('Number of images: {}'.format(TestImages.shape[0]))\n",
    "        print('Chosen batch size: {}'.format( test_batch))\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start = i*test_batch\n",
    "        end = start + test_batch\n",
    "        test_logits = model(TestImages[start:end], training = False)\n",
    "        test_pos_prob = tf.nn.softmax(test_logits)[:,:,:,1]\n",
    "        test_pos_prob = np.array(test_pos_prob)\n",
    "\n",
    "        if i == 0:\n",
    "            predicted_prob_maps = test_pos_prob\n",
    "        else:\n",
    "            predicted_prob_maps = np.vstack((predicted_prob_maps, test_pos_prob))\n",
    "    print('Test data predicted')\n",
    "  \n",
    "    # test_positive_prob gives the output probability map that can be used as an input to the test function\n",
    "    return predicted_prob_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_annot = Reload_and_Predict(test_images, Drop_Prob = 0.1, Init_Filters = 64, log_dir_trained_model = log_dir_model, epochs = 2, test_batch = 8)\n",
    "# epochs are needed to define the name of the file to restore the model\n",
    "# test batch has to be defined as a number through which the total amount of test images if dividable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('C:/Users/lunam/Documents/1steMaster/Stage/Data_FinalArrays/Kaggle/Arrays_10GB_float32/test_annotations_'+LesionType+'.npy', predicted_annot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
