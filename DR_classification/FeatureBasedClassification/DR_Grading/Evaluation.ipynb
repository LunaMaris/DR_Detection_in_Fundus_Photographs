{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.load('C:/Users/lunam/Documents/1steMaster/Stage/Data_FinalArrays/Kaggle/Arrays_10GB/predicted_test_labels.npy')\n",
    "test_labels = np.load('C:/Users/lunam/Documents/1steMaster/Stage/Data_FinalArrays/Kaggle/Arrays_10GB/test_labels_Final.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(pred_probs, true_probs, plot = True):\n",
    "    '''\n",
    "    This function calculates the ROC-AUC value\n",
    "    and it also calculates and visualizes the ROC-curve (if plot is true)\n",
    "    '''\n",
    "    \n",
    "    # calculate and print out ROC-AUC value\n",
    "    ROC_AUC = roc_auc_score(true_probs, pred_probs)\n",
    "    \n",
    "    # calculate and plot the ROC-curve\n",
    "    if plot:\n",
    "        FPRate, TPRate, Thresh = roc_curve(true_probs, pred_probs)\n",
    "        plt.figure()\n",
    "        plt.plot(FPRate, TPRate)\n",
    "        plt.title('ROC curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.show()\n",
    "\n",
    "    return ROC_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRC(pred_probs, true_probs, plot = True):\n",
    "    '''\n",
    "    Calculate the aupr value = the area under the precision-recall curve\n",
    "    and plot the precision-recall curve (if plot is true)\n",
    "    '''\n",
    "    \n",
    "    # calculate the precision-recall curve\n",
    "    Precision, Recall, Thresh = precision_recall_curve(true_probs, pred_probs)\n",
    "    Precision = np.fliplr([Precision])[0]  # so the array is increasing (you won't get negative AUC)\n",
    "    Recall = np.fliplr([Recall])[0]  # so the array is increasing (you won't get negative AUC)\n",
    "    AUPR = np.trapz(Precision, Recall)\n",
    "      \n",
    "    \n",
    "    # plot the precision-recall curve\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.plot(Precision, Recall)\n",
    "        plt.title('Precision-Recall curve')\n",
    "        plt.xlabel('Precision')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.show()\n",
    "    \n",
    "    return AUPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(pred_probs, true_probs, threshold_confusion = 0.5):\n",
    "    '''\n",
    "    Calculate and print out the confusion matrix\n",
    "    The values for the confusion matrix are also returned\n",
    "    A standard threhsold of 0.5 is used to calculate the confusion matrix\n",
    "    '''\n",
    "    # print out the threshold that is used for the confusion matrix\n",
    "    print(\"Confusion matrix with a used threshold of {} for the positive class:\".format(threshold_confusion))\n",
    "    \n",
    "    # turn the predicted probability maps into binary outputs according to the threshold\n",
    "    thresh_pred_probs = np.empty((pred_probs.shape[0]))\n",
    "    for i in range(pred_probs.shape[0]):\n",
    "        if pred_probs[i] >= threshold_confusion:\n",
    "            thresh_pred_probs[i] = 1\n",
    "        else:\n",
    "            thresh_pred_probs[i] = 0\n",
    "    \n",
    "    # calculate the confusion-matrix and print it out\n",
    "    TN, FP, FN, TP = confusion_matrix(true_probs, thresh_pred_probs).ravel()\n",
    "    print('Amount of true positives: {}'.format(TP))\n",
    "    print('Amount of false positives: {}'.format(FP))\n",
    "    print('Amount of true negatives: {}'.format(TN))\n",
    "    print('Amount of false negatives: {}'.format(FN))\n",
    "    \n",
    "    return TN, FP, FN, TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metrics(tn, fp, fn, tp):\n",
    "    '''Print out some metrics like the accuracy, specificity, sensitivity and precision'''\n",
    "    \n",
    "    print('Some metrics:')\n",
    "    \n",
    "    # print out accuracy\n",
    "    accuracy = 0\n",
    "    if (tn+fp+fn+tp) != 0:\n",
    "        accuracy = (tp+tn)/(tn+fp+fn+tp)\n",
    "    print('Accuracy: {}'.format(accuracy))\n",
    "    \n",
    "    # print out specificity\n",
    "    specificity = 0\n",
    "    if (tn+fp) != 0:\n",
    "        specificity = tn/(tn+fp)\n",
    "    print('Specificity: {}'.format(specificity))\n",
    "    \n",
    "    # print out sensitivity\n",
    "    sensitivity = 0\n",
    "    if (tp+fn) != 0:\n",
    "        sensitivity = tp/(tp+fn)\n",
    "    print('Sensitivity: {}'.format(sensitivity))\n",
    "    \n",
    "    # print out precision\n",
    "    precision = 0\n",
    "    if (tp+fp) != 0:\n",
    "        precision = tp/(tp+fp)\n",
    "    print('Precision: {}'.format(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of the model\n",
    "def Evaluate(pred_labels, true_labels):\n",
    "    '''\n",
    "    Evaluation of the network, based on comparisson between the true and predicted outputs\n",
    "    '''\n",
    "    print('Evaluating the model...')\n",
    "    \n",
    "    # creat a 1D numpy array with the predicted and true outputs\n",
    "    pred_pos_probs = pred_labels[:,0]\n",
    "    true_pos_probs = true_labels[:,0]\n",
    "\n",
    "    # print out ROC-AUC value and plot the ROC-curve\n",
    "    ROC_AUC = ROC(pred_pos_probs, true_pos_probs)\n",
    "    print('Area under the ROC curve: {}'.format(ROC_AUC))\n",
    "\n",
    "    # print out area under the precision-recall curve, AUPR value\n",
    "    AUPR = PRC(pred_pos_probs, true_pos_probs)\n",
    "    print('Area under Precision-Recall curve: {}'.format(AUPR))\n",
    "    \n",
    "    # print out confusion matrix\n",
    "    TN, FP, FN, TP = ConfusionMatrix(pred_pos_probs, true_pos_probs)\n",
    "    \n",
    "    # print out some metrics\n",
    "    Metrics(TN, FP, FN, TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluate(predicted_labels , test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
