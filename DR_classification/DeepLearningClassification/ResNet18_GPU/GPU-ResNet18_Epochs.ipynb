{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GPU-ResNet18_Epochs.ipynb","provenance":[{"file_id":"1MshBmCalzqGz5xnx1H0ErFeGsBx_02XT","timestamp":1596527489779},{"file_id":"1pvSj2cOo177dqWp1y6lbiwohGTGeGVm1","timestamp":1596124850987},{"file_id":"1ko8HCMMTNlaR1MCC-1f6QL1uAaqzHXwO","timestamp":1595928299309},{"file_id":"1cjKN36peUbhC36sgbu1cp5rwTPmzAtcg","timestamp":1595852695030}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"9eyAksxJOFsi","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597750349520,"user_tz":-120,"elapsed":2645,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"08986518200734496308"}},"outputId":"abff7e05-1792-441a-c4c8-0ad1b76e5518"},"source":["# import necessary libraries\n","\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import precision_recall_curve\n","\n","from sklearn.model_selection import train_test_split\n","\n","import time\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D-lg7YM4d9uB","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597750349521,"user_tz":-120,"elapsed":2628,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"08986518200734496308"}},"outputId":"a4a9d0c8-5cfc-4c0d-b018-4bff56441ee5"},"source":["# define how many gpus are available and set a memmory limit\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","print(\"Number of GPUs Available: \", len(gpus))\n","for i in range(len(gpus)):\n","    tf.config.experimental.set_virtual_device_configuration(gpus[i], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7900)]) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of GPUs Available:  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rg73aXvDxvcP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597750349962,"user_tz":-120,"elapsed":3054,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"08986518200734496308"}},"outputId":"2762a378-ca9b-47e2-d45a-102297b6c5db"},"source":["strategy = tf.distribute.MirroredStrategy()\n","# the number of replicas that is created by the strategy should be equal to the number of GPU's available\n","print ('Number of synchronized replicas created: {}'.format(strategy.num_replicas_in_sync))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","Number of synchronized replicas created: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"M85RveB_uPv3","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597750349963,"user_tz":-120,"elapsed":3041,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"08986518200734496308"}},"outputId":"fb26573b-c1bd-4cfc-9756-2ada5a3923a2"},"source":["# read in train and test data in case Google DRIVE is used\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Na905WmrltfS","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1597750349964,"user_tz":-120,"elapsed":3022,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"08986518200734496308"}},"outputId":"bcda6ee0-ebab-4f7a-e879-5ab6e3de4d45"},"source":["# Basepath for Google DRIVE:\n","Basepath = '/content/drive/My Drive/Stage_ENT_Studios_2/Data/Kaggle/Arrays_5GB_float32/'\n","\n","# Basepath for Jupyter notebooks:\n","# Basepath = 'C:/Users/lunam/Documents/1steMaster/Stage/Data_FinalArrays/Kaggle/Array_10GB/'\n","# also a version with 5GB and 20GB\n","\n","# Basepath for KILI\n","# Basepath = '/home/kili/Desktop/Data_FinalArrays/Kaggle/Arrays/'\n","\n","# train data\n","train_images = np.load(Basepath + 'train_images_Final.npy')\n","print('Shape train images: {}'.format(train_images.shape))\n","\n","train_labels =  np.load(Basepath + 'train_labels_Final.npy')\n","print('Shape train labels: {}'.format(train_labels.shape))\n","\n","train_images, test_images, train_labels, test_labels = train_test_split(train_images, train_labels, test_size=0.33, random_state=42)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape train images: (324, 256, 256, 3)\n","Shape train labels: (324, 2)\n","Shape test images: (156, 256, 256, 3)\n","Shape test labels: (156, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WWjV59oPltfZ","colab":{}},"source":["# path to save the model and the tensorboard logs\n","\n","# Basepath for Google DRIVE:\n","base_path = '/content/drive/My Drive/Stage_ENT_Studios_2/DR_Grading/Logs/'\n","\n","# Basepath for jupyter notebooks:\n","# base_path = 'C:/Users/lunam/Documents/1steMaster/Stage/Code_Final/DR_classification/DeepLearningClassification/ResNet18_GPU/Logs/'\n","\n","# direction where the tensorboard files will be stored\n","log_dir_tens = base_path + 'Tensorboard_Logs/'\n","# direction where the trained models will be stored\n","log_dir_model = base_path + 'Trained_Model/'\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Qilk49xu8_9D"},"source":["Some information on the Identity Block:\n","\n","The input and output of the identity block have the same dimensions.\n","    \n","The shortcut path consists of the identity function.\n","    \n","The main path consists of 3 convolutional layers each time followed by a batch normalization.\n","The first two convolutional layers are also followed by a Relu activation function.\n","    \n","At the end the shortcut path and main path are brought together and a Relu activation is added. \n","    \n","n_filters = number of filters for the first layer of the main path\n","dropout can be added to avoid overfitting, the dropout rate can be defined\n","    \n","The function returns an output with the same dimensions as the input\n","\n","\n","\n","\n","Some information on the Convolutional Block:\n","\n","Input and output of the convolutional block don't have the same dimensions.\n","    \n","Shortcut path consists of a convolutional layer followed by a batch normalization\n","    \n","Main path consists of 3 convolutional layers each time followed by a batch normalization.\n","The first two convolutional layers are also followed by a Relu activation function.\n","    \n","At the end the shortcut path and main path are brought together and a Relu activation is added. \n","    \n","Input Image = the input to this identity block.\n","n_filters = a list with 3 values indicating the numer of filters used in the three convolutional layers of the main path\n","the amount of filters used in the third layer of the main path equals the amount of filters used in the shortcut path\n","dropout and maxpooling can be added to avoid overfitting and to make the amount of parameters smaller (the dropout rate can also be defined)\n","    \n","The function returns an output image with a width and height that are half of the width and height of the input image"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yk5fG0BbDQlF","colab":{}},"source":["def Identity_Block(Input_Image, n_filters, dropout_prob = 0):\n","\n","\n","    # initialization of the weights\n","    W_init = tf.initializers.GlorotUniform()\n","    \n","    # Shortcut path: Identity function\n","    Shortcut_Image = Input_Image\n","    \n","    # Main path, Main_Image represents the image that is passed through different layers\n","    Main_Image = Input_Image\n","\n","    Main_Image = tf.keras.layers.Conv2D(n_filters, (3,3), kernel_initializer= W_init, padding = 'same')(Main_Image) # padding is needed to keep the same dimensions\n","    Main_Image = tf.keras.layers.BatchNormalization(axis = 1)(Main_Image)\n","    Main_Image = tf.nn.relu(Main_Image)\n","\n","    Main_Image = tf.keras.layers.Conv2D(n_filters, (3,3), kernel_initializer= W_init, padding = 'same')(Main_Image)\n","    \n","    # Output: Relu activation from (Shortcut path + Main path)\n","    Output_Image = tf.keras.layers.Add()([Main_Image, Shortcut_Image])\n","\n","    if dropout_prob != 0:\n","        Output_Image = tf.keras.layers.Dropout(rate = dropout_prob)(Output_Image)\n","\n","    Output_Image = tf.nn.relu(Output_Image)\n","    \n","    return Output_Image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Bf3-xOK6DRba","colab":{}},"source":["def Convolutional_Block(Input_Image, n_filters, dropout_prob = 0, pooling = False):\n","    \n","\n","    # initialization of the weights\n","    W_init = tf.initializers.GlorotUniform()\n","\n","    # Shortcut path\n","    Shortcut_Image = Input_Image\n","    Shortcut_Image = tf.keras.layers.Conv2D(n_filters, (3,3), strides = (2,2), kernel_initializer= W_init)(Shortcut_Image) # stride leads to a reduction in size\n","    Shortcut_Image = tf.keras.layers.BatchNormalization(axis = 1)(Shortcut_Image)\n","\n","    # Main path\n","    Main_Image = Input_Image\n","    \n","    Main_Image = tf.keras.layers.Conv2D(n_filters, (3,3), strides = (2,2), kernel_initializer= W_init)(Main_Image)\n","    Main_Image = tf.keras.layers.BatchNormalization(axis = 1)(Main_Image)\n","    Main_Image = tf.nn.relu(Main_Image)\n","\n","    Main_Image = tf.keras.layers.Conv2D(n_filters, (3,3), kernel_initializer= W_init, padding = 'same')(Main_Image) # padding is needed to keep the same dimensions\n","    Main_Image = tf.keras.layers.BatchNormalization(axis = 1)(Main_Image)\n","\n","\n","    # Output: Relu activation from (Shortcut path + Main path)\n","    Output_Image = tf.keras.layers.Add()([Main_Image, Shortcut_Image])\n","\n","    if pooling:\n","        Output_Image = tf.keras.layers.MaxPool2D((2,2))(Output_Image)\n","\n","    if dropout_prob != 0:\n","        Output_Image = tf.keras.layers.Dropout(rate = dropout_prob)(Output_Image)\n","\n","    Output_Image = tf.nn.relu(Output_Image)\n","    \n","    return Output_Image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yNRXH90ZCLVI","colab":{}},"source":["def ResNet18(init_filters = 64, drop_prob = 0.1, dense_nodes = 1000, ExtraPooling = False):\n","    '''This function defines the original ResNet18 network'''\n","    \n","    # initialization of the weights\n","    W_init = tf.initializers.GlorotUniform()\n","    \n","    # Input\n","    X_Input = tf.keras.layers.Input(shape = (256,256, 3))\n","    \n","    # Convolutional layer 1\n","    X = tf.keras.layers.ZeroPadding2D((3, 3))(X_Input)\n","    X = tf.keras.layers.Conv2D(init_filters, (7,7), strides=(2, 2), kernel_initializer= W_init)(X)\n","    X = tf.keras.layers.BatchNormalization(axis = 1)(X)\n","    X = tf.nn.relu(X)\n","    \n","    # Convolutional layer 2\n","    X = tf.keras.layers.ZeroPadding2D((1, 1))(X)\n","    X = tf.keras.layers.MaxPool2D((3,3), strides = (2,2))(X)\n","    X = Identity_Block(X, init_filters, dropout_prob = drop_prob)\n","    X = Identity_Block(X, init_filters, dropout_prob = 0)\n","\n","    # Convolutional layer 3\n","    X = tf.keras.layers.ZeroPadding2D((1, 1))(X)\n","    X = Convolutional_Block(X, 2* init_filters, dropout_prob = drop_prob, pooling = ExtraPooling)\n","    X = Identity_Block(X, 2* init_filters, dropout_prob = 0)\n","\n","    # Convolutional layer 4\n","    X = tf.keras.layers.ZeroPadding2D((1, 1))(X)\n","    X = Convolutional_Block(X,  4* init_filters, dropout_prob = drop_prob, pooling = ExtraPooling)\n","    X = Identity_Block(X, 4* init_filters, dropout_prob = 0)\n","\n","    # Convolutional layer 5\n","    X = tf.keras.layers.ZeroPadding2D((1, 1))(X)\n","    X = Convolutional_Block(X, 8* init_filters, dropout_prob = drop_prob)\n","    X = Identity_Block(X, 8* init_filters, dropout_prob = 0)\n","\n","    # Output layer\n","    X = tf.keras.layers.AveragePooling2D((2,2))(X)\n","    X = tf.keras.layers.Flatten()(X)\n","    X = tf.keras.layers.Dense(dense_nodes, activation= tf.nn.relu)(X)\n","    X_Output = tf.keras.layers.Dense(2, activation= tf.nn.softmax)(X) # dense layer where the chance on benign and malignant is represented\n","\n","    # define the model\n","    model = tf.keras.Model(inputs = X_Input, outputs = X_Output)\n","      \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uXe-N1A0JgM5","colab":{}},"source":["# loss function\n","def Bin_CrossEntropy_Loss(pred_labels, true_labels, GlobalBatchSize):\n","    true_labels_pos = true_labels[:,0]\n","    pred_labels_pos = pred_labels[:,0]\n","\n","    loss_object = tf.keras.losses.BinaryCrossentropy(reduction= tf.keras.losses.Reduction.NONE)\n","    loss = loss_object([true_labels_pos], [pred_labels_pos])[0]\n","    return (loss/ GlobalBatchSize)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VQg7qWs9Wo9r","colab":{}},"source":["def train_network(TrainImages, TrainLabels, TestImages, TestLabels, \n","                  Drop_Prob = 0.1, Init_Filters = 64, Dense_Nodes = 1000, extra_pooling = False, batch_size = 3, loss_function = 'BinCrossEntr', optim = 'Adam', \n","                  learning_rate = tf.Variable(1e-5, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True, print_freq = 1):\n","    '''\n","    This function trains the UNet on the indicated train data with corresponding annotations\n","    At the end the trained model is being saved\n","    '''\n","    # setting up saver for the tensorboard logs\n","    if SaveResults:\n","        # creating summary which stores the results that can be visualised with tensorboard\n","        print(\"Setting up summary writer for tensorboard...\")\n","        summary_writer = tf.summary.create_file_writer(log_dir_tens)\n","\n","    # define the train and test batches that can be fed into the network\n","    # global batch size defines the batch size over all availabel GPU's\n","    print('Creating distributed data')\n","    Global_batch_size = batch_size * strategy.num_replicas_in_sync\n","    train_batch_data  = tf.data.Dataset.from_tensor_slices((TrainImages, TrainLabels)).shuffle(TrainImages.shape[0]).batch(Global_batch_size) \n","    test_batch_data = tf.data.Dataset.from_tensor_slices((TestImages, TestLabels)).batch(Global_batch_size) \n","\n","    # distribute the data over the different GPU's\n","    train_dist_data =  strategy.experimental_distribute_dataset(train_batch_data)\n","    test_dist_data =  strategy.experimental_distribute_dataset(test_batch_data)\n","\n","    # define the model that will be used for training and for testing\n","    # the model, optimisation and loss have to be distributed among GPU's\n","    tf.compat.v1.reset_default_graph()\n","    with strategy.scope():\n","\n","        # model\n","        print('Defining the model')\n","        model = ResNet18(init_filters = Init_Filters, drop_prob = Drop_Prob, dense_nodes= Dense_Nodes, ExtraPooling= extra_pooling)\n","        \n","        # loss\n","        print('Defining loss')\n","        def compute_loss(PredictedLabels, TrueLabels):\n","            if loss_function == 'BinCrossEntr':\n","                loss = Bin_CrossEntropy_Loss(PredictedLabels, TrueLabels, Global_batch_size)\n","            return loss\n","\n","        # optimization\n","        # a decaying learning rate is used\n","        steps_per_epoch = int(TrainImages.shape[0]/Global_batch_size)\n","        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate= learning_rate, decay_steps= MAX_EPOCH*steps_per_epoch*0.25, decay_rate=0.2, staircase = True)\n","        print('Defining optimization')\n","        if optim == 'Adam':\n","            train_op = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n","        elif optim == 'sgd':\n","            train_op = tf.keras.optimizers.SGD(learning_rate = lr_schedule)\n","\n","        # defining the metrics\n","        print('Defining the metrics')\n","        train_roc_auc = tf.keras.metrics.AUC(curve = 'ROC')\n","        train_pr_auc = tf.keras.metrics.AUC(curve = 'PR')\n","        test_roc_auc = tf.keras.metrics.AUC(curve = 'ROC')\n","        test_pr_auc = tf.keras.metrics.AUC(curve = 'PR')\n","\n","    # one train step is a step in which one batch of data is fed to every GPU\n","    # one train step is a step in which one batch of data is fed to every GPU\n","    def Train_Step(input):\n","\n","        with tf.GradientTape() as tape:\n","            train_images_batch, train_labels_batch = input\n","        \n","            # make prediction with model\n","            pred_train_labels = model(train_images_batch, training = True)\n","            # compute loss\n","            train_err = compute_loss(pred_train_labels, train_labels_batch)\n","        \n","        # update model\n","        train_weights = model.trainable_variables\n","        gradients = tape.gradient(train_err, train_weights)\n","        train_op.apply_gradients(zip(gradients, train_weights))\n","\n","        # compute auc and aupr score\n","        temp_train_labels = train_labels_batch[:,0]\n","        temp_train_pred_labels = pred_train_labels[:,0]\n","        train_roc_auc.update_state(temp_train_labels, temp_train_pred_labels)\n","        train_pr_auc.update_state(temp_train_labels, temp_train_pred_labels)\n","\n","        # the error per replica is returned\n","        return train_err, train_roc_auc.result(), train_pr_auc.result()\n","\n","\n","    # for the last epoch some testing has to be done, in a test step one batch of test data is fed to every GPU\n","    def Test_Step(input):\n","        test_images_batch, test_labels_batch = input\n","\n","        # make prediction with model\n","        pred_test_labels = model(test_images_batch, training = False)\n","        # compute loss\n","        test_err = compute_loss(pred_test_labels, test_labels_batch)\n","        \n","        # compute auc and aupr score\n","        temp_test_labels = test_labels_batch[:,0]\n","        temp_test_pred_labels = pred_test_labels[:,0]\n","        test_roc_auc.update_state(temp_test_labels, temp_test_pred_labels)\n","        test_pr_auc.update_state(temp_test_labels, temp_test_pred_labels)\n","\n","        # the error per replica is returned\n","        return test_err, test_roc_auc.result(), test_pr_auc.result()\n","\n","    @tf.function\n","    def distributed_train_step(dataset_inputs):\n","        per_replica_losses, per_replica_roc_auc, per_replica_pr_auc = strategy.run(Train_Step, args=(dataset_inputs,))\n","        return per_replica_losses, per_replica_roc_auc, per_replica_pr_auc\n","\n","    @tf.function\n","    def distributed_test_step(dataset_inputs):\n","        per_replica_losses, per_replica_roc_auc, per_replica_pr_auc = strategy.run(Test_Step, args=(dataset_inputs,))\n","        return per_replica_losses, per_replica_roc_auc, per_replica_pr_auc\n","        \n","    \n","    # the train and test steps now have to be performed with the distributed strategy\n","    print('Training')\n","    for epo in range(1,MAX_EPOCH+1):\n","        start_time = time.time()\n","        \n","        n_train_steps = 0\n","        total_train_loss = 0\n","        total_train_auc = 0\n","        total_train_aupr = 0\n","        # go over all global batches\n","        for train_input_data in train_dist_data:\n","            n_train_steps+=1\n","            per_replica_train_losses, per_replica_train_roc_auc, per_replica_train_pr_auc = distributed_train_step(train_input_data)\n","            total_train_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_train_losses, axis=None)\n","            total_train_auc += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_train_roc_auc, axis=None)\n","            total_train_aupr += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_train_pr_auc, axis=None)\n","    \n","\n","        # every print frequency the train and test resutls are printed out\n","        if epo % print_freq == 0 or epo == 1 or epo == (MAX_EPOCH):\n","\n","            # calculate the final training results for this epoch\n","            total_train_loss = total_train_loss/n_train_steps\n","            total_train_auc = total_train_auc/n_train_steps\n","            total_train_aupr = total_train_aupr/n_train_steps\n","\n","            # print out the train results\n","            print('epoch {} took {}s'.format(epo, time.time() - start_time))\n","            print('   train loss: {}'.format(total_train_loss))\n","            print('   train auc: {}'.format(total_train_auc))\n","            print('   train aupr: {}'.format(total_train_aupr))\n","\n","            if SaveResults:      \n","                # save these values to visualize them later with tensorboard\n","                with summary_writer.as_default():\n","                    tf.summary.scalar('train_loss', total_train_loss, step = epo)\n","                    tf.summary.scalar('train_roc_auc', total_train_auc, step = epo)\n","                    tf.summary.scalar('train_pr_auc', total_train_aupr, step = epo)\n","\n","\n","            # some testing has to be done at these print frequencies\n","            print('Testing')\n","            n_test_steps = 0\n","            total_test_loss = 0\n","            total_test_auc = 0\n","            total_test_aupr = 0\n","\n","            for test_input_data in test_dist_data:\n","                n_test_steps+=1\n","                per_replica_test_losses, per_replica_test_roc_auc, per_replica_test_pr_auc = distributed_test_step(test_input_data)\n","                total_test_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_test_losses, axis=None)\n","                total_test_auc += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_test_roc_auc, axis=None)\n","                total_test_aupr += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_test_pr_auc, axis=None)\n","\n","            total_test_loss = total_test_loss/n_test_steps\n","            total_test_auc = total_test_auc/n_test_steps\n","            total_test_aupr = total_test_aupr/n_test_steps\n","\n","            # print out the test results      \n","            print('   validation loss: {}'.format(total_test_loss))\n","            print('   validation auc: {}'.format(total_test_auc))\n","            print('   validation aupr: {}'.format(total_test_aupr))\n","\n","            if SaveResults:\n","                with summary_writer.as_default():\n","                    tf.summary.scalar('validation_loss', total_test_loss, step = epo)\n","                    tf.summary.scalar('validation_roc_auc', total_test_auc, step = epo)\n","                    tf.summary.scalar('validation_pr_auc', total_test_aupr, step = epo)  \n","                    summary_writer.flush()     \n","\n","\n","        if SaveResults:\n","        # storing the model weights at two time-points\n","            if epo == int(MAX_EPOCH/2):\n","                print('Saving the intermediate model weights...')\n","                model.save_weights(log_dir_model + 'ResNet18_' + str(epo) +'_epochs')\n","                print('Done')\n","\n","            if epo == MAX_EPOCH:\n","                print('Saving the model weights...')\n","                model.save_weights(log_dir_model + 'ResNet18_' + str(epo) +'_epochs')\n","                print('Done')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AY41_1b53yRn","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1597750473185,"user_tz":-120,"elapsed":126171,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"08986518200734496308"}},"outputId":"3d0d4bec-4b25-4996-f60a-6e48e17c7d02"},"source":["train_network(train_images, train_labels, test_images, test_labels, MAX_EPOCH = 100, learning_rate = tf.Variable(1e-5, dtype=tf.float32), extra_pooling = True, Drop_Prob = 0.5, Dense_Nodes = 100, batch_size = 3)\n","# (TrainImages, TrainLabels, TestImages, TestLabels, \n","#                   Drop_Prob = 0.1, Init_Filters = 64, Dense_Nodes = 1000, extra_pooling = False, batch_size = 3, loss_function = 'BinCrossEntr', optim = 'Adam', \n","#                   learning_rate = tf.Variable(1e-5, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True, print_freq = 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setting up summary writer for tensorboard...\n","Creating distributed data\n","Defining the model\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Iterator.get_next_as_optional()` instead.\n","epoch 1 took 13.830925703048706s\n","   train loss: 0.30618780851364136\n","   train auc: 0.39174407720565796\n","   train aupr: 0.43574732542037964\n","Testing\n","   test loss: 0.23150362074375153\n","   test auc: 0.361540824174881\n","   test aupr: 0.36661168932914734\n","epoch 2 took 6.887464284896851s\n","   train loss: 0.24933497607707977\n","   train auc: 0.4397951364517212\n","   train aupr: 0.4631093740463257\n","Testing\n","   test loss: 0.23127548396587372\n","   test auc: 0.4829382300376892\n","   test aupr: 0.4455494284629822\n","epoch 3 took 6.902566432952881s\n","   train loss: 0.26750269532203674\n","   train auc: 0.46695348620414734\n","   train aupr: 0.48079338669776917\n","Testing\n","   test loss: 0.23276397585868835\n","   test auc: 0.49180805683135986\n","   test aupr: 0.45852404832839966\n","epoch 4 took 6.862426996231079s\n","   train loss: 0.2663891017436981\n","   train auc: 0.46396899223327637\n","   train aupr: 0.476161926984787\n","Testing\n","   test loss: 0.23107515275478363\n","   test auc: 0.4817219078540802\n","   test aupr: 0.45935410261154175\n","epoch 5 took 6.851373910903931s\n","   train loss: 0.24900515377521515\n","   train auc: 0.47083964943885803\n","   train aupr: 0.47919711470603943\n","Testing\n","   test loss: 0.23104926943778992\n","   test auc: 0.4796277582645416\n","   test aupr: 0.45840781927108765\n","epoch 6 took 6.882370471954346s\n","   train loss: 0.2641064524650574\n","   train auc: 0.473305344581604\n","   train aupr: 0.48092904686927795\n","Testing\n","   test loss: 0.2367384284734726\n","   test auc: 0.4846556782722473\n","   test aupr: 0.4686741828918457\n","epoch 7 took 6.889599561691284s\n","   train loss: 0.2540060877799988\n","   train auc: 0.4758288860321045\n","   train aupr: 0.4815203845500946\n","Testing\n","   test loss: 0.23248478770256042\n","   test auc: 0.4914361834526062\n","   test aupr: 0.4829866588115692\n","epoch 8 took 6.868798017501831s\n","   train loss: 0.25662609934806824\n","   train auc: 0.47899898886680603\n","   train aupr: 0.48137345910072327\n","Testing\n","   test loss: 0.2318064421415329\n","   test auc: 0.4948701560497284\n","   test aupr: 0.4852578341960907\n","epoch 9 took 6.8327343463897705s\n","   train loss: 0.2539385259151459\n","   train auc: 0.47778579592704773\n","   train aupr: 0.482939213514328\n","Testing\n","   test loss: 0.23103955388069153\n","   test auc: 0.497491717338562\n","   test aupr: 0.4864538013935089\n","epoch 10 took 6.8723931312561035s\n","   train loss: 0.24682560563087463\n","   train auc: 0.4775005877017975\n","   train aupr: 0.4816306531429291\n","Testing\n","   test loss: 0.2314370721578598\n","   test auc: 0.4952099621295929\n","   test aupr: 0.484817773103714\n","epoch 11 took 6.864679574966431s\n","   train loss: 0.24574220180511475\n","   train auc: 0.4786103665828705\n","   train aupr: 0.48396193981170654\n","Testing\n","   test loss: 0.23099854588508606\n","   test auc: 0.49691498279571533\n","   test aupr: 0.4853477478027344\n","epoch 12 took 6.857494831085205s\n","   train loss: 0.2541128396987915\n","   train auc: 0.47680944204330444\n","   train aupr: 0.48133760690689087\n","Testing\n","   test loss: 0.23222610354423523\n","   test auc: 0.49710413813591003\n","   test aupr: 0.4848826229572296\n","epoch 13 took 6.888476610183716s\n","   train loss: 0.24657590687274933\n","   train auc: 0.47852274775505066\n","   train aupr: 0.4831542670726776\n","Testing\n","   test loss: 0.23113586008548737\n","   test auc: 0.4992893636226654\n","   test aupr: 0.486460417509079\n","epoch 14 took 6.901388645172119s\n","   train loss: 0.23829391598701477\n","   train auc: 0.4800224304199219\n","   train aupr: 0.48446014523506165\n","Testing\n","   test loss: 0.23113378882408142\n","   test auc: 0.4982061982154846\n","   test aupr: 0.4855285882949829\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-7cf76e8ccc08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_EPOCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_pooling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDrop_Prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense_Nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# (TrainImages, TrainLabels, TestImages, TestLabels,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#                   Drop_Prob = 0.1, Init_Filters = 64, Dense_Nodes = 1000, extra_pooling = False, batch_size = 3, loss_function = 'BinCrossEntr', optim = 'Adam',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#                   learning_rate = tf.Variable(1e-5, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True, print_freq = 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-4a277d405e46>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(TrainImages, TrainLabels, TestImages, TestLabels, Drop_Prob, Init_Filters, Dense_Nodes, extra_pooling, batch_size, loss_function, optim, learning_rate, MAX_EPOCH, SaveResults, print_freq)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_input_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dist_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mn_train_steps\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mper_replica_train_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_replica_train_roc_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_replica_train_pr_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistributed_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceOp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_replica_train_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mtotal_train_auc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceOp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_replica_train_roc_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}