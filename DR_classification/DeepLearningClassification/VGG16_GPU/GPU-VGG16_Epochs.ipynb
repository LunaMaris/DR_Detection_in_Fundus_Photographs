{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GPU-VGG16_Epochs.ipynb","provenance":[{"file_id":"1lzWzX_5q0RZI7sgl8M_qxse7JgTWjbxS","timestamp":1597058559315},{"file_id":"1MshBmCalzqGz5xnx1H0ErFeGsBx_02XT","timestamp":1596527489779},{"file_id":"1pvSj2cOo177dqWp1y6lbiwohGTGeGVm1","timestamp":1596124850987},{"file_id":"1ko8HCMMTNlaR1MCC-1f6QL1uAaqzHXwO","timestamp":1595928299309},{"file_id":"1cjKN36peUbhC36sgbu1cp5rwTPmzAtcg","timestamp":1595852695030}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"9eyAksxJOFsi","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597739975731,"user_tz":-120,"elapsed":2935,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"08986518200734496308"}},"outputId":"a3029a92-7ccf-4c84-a76a-cc9db145cc03"},"source":["# import necessary libraries\n","\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import precision_recall_curve\n","\n","from sklearn.model_selection import train_test_split\n","\n","import time\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D-lg7YM4d9uB","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597739977050,"user_tz":-120,"elapsed":4238,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"08986518200734496308"}},"outputId":"181bc077-2acd-4395-925a-3bc930b20491"},"source":["# define how many gpus are available and set a memmory limit\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","print(\"Number of GPUs Available: \", len(gpus))\n","for i in range(len(gpus)):\n","    tf.config.experimental.set_virtual_device_configuration(gpus[i], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7900)]) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of GPUs Available:  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5FR8-3L4xx0Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597739980999,"user_tz":-120,"elapsed":8175,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"08986518200734496308"}},"outputId":"e9ed906c-0824-45e3-b2db-cf00bd33aaa5"},"source":["strategy = tf.distribute.MirroredStrategy()\n","# the number of replicas that is created by the strategy should be equal to the number of GPU's available\n","print ('Number of synchronized replicas created: {}'.format(strategy.num_replicas_in_sync))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","Number of synchronized replicas created: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"M85RveB_uPv3","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1597740002181,"user_tz":-120,"elapsed":29343,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"08986518200734496308"}},"outputId":"62506443-1fd3-4e4c-f2e9-b7f1fee74f33"},"source":["# read in train and test data in case Google DRIVE is used\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Na905WmrltfS","colab":{}},"source":["# Basepath for Google DRIVE:\n","Basepath = '/content/drive/My Drive/Stage_ENT_Studios_2/Data/Kaggle/Arrays_5GB_float32/'\n","\n","# Basepath for Jupyter notebooks:\n","# Basepath = 'C:/Users/lunam/Documents/1steMaster/Stage/Data_FinalArrays/Kaggle/Array_10GB/'\n","# also an array of 5GB and 20GB available\n","\n","# Basepath for KILI\n","# Basepath = '/home/kili/Desktop/Data_FinalArrays/Kaggle/Arrays/'\n","\n","# train data\n","train_images = np.load(Basepath + 'train_images_Final.npy')\n","print('Shape train images: {}'.format(train_images.shape))\n","\n","train_labels =  np.load(Basepath + 'train_labels_Final.npy')\n","print('Shape train labels: {}'.format(train_labels.shape))\n","\n","train_images, test_images, train_labels, test_labels = train_test_split(train_images, train_labels, test_size=0.33, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WWjV59oPltfZ","colab":{}},"source":["# path to save the model and the tensorboard logs\n","\n","# Basepath for Google DRIVE:\n","base_path = '/content/drive/My Drive/Stage_ENT_Studios_2/DR_Grading/Logs/'\n","\n","# Basepath for jupyter notebooks:\n","# base_path = 'C:/Users/lunam/Documents/1steMaster/Stage/Code_Final/DR_classification/DeepLearningClassification/VGG16_GPU/Logs/'\n","\n","# direction where the tensorboard files will be stored\n","log_dir_tens = base_path + 'Tensorboard_Logs/'\n","# direction where the trained models will be stored\n","log_dir_model = base_path + 'Trained_Model/'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PwUIGJ5wBrSj","colab":{}},"source":["def VGG16(drop_prob = 0.1, init_filters = 64, dense_nodes = [4096,4096,1000]):\n","    # initialization of the weights\n","    W_init = tf.initializers.GlorotUniform()\n","    \n","    # input layer\n","    input_image = tf.keras.layers.Input(shape = (256,256,3))\n","        \n","    # Convolutional block 1\n","    conv2d_1 = tf.keras.layers.Conv2D(init_filters, (3, 3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(input_image)\n","    conv2d_2 = tf.keras.layers.Conv2D(init_filters, (3, 3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_1)\n","    pool_1 = tf.keras.layers.MaxPool2D((2, 2), (2, 2))(conv2d_2)\n","    dropout_1 = tf.keras.layers.Dropout(rate = drop_prob)(pool_1)\n","        \n","    # Convolutional block 2\n","    conv2d_3 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_1)\n","    conv2d_4 = tf.keras.layers.Conv2D(2*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_3)\n","    pool_2 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_4)\n","    dropout_2 = tf.keras.layers.Dropout(rate = drop_prob)(pool_2 )\n","    \n","    # Convolutional block 3\n","    conv2d_5 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_2)\n","    conv2d_6 = tf.keras.layers.Conv2D(4*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_5)\n","    conv2d_7 = tf.keras.layers.Conv2D(4*init_filters, (1,1), activation= tf.nn.relu, kernel_initializer= W_init)(conv2d_6)\n","    pool_3 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_7)\n","    dropout_3 = tf.keras.layers.Dropout(rate = drop_prob)(pool_3)\n","\n","    # Convolutional block 4\n","    conv2d_8 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_3)\n","    conv2d_9 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_8)\n","    conv2d_10 = tf.keras.layers.Conv2D(8*init_filters, (1,1), activation= tf.nn.relu, kernel_initializer= W_init)(conv2d_9)\n","    pool_4 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_10)\n","    dropout_4 = tf.keras.layers.Dropout(rate = drop_prob)(pool_4)\n","\n","    # Convolutional block 5\n","    conv2d_11 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(dropout_4)\n","    conv2d_12 = tf.keras.layers.Conv2D(8*init_filters, (3,3), activation= tf.nn.relu, kernel_initializer= W_init, padding = 'same')(conv2d_11)\n","    conv2d_13 = tf.keras.layers.Conv2D(8*init_filters, (1,1), activation= tf.nn.relu, kernel_initializer= W_init)(conv2d_12)\n","    pool_5 = tf.keras.layers.MaxPool2D((2,2), (2, 2))(conv2d_13)\n","    dropout_5 = tf.keras.layers.Dropout(rate = drop_prob)(pool_5)\n","\n","    # fully connected layers\n","    flatten = tf.keras.layers.Flatten()(dropout_5)\n","    dense_1 = tf.keras.layers.Dense(dense_nodes[0], activation = tf.nn.relu, kernel_initializer= W_init)(flatten)\n","    dense_2 = tf.keras.layers.Dense(dense_nodes[1], activation = tf.nn.relu, kernel_initializer= W_init)(dense_1)\n","    dense_3 = tf.keras.layers.Dense(dense_nodes[2], activation = tf.nn.relu, kernel_initializer= W_init)(dense_2)\n","    output_label = tf.keras.layers.Dense(2, activation = tf.nn.softmax, kernel_initializer= W_init)(dense_3)\n","\n","\n","    model = tf.keras.Model(inputs=input_image, outputs=output_label)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1wSufDo909C5","colab":{}},"source":["# loss function\n","def Bin_CrossEntropy_Loss(pred_labels, true_labels, GlobalBatchSize):\n","\n","    true_labels_pos = true_labels[:,0]\n","    pred_labels_pos = pred_labels[:,0]\n","\n","    loss_object = tf.keras.losses.BinaryCrossentropy(reduction= tf.keras.losses.Reduction.NONE)\n","    loss = loss_object([true_labels_pos], [pred_labels_pos])[0]\n","    return (loss/ GlobalBatchSize)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VQg7qWs9Wo9r","colab":{}},"source":["def train_network(TrainImages, TrainLabels, TestImages, TestLabels, \n","                  Drop_Prob = 0.1, Init_Filters = 64, Dense_Nodes = [4096,4096,1000], batch_size = 3, loss_function = 'BinCrossEntr', optim = 'Adam', \n","                  learning_rate = tf.Variable(1e-5, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True, print_freq = 1):\n","    '''\n","    This function trains the UNet on the indicated train data with corresponding annotations\n","    At the end the trained model is being saved\n","    '''\n","    # setting up saver for the tensorboard logs\n","    if SaveResults:\n","        # creating summary which stores the results that can be visualised with tensorboard\n","        print(\"Setting up summary writer for tensorboard...\")\n","        summary_writer = tf.summary.create_file_writer(log_dir_tens)\n","\n","    # define the train and test batches that can be fed into the network\n","    # global batch size defines the batch size over all availabel GPU's\n","    print('Creating distributed data')\n","    Global_batch_size = batch_size * strategy.num_replicas_in_sync\n","    train_batch_data  = tf.data.Dataset.from_tensor_slices((TrainImages, TrainLabels)).shuffle(TrainImages.shape[0]).batch(Global_batch_size) \n","    test_batch_data = tf.data.Dataset.from_tensor_slices((TestImages, TestLabels)).batch(Global_batch_size) \n","\n","    # distribute the data over the different GPU's\n","    train_dist_data =  strategy.experimental_distribute_dataset(train_batch_data)\n","    test_dist_data =  strategy.experimental_distribute_dataset(test_batch_data)\n","\n","    # define the model that will be used for training and for testing\n","    # the model, optimisation and loss have to be distributed among GPU's\n","    tf.compat.v1.reset_default_graph()\n","    with strategy.scope():\n","\n","        # model\n","        print('Defining the model')\n","        model = VGG16(drop_prob = Drop_Prob, init_filters = Init_Filters, dense_nodes = Dense_Nodes)\n","        \n","        # loss\n","        print('Defining loss')\n","        def compute_loss(PredictedLabels, TrueLabels):\n","            if loss_function == 'BinCrossEntr':\n","                loss = Bin_CrossEntropy_Loss(PredictedLabels, TrueLabels, Global_batch_size)\n","            return loss\n","\n","        # optimization\n","        # a decaying learning rate is used\n","        steps_per_epoch = int(TrainImages.shape[0]/Global_batch_size)\n","        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate= learning_rate, decay_steps= MAX_EPOCH*steps_per_epoch*0.25, decay_rate=0.2, staircase = True)\n","        print('Defining optimization')\n","        if optim == 'Adam':\n","            train_op = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n","        elif optim == 'sgd':\n","            train_op = tf.keras.optimizers.SGD(learning_rate = lr_schedule)\n","\n","        # defining the metrics\n","        print('Defining the metrics')\n","        train_roc_auc = tf.keras.metrics.AUC(curve = 'ROC')\n","        train_pr_auc = tf.keras.metrics.AUC(curve = 'PR')\n","        test_roc_auc = tf.keras.metrics.AUC(curve = 'ROC')\n","        test_pr_auc = tf.keras.metrics.AUC(curve = 'PR')\n","\n","    # one train step is a step in which one batch of data is fed to every GPU\n","    # one train step is a step in which one batch of data is fed to every GPU\n","    def Train_Step(input):\n","\n","        with tf.GradientTape() as tape:\n","            train_images_batch, train_labels_batch = input\n","\n","            # make prediction with model\n","            pred_train_labels = model(train_images_batch, training = True)\n","            # compute loss\n","            train_err = compute_loss(pred_train_labels, train_labels_batch)\n","\n","        # update model\n","        train_weights = model.trainable_variables\n","        gradients = tape.gradient(train_err, train_weights)\n","        train_op.apply_gradients(zip(gradients, train_weights))\n","\n","        # compute auc and aupr score\n","        temp_train_labels = train_labels_batch[:,0]\n","        temp_train_pred_labels = pred_train_labels[:,0]\n","        train_roc_auc.update_state(temp_train_labels, temp_train_pred_labels)\n","        train_pr_auc.update_state(temp_train_labels, temp_train_pred_labels)\n","\n","        # the error per replica is returned\n","        return train_err, train_roc_auc.result(), train_pr_auc.result()\n","\n","    # for the last epoch some testing has to be done, in a test step one batch of test data is fed to every GPU\n","    def Test_Step(input):\n","        test_images_batch, test_labels_batch = input\n","\n","        # make prediction with model\n","        pred_test_labels = model(test_images_batch, training = False)\n","        # compute loss\n","        test_err = compute_loss(pred_test_labels, test_labels_batch)\n","        \n","        # compute auc and aupr score\n","        temp_test_labels = test_labels_batch[:,0]\n","        temp_test_pred_labels = pred_test_labels[:,0]\n","        test_roc_auc.update_state(temp_test_labels, temp_test_pred_labels)\n","        test_pr_auc.update_state(temp_test_labels, temp_test_pred_labels)\n","\n","        # the error per replica is returned\n","        return test_err, test_roc_auc.result(), test_pr_auc.result()\n","\n","\n","    @tf.function\n","    def distributed_train_step(dataset_inputs):\n","        per_replica_losses, per_replica_roc_auc, per_replica_pr_auc = strategy.run(Train_Step, args=(dataset_inputs,))\n","        return per_replica_losses, per_replica_roc_auc, per_replica_pr_auc\n","\n","    @tf.function\n","    def distributed_test_step(dataset_inputs):\n","        per_replica_losses, per_replica_roc_auc, per_replica_pr_auc = strategy.run(Test_Step, args=(dataset_inputs,))\n","        return per_replica_losses, per_replica_roc_auc, per_replica_pr_auc\n","        \n","    \n","    # the train and test steps now have to be performed with the distributed strategy\n","    print('Training')\n","    for epo in range(1,MAX_EPOCH+1):\n","        start_time = time.time()\n","        \n","        n_train_steps = 0\n","        total_train_loss = 0\n","        total_train_auc = 0\n","        total_train_aupr = 0\n","        # go over all global batches\n","        for train_input_data in train_dist_data:\n","            n_train_steps+=1\n","            per_replica_train_losses, per_replica_train_roc_auc, per_replica_train_pr_auc = distributed_train_step(train_input_data)\n","            total_train_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_train_losses, axis=None)\n","            total_train_auc += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_train_roc_auc, axis=None)\n","            total_train_aupr += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_train_pr_auc, axis=None)\n","\n","        # every print frequency the train and test resutls are printed out\n","        if epo % print_freq == 0 or epo == 1 or epo == (MAX_EPOCH):\n","\n","            # calculate the final training results for this epoch\n","            total_train_loss = total_train_loss/n_train_steps\n","            total_train_auc = total_train_auc/n_train_steps\n","            total_train_aupr = total_train_aupr/n_train_steps\n","\n","            # print out the train results\n","            print('epoch {} took {}s'.format(epo, time.time() - start_time))\n","            print('   train loss: {}'.format(total_train_loss))\n","            print('   train auc: {}'.format(total_train_auc))\n","            print('   train aupr: {}'.format(total_train_aupr))\n","\n","            if SaveResults:      \n","                # save these values to visualize them later with tensorboard\n","                with summary_writer.as_default():\n","                    tf.summary.scalar('train_loss', total_train_loss, step = epo)\n","                    tf.summary.scalar('train_roc_auc', total_train_auc, step = epo)\n","                    tf.summary.scalar('train_pr_auc', total_train_aupr, step = epo)\n","\n","\n","            # some testing has to be done at these print frequencies\n","            print('Testing')\n","            n_test_steps = 0\n","            total_test_loss = 0\n","            total_test_auc = 0\n","            total_test_aupr = 0\n","\n","            for test_input_data in test_dist_data:\n","                n_test_steps+=1\n","                per_replica_test_losses, per_replica_test_roc_auc, per_replica_test_pr_auc = distributed_test_step(test_input_data)\n","                total_test_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_test_losses, axis=None)\n","                total_test_auc += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_test_roc_auc, axis=None)\n","                total_test_aupr += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_test_pr_auc, axis=None)\n","\n","\n","            total_test_loss = total_test_loss/n_test_steps\n","            total_test_auc = total_test_auc/n_test_steps\n","            total_test_aupr = total_test_aupr/n_test_steps\n","\n","            # print out the test results      \n","            print('   validation loss: {}'.format(total_test_loss))\n","            print('   validation auc: {}'.format(total_test_auc))\n","            print('   validation aupr: {}'.format(total_test_aupr))\n","\n","            if SaveResults:\n","                with summary_writer.as_default():\n","                    tf.summary.scalar('validation_loss', total_test_loss, step = epo)\n","                    tf.summary.scalar('validation_roc_auc', total_test_auc, step = epo)\n","                    tf.summary.scalar('validation_pr_auc', total_test_aupr, step = epo)  \n","                    summary_writer.flush()     \n","\n","\n","        if SaveResults:\n","            # storing the model weights at two time-points\n","            if epo == int(MAX_EPOCH/2):\n","                print('Saving the intermediate model weights...')\n","                model.save_weights(log_dir_model + 'VGG16_' + str(epo) +'_epochs')\n","                print('Done')\n","\n","            if epo == MAX_EPOCH:\n","                print('Saving the model weights...')\n","                model.save_weights(log_dir_model + 'VGG16_' + str(epo) +'_epochs')\n","                print('Done')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AY41_1b53yRn","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1597740080850,"user_tz":-120,"elapsed":107982,"user":{"displayName":"Luna Maris","photoUrl":"","userId":"08986518200734496308"}},"outputId":"7ff4e6ee-3fa3-4390-dd35-799e9108b0e7"},"source":["train_network(train_images, train_labels, test_images, test_labels, MAX_EPOCH = 100, learning_rate = tf.Variable(1e-5, dtype=tf.float32), Drop_Prob = 0.3, batch_size = 40)\n","# (TrainImages, TrainLabels, TestImages, TestLables, \n","#                   Drop_Prob = 0.1, Init_Filters = 64, Dense_Nodes = [4096,4096,1000], batch_size = 3, loss_function = 'BinCrossEntr', optim = 'Adam', \n","#                   learning_rate = tf.Variable(1e-5, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True, print_freq = 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setting up summary writer for tensorboard...\n","Creating distributed data\n","Defining the model\n","Defining loss\n","Defining optimization\n","Defining the metrics\n","Training\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Iterator.get_next_as_optional()` instead.\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","epoch 1 took 32.6915545463562s\n","   train loss: 0.0491248220205307\n","   train auc: 0.4308297038078308\n","   train aupr: 0.45293405652046204\n","Testing\n","   test loss: 0.01786845736205578\n","   test auc: 0.575635552406311\n","   test aupr: 0.5102670192718506\n","epoch 2 took 11.134710550308228s\n","   train loss: 0.02367829531431198\n","   train auc: 0.4628523290157318\n","   train aupr: 0.46833011507987976\n","Testing\n","   test loss: 0.017317751422524452\n","   test auc: 0.5287131071090698\n","   test aupr: 0.5148479342460632\n","epoch 3 took 11.148507118225098s\n","   train loss: 0.01983882300555706\n","   train auc: 0.4807896614074707\n","   train aupr: 0.4861927628517151\n","Testing\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-515dea3820cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_EPOCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDrop_Prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# (TrainImages, TrainLabels, TestImages, TestLables,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#                   Drop_Prob = 0.1, Init_Filters = 64, Dense_Nodes = [4096,4096,1000], batch_size = 3, loss_function = 'BinCrossEntr', optim = 'Adam',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#                   learning_rate = tf.Variable(1e-5, dtype=tf.float32), MAX_EPOCH = 10, SaveResults = True, print_freq = 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-e84885644f26>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(TrainImages, TrainLabels, TestImages, TestLabels, Drop_Prob, Init_Filters, Dense_Nodes, batch_size, loss_function, optim, learning_rate, MAX_EPOCH, SaveResults, print_freq)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtest_input_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dist_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mn_test_steps\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mper_replica_test_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_replica_test_roc_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_replica_test_pr_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistributed_test_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mtotal_test_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceOp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_replica_test_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mtotal_test_auc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceOp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_replica_test_roc_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"hiAk0m1SZ2A4","colab_type":"code","colab":{}},"source":["print(test_labels)"],"execution_count":null,"outputs":[]}]}